{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Save img for FID_GPT2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sairamkiran9/AttnGAN-trans/blob/structure/Save_img_for_FID_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm4T8mB56t0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de5dc01d-3b29-4371-b5c4-5b5894a87f61"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc8y1Ab_ZXqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d983ce6-a064-4752-9114-0fd910eb2d66"
      },
      "source": [
        "cd '/content/drive/My Drive/data'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utOKqtKHhC9T"
      },
      "source": [
        "# rm -r '/content/data/DAMSMencoders'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLtvO9nNVjta"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "from tqdm.notebook import tqdm,trange\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "import textwrap,csv\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import tarfile\n",
        "import zipfile\n",
        "import csv\n",
        "import os\n",
        "from tqdm.notebook import tqdm,trange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ditVkOybNO7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d6af1b-554e-4840-f526-05f5cbfeac28"
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 21.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 27.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 25.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 22.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 19.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 16.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 15.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/c4/6a866d878e41d0fc4e29bc7782118e5202f21beee8fc492b10917e138a75/boto3-1.16.55-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 28.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.7.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[?25hCollecting botocore<1.20.0,>=1.19.55\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/47/45d58c8a96646acd36b81899d9c6bdc2b3e7fe0aac253f1424e9f7071c00/botocore-1.19.55-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 41.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.55->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.55->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.19.55 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.55 botocore-1.19.55 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxJ_wJV_5yZO",
        "outputId": "ff745bd7-ccf7-4048-d11b-2c5fdc084e2c"
      },
      "source": [
        "if 'data' not in os.listdir('/content/'):\n",
        "  !mkdir data\n",
        "\n",
        "  zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/data/birds.zip\", 'r')\n",
        "  zip_ref.extractall(\"/content/data\")\n",
        "  zip_ref.close()\n",
        "\n",
        "  zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/data/cfg.zip\", 'r')\n",
        "  zip_ref.extractall(\"/content/data/\")\n",
        "  zip_ref.close()\n",
        "\n",
        "  zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/data/gpt2.zip\", 'r')\n",
        "  zip_ref.extractall(\"/content/data/DAMSMencoders/bird\")\n",
        "  zip_ref.close()\n",
        "\n",
        "  fname = \"/content/drive/My Drive/data/CUB_200_2011.tar.gz\"\n",
        "  if fname.endswith(\"tar.gz\"):\n",
        "      tar = tarfile.open(fname, \"r:gz\")\n",
        "      tar.extractall(\"/content/data/birds\")\n",
        "      tar.close()\n",
        "  elif fname.endswith(\"tar\"):\n",
        "      tar = tarfile.open(fname, \"r:\")\n",
        "      tar.extractall(\"/content/data/birds\")\n",
        "      tar.close()\n",
        "  print(\"Data loaded to Colab Notebook!\")\n",
        "else:\n",
        "  print(\"Data already loaded!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "Data loaded to Colab Notebook!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew14GdTeXP60"
      },
      "source": [
        "import pickle\n",
        "with open('/content/data/birds/example_filenames.txt','w') as file:\n",
        "  with open('/content/data/birds/test/filenames.pickle','rb') as f:\n",
        "    dir=pickle.load(f)\n",
        "    for d in dir:\n",
        "      d = 'text_c10/'+d\n",
        "      file.write(d)\n",
        "      file.write('\\n')\n",
        "    f.close()\n",
        "  file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twh4FQC650yf"
      },
      "source": [
        "code/miscc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo3MsrQ15inx"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "from easydict import EasyDict as edict\n",
        "from pytorch_pretrained_bert import GPT2Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95SszhEs5lMm"
      },
      "source": [
        "__C = edict()\n",
        "cfg = __C\n",
        "\n",
        "# Dataset name: flowers, birds\n",
        "__C.DATASET_NAME = 'birds'\n",
        "__C.CONFIG_NAME = ''\n",
        "__C.DATA_DIR = ''\n",
        "__C.GPU_ID = 0\n",
        "__C.CUDA = True\n",
        "__C.WORKERS = 6\n",
        "\n",
        "__C.RNN_TYPE = 'LSTM'   # 'GRU'\n",
        "__C.B_VALIDATION = False\n",
        "\n",
        "__C.TREE = edict()\n",
        "__C.TREE.BRANCH_NUM = 3\n",
        "__C.TREE.BASE_SIZE = 64\n",
        "\n",
        "\n",
        "# Training options\n",
        "__C.TRAIN = edict()\n",
        "__C.TRAIN.BATCH_SIZE = 64\n",
        "__C.TRAIN.MAX_EPOCH = 600\n",
        "__C.TRAIN.SNAPSHOT_INTERVAL = 1000\n",
        "__C.TRAIN.DISCRIMINATOR_LR = 2e-4\n",
        "__C.TRAIN.GENERATOR_LR = 2e-4\n",
        "__C.TRAIN.ENCODER_LR = 2e-4\n",
        "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
        "__C.TRAIN.FLAG = True\n",
        "__C.TRAIN.NET_E = ''\n",
        "__C.TRAIN.NET_G = '/content/drive/My Drive/data/gpt2_output_attn1/Model/netG_epoch_8.pth'\n",
        "__C.TRAIN.B_NET_D = True\n",
        "\n",
        "__C.TRAIN.SMOOTH = edict()\n",
        "__C.TRAIN.SMOOTH.GAMMA1 = 5.0\n",
        "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
        "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
        "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
        "\n",
        "\n",
        "# Modal options\n",
        "__C.GAN = edict()\n",
        "__C.GAN.DF_DIM = 64\n",
        "__C.GAN.GF_DIM = 128\n",
        "__C.GAN.Z_DIM = 100\n",
        "__C.GAN.CONDITION_DIM = 100\n",
        "__C.GAN.R_NUM = 2\n",
        "__C.GAN.B_ATTENTION = True\n",
        "__C.GAN.B_DCGAN = False\n",
        "\n",
        "\n",
        "__C.TEXT = edict()\n",
        "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
        "__C.TEXT.EMBEDDING_DIM = 256\n",
        "__C.TEXT.WORDS_NUM = 18\n",
        "\n",
        "\n",
        "def _merge_a_into_b(a, b):\n",
        "    \"\"\"Merge config dictionary a into config dictionary b, clobbering the\n",
        "    options in b whenever they are also specified in a.\n",
        "    \"\"\"\n",
        "    if type(a) is not edict:\n",
        "        return\n",
        "\n",
        "    for k, v in a.items():\n",
        "        # a must specify keys that are in b\n",
        "        if k not in b:\n",
        "            raise KeyError('{} is not a valid config key'.format(k))\n",
        "\n",
        "        # the types must match, too\n",
        "        old_type = type(b[k])\n",
        "        if old_type is not type(v):\n",
        "            if isinstance(b[k], np.ndarray):\n",
        "                v = np.array(v, dtype=b[k].dtype)\n",
        "            else:\n",
        "                raise ValueError(('Type mismatch ({} vs. {}) '\n",
        "                                  'for config key: {}').format(type(b[k]),\n",
        "                                                               type(v), k))\n",
        "\n",
        "        # recursively merge dicts\n",
        "        if type(v) is edict:\n",
        "            try:\n",
        "                _merge_a_into_b(a[k], b[k])\n",
        "            except:\n",
        "                print('Error under config key: {}'.format(k))\n",
        "                raise\n",
        "        else:\n",
        "            b[k] = v\n",
        "\n",
        "\n",
        "def cfg_from_file(filename):\n",
        "    \"\"\"Load a config file and merge it into the default options.\"\"\"\n",
        "    # print(filename)\n",
        "    import yaml\n",
        "    with open(filename, 'r') as f:\n",
        "        yaml_cfg = edict(yaml.load(f))\n",
        "        # print(\"success\")\n",
        "    _merge_a_into_b(yaml_cfg, __C)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZwoZApa_HdU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=False)\n",
        "\n",
        "\n",
        "def func_attention(query, context, gamma1):\n",
        "    \"\"\"\n",
        "    query: batch x ndf x queryL\n",
        "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
        "    mask: batch_size x sourceL\n",
        "    \"\"\"\n",
        "    batch_size, queryL = query.size(0), query.size(2)\n",
        "    ih, iw = context.size(2), context.size(3)\n",
        "    sourceL = ih * iw\n",
        "\n",
        "    # --> batch x sourceL x ndf\n",
        "    context = context.view(batch_size, -1, sourceL)\n",
        "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
        "\n",
        "    # Get attention\n",
        "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
        "    # -->batch x sourceL x queryL\n",
        "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
        "    # --> batch*sourceL x queryL\n",
        "    attn = attn.view(batch_size*sourceL, queryL)\n",
        "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
        "\n",
        "    # --> batch x sourceL x queryL\n",
        "    attn = attn.view(batch_size, sourceL, queryL)\n",
        "    # --> batch*queryL x sourceL\n",
        "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "    attn = attn.view(batch_size*queryL, sourceL)\n",
        "    #  Eq. (9)\n",
        "    attn = attn * gamma1\n",
        "    attn = nn.Softmax()(attn)\n",
        "    attn = attn.view(batch_size, queryL, sourceL)\n",
        "    # --> batch x sourceL x queryL\n",
        "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
        "    # --> batch x ndf x queryL\n",
        "    weightedContext = torch.bmm(context, attnT)\n",
        "\n",
        "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "\n",
        "class GlobalAttentionGeneral(nn.Module):\n",
        "    def __init__(self, idf, cdf):\n",
        "        super(GlobalAttentionGeneral, self).__init__()\n",
        "        self.conv_context = conv1x1(cdf, idf)\n",
        "        self.sm = nn.Softmax()\n",
        "        self.mask = None\n",
        "\n",
        "    def applyMask(self, mask):\n",
        "        self.mask = mask  # batch x sourceL\n",
        "\n",
        "    def forward(self, input, context):\n",
        "        \"\"\"\n",
        "            input: batch x idf x ih x iw (queryL=ihxiw)\n",
        "            context: batch x cdf x sourceL\n",
        "        \"\"\"\n",
        "        ih, iw = input.size(2), input.size(3)\n",
        "        queryL = ih * iw\n",
        "        batch_size, sourceL = context.size(0), context.size(2)\n",
        "\n",
        "        # --> batch x queryL x idf\n",
        "        target = input.view(batch_size, -1, queryL)\n",
        "        targetT = torch.transpose(target, 1, 2).contiguous()\n",
        "        # batch x cdf x sourceL --> batch x cdf x sourceL x 1\n",
        "        sourceT = context.unsqueeze(3)\n",
        "        # --> batch x idf x sourceL\n",
        "        sourceT = self.conv_context(sourceT).squeeze(3)\n",
        "\n",
        "        # Get attention\n",
        "        # (batch x queryL x idf)(batch x idf x sourceL)\n",
        "        # -->batch x queryL x sourceL\n",
        "        attn = torch.bmm(targetT, sourceT)\n",
        "        # --> batch*queryL x sourceL\n",
        "        attn = attn.view(batch_size*queryL, sourceL)\n",
        "        if self.mask is not None:\n",
        "            # batch_size x sourceL --> batch_size*queryL x sourceL\n",
        "            mask = self.mask.repeat(queryL, 1)\n",
        "            attn.data.masked_fill_(mask.data, -float('inf'))\n",
        "        attn = self.sm(attn)  # Eq. (2)\n",
        "        # --> batch x queryL x sourceL\n",
        "        attn = attn.view(batch_size, queryL, sourceL)\n",
        "        # --> batch x sourceL x queryL\n",
        "        attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "        # (batch x idf x sourceL)(batch x sourceL x queryL)\n",
        "        # --> batch x idf x queryL\n",
        "        weightedContext = torch.bmm(sourceT, attn)\n",
        "        weightedContext = weightedContext.view(batch_size, -1, ih, iw)\n",
        "        attn = attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "        return weightedContext, attn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3usYkyhk57SZ"
      },
      "source": [
        "import numpy as np\n",
        "# from miscc.config import cfg\n",
        "\n",
        "# from GlobalAttention import func_attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFhKtbs7hdRY"
      },
      "source": [
        "def cal_activations(act):\n",
        "  mu = np.mean(act, axis=0)\n",
        "  sigma = np.cov(act, rowvar=False)\n",
        "  return mu, sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roXxI7rxf484"
      },
      "source": [
        "\n",
        "def FID(x1, x2, dim=1, eps=1e-8):\n",
        "    \"\"\"Returns FID between x1 and x2, computed along dim.\n",
        "    \"\"\"\n",
        "    mu1 = np.atleast_1d(cal_activations(x1))\n",
        "    mu2 = np.atleast_1d(cal_activations(x2))\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    # Product might be almost singular\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) +\n",
        "            np.trace(sigma2) - 2 * tr_covmean)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVKvWe415lPo"
      },
      "source": [
        " ##################Loss for matching text-image###################\n",
        "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
        "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\n",
        "    \"\"\"\n",
        "    w12 = torch.sum(x1 * x2, dim)\n",
        "    w1 = torch.norm(x1, 2, dim)\n",
        "    w2 = torch.norm(x2, 2, dim)\n",
        "    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n",
        "\n",
        "\n",
        "def sent_loss(cnn_code, rnn_code, labels, class_ids,\n",
        "              batch_size, eps=1e-8):\n",
        "    # ### Mask mis-match samples  ###\n",
        "    # that come from the same class as the real sample ###\n",
        "    masks = []\n",
        "    if class_ids is not None:\n",
        "        for i in range(batch_size):\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.ByteTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    # --> seq_len x batch_size x nef\n",
        "    if cnn_code.dim() == 2:\n",
        "        cnn_code = cnn_code.unsqueeze(0)\n",
        "        rnn_code = rnn_code.unsqueeze(0)\n",
        "\n",
        "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
        "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
        "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
        "    # scores* / norm*: seq_len x batch_size x batch_size\n",
        "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
        "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
        "    scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "\n",
        "    # --> batch_size x batch_size\n",
        "    scores0 = scores0.squeeze()\n",
        "    if class_ids is not None:\n",
        "        scores0.data.masked_fill_(masks, -float('inf'))\n",
        "    scores1 = scores0.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQvmXR8I5lSa"
      },
      "source": [
        "def words_loss(img_features, words_emb, labels,\n",
        "               cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "    for i in range(batch_size):\n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = cosine_similarity(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.ByteTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8xph8OVfRc9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkynFFjQfIbF"
      },
      "source": [
        "def Compute_FID(img_features, words_emb, labels,\n",
        "               cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "    for i in range(batch_size):\n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = FID(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.ByteTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luEeSiq25lYQ"
      },
      "source": [
        "# ##################Loss for G and Ds##############################\n",
        "def discriminator_loss(netD, real_imgs, fake_imgs, conditions,\n",
        "                       real_labels, fake_labels):\n",
        "    # Forward\n",
        "    real_features = netD(real_imgs)\n",
        "    fake_features = netD(fake_imgs.detach())\n",
        "    # loss\n",
        "    #\n",
        "    cond_real_logits = netD.COND_DNET(real_features, conditions)\n",
        "    cond_real_errD = nn.BCELoss()(cond_real_logits, real_labels)\n",
        "    cond_fake_logits = netD.COND_DNET(fake_features, conditions)\n",
        "    cond_fake_errD = nn.BCELoss()(cond_fake_logits, fake_labels)\n",
        "    #\n",
        "    batch_size = real_features.size(0)\n",
        "    cond_wrong_logits = netD.COND_DNET(real_features[:(batch_size - 1)], conditions[1:batch_size])\n",
        "    cond_wrong_errD = nn.BCELoss()(cond_wrong_logits, fake_labels[1:batch_size])\n",
        "\n",
        "    if netD.UNCOND_DNET is not None:\n",
        "        real_logits = netD.UNCOND_DNET(real_features)\n",
        "        fake_logits = netD.UNCOND_DNET(fake_features)\n",
        "        real_errD = nn.BCELoss()(real_logits, real_labels)\n",
        "        fake_errD = nn.BCELoss()(fake_logits, fake_labels)\n",
        "        errD = ((real_errD + cond_real_errD) / 2. +\n",
        "                (fake_errD + cond_fake_errD + cond_wrong_errD) / 3.)\n",
        "    else:\n",
        "        errD = cond_real_errD + (cond_fake_errD + cond_wrong_errD) / 2.\n",
        "    return errD\n",
        "\n",
        "\n",
        "def generator_loss(netsD, image_encoder, fake_imgs, real_labels,\n",
        "                   words_embs, sent_emb, match_labels,\n",
        "                   cap_lens, class_ids):\n",
        "    numDs = len(netsD)\n",
        "    batch_size = real_labels.size(0)\n",
        "    logs = ''\n",
        "    # Forward\n",
        "    errG_total = 0\n",
        "    for i in range(numDs):\n",
        "        features = netsD[i](fake_imgs[i])\n",
        "        cond_logits = netsD[i].COND_DNET(features, sent_emb)\n",
        "        cond_errG = nn.BCELoss()(cond_logits, real_labels)\n",
        "        if netsD[i].UNCOND_DNET is  not None:\n",
        "            logits = netsD[i].UNCOND_DNET(features)\n",
        "            errG = nn.BCELoss()(logits, real_labels)\n",
        "            g_loss = errG + cond_errG\n",
        "        else:\n",
        "            g_loss = cond_errG\n",
        "        errG_total += g_loss\n",
        "        # err_img = errG_total.data[0]\n",
        "        logs += 'g_loss%d: %.2f ' % (i, g_loss)\n",
        "\n",
        "        # Ranking loss\n",
        "        if i == (numDs - 1):\n",
        "            # words_features: batch_size x nef x 17 x 17\n",
        "            # sent_code: batch_size x nef\n",
        "            region_features, cnn_code = image_encoder(fake_imgs[i])\n",
        "            w_loss0, w_loss1, _ = words_loss(region_features, words_embs,\n",
        "                                             match_labels, cap_lens,\n",
        "                                             class_ids, batch_size)\n",
        "            w_loss = (w_loss0 + w_loss1) * \\\n",
        "                cfg.TRAIN.SMOOTH.LAMBDA\n",
        "            # err_words = err_words + w_loss.data[0]\n",
        "\n",
        "            s_loss0, s_loss1 = sent_loss(cnn_code, sent_emb,\n",
        "                                         match_labels, class_ids, batch_size)\n",
        "            s_loss = (s_loss0 + s_loss1) * \\\n",
        "                cfg.TRAIN.SMOOTH.LAMBDA\n",
        "            # err_sent = err_sent + s_loss.data[0]\n",
        "\n",
        "            errG_total += w_loss + s_loss\n",
        "            logs += 'w_loss: %.2f s_loss: %.2f ' % (w_loss, s_loss)\n",
        "    return errG_total, logs\n",
        "\n",
        "\n",
        "##################################################################\n",
        "def KL_loss(mu, logvar):\n",
        "    # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
        "    KLD = torch.mean(KLD_element).mul_(-0.5)\n",
        "    return KLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upHx7d-36Qlj"
      },
      "source": [
        "miscc/utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOZc_mSe6UNv"
      },
      "source": [
        "import os\n",
        "import errno\n",
        "import numpy as np\n",
        "from torch.nn import init\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from copy import deepcopy\n",
        "import skimage.transform\n",
        "\n",
        "# from miscc.config import cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNKfEefWs86n"
      },
      "source": [
        "class INCEPTION_V3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(INCEPTION_V3, self).__init__()\n",
        "        self.model = models.inception_v3()\n",
        "        # url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        ## print(next(model.parameters()).data)\n",
        "        # state_dict = \\\n",
        "        #     model_zoo.load_url(url, map_location=lambda storage, loc: storage)\n",
        "        state_dict = torch.load('/content/drive/My Drive/data/inception_v3.pth')\n",
        "        self.model.load_state_dict(state_dict)\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        # print('Load pretrained model from ', url)\n",
        "        # print(next(self.model.parameters()).data)\n",
        "        # print(self.model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # [-1.0, 1.0] --> [0, 1.0]\n",
        "        x = input * 0.5 + 0.5\n",
        "        # mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]\n",
        "        # --> mean = 0, std = 1\n",
        "        x[:, 0] = (x[:, 0] - 0.485) / 0.229\n",
        "        x[:, 1] = (x[:, 1] - 0.456) / 0.224\n",
        "        x[:, 2] = (x[:, 2] - 0.406) / 0.225\n",
        "        #\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.model(x)\n",
        "        x = nn.Softmax()(x)\n",
        "        return x\n",
        "        \n",
        "def compute_inception_score(predictions, num_splits=1):\n",
        "    # print('predictions', predictions.shape)\n",
        "    scores = []\n",
        "    for i in range(num_splits):\n",
        "        istart = i * predictions.shape[0] // num_splits\n",
        "        iend = (i + 1) * predictions.shape[0] // num_splits\n",
        "        part = predictions[istart:iend, :]\n",
        "        kl = part * \\\n",
        "            (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
        "        kl = np.mean(np.sum(kl, 1))\n",
        "        scores.append(np.exp(kl))\n",
        "    #print(\"Inception Score: \",np.mean(scores), np.std(scores))\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "\n",
        "def negative_log_posterior_probability(predictions, num_splits=1):\n",
        "    # print('predictions', predictions.shape)\n",
        "    scores = []\n",
        "    for i in range(num_splits):\n",
        "        istart = i * predictions.shape[0] // num_splits\n",
        "        iend = (i + 1) * predictions.shape[0] // num_splits\n",
        "        part = predictions[istart:iend, :]\n",
        "        result = -1. * np.log(np.max(part, 1))\n",
        "        result = np.mean(result)\n",
        "        scores.append(result)\n",
        "    return np.mean(scores), np.std(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_IOksS75lbd"
      },
      "source": [
        "# For visualization ################################################\n",
        "COLOR_DIC = {0:[128,64,128],  1:[244, 35,232],\n",
        "             2:[70, 70, 70],  3:[102,102,156],\n",
        "             4:[190,153,153], 5:[153,153,153],\n",
        "             6:[250,170, 30], 7:[220, 220, 0],\n",
        "             8:[107,142, 35], 9:[152,251,152],\n",
        "             10:[70,130,180], 11:[220,20, 60],\n",
        "             12:[255, 0, 0],  13:[0, 0, 142],\n",
        "             14:[119,11, 32], 15:[0, 60,100],\n",
        "             16:[0, 80, 100], 17:[0, 0, 230],\n",
        "             18:[0,  0, 70],  19:[0, 0,  0]}\n",
        "FONT_MAX = 50\n",
        "\n",
        "\n",
        "def drawCaption(convas, captions, ixtoword, vis_size, off1=2, off2=2):\n",
        "    num = captions.size(0)\n",
        "    img_txt = Image.fromarray(convas)\n",
        "    # get a font\n",
        "    # fnt = None  # ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
        "    # fnt = ImageFont.truetype('arial.ttf', 50)\n",
        "    fnt = ImageFont.load_default()\n",
        "    # get a drawing context\n",
        "    d = ImageDraw.Draw(img_txt)\n",
        "    sentence_list = []\n",
        "    for i in range(num):\n",
        "        cap = captions[i].data.cpu().numpy()\n",
        "        sentence = []\n",
        "        for j in range(len(cap)):\n",
        "            if cap[j] == 0:\n",
        "                break\n",
        "            word = ixtoword[cap[j]].encode('ascii', 'ignore').decode('ascii')\n",
        "            d.text(((j + off1) * (vis_size + off2), i * FONT_MAX), '%d:%s' % (j, word[:6]),\n",
        "                   font=fnt, fill=(255, 255, 255, 255))\n",
        "            sentence.append(word)\n",
        "        sentence_list.append(sentence)\n",
        "    return img_txt, sentence_list\n",
        "\n",
        "\n",
        "def build_super_images(real_imgs, captions, ixtoword,\n",
        "                       attn_maps, att_sze, lr_imgs=None,\n",
        "                       batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                       max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    \n",
        "    nvis = 1\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    text_convas = \\\n",
        "        np.ones([batch_size * FONT_MAX,\n",
        "                 (max_word_num + 2) * (vis_size + 2), 3],\n",
        "                dtype=np.uint8)\n",
        "\n",
        "    for i in range(max_word_num):\n",
        "        istart = (i + 2) * (vis_size + 2)\n",
        "        iend = (i + 3) * (vis_size + 2)\n",
        "        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    seq_len = max_word_num\n",
        "    img_set = []\n",
        "    num = nvis  # len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        # --> 1 x 1 x 17 x 17\n",
        "        attn_max = attn.max(dim=1, keepdim=True)\n",
        "        attn = torch.cat([attn_max[0], attn], 1)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = attn.shape[0]\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        if lr_imgs is None:\n",
        "            lrI = img\n",
        "        else:\n",
        "            lrI = lr_imgs[i]\n",
        "        row = [lrI, middle_pad]\n",
        "        row_merge = [img, middle_pad]\n",
        "        row_beforeNorm = []\n",
        "        minVglobal, maxVglobal = 1, 0\n",
        "        for j in range(num_attn):\n",
        "            one_map = attn[j]\n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map=cv2.resize(one_map,(vis_size,vis_size),interpolation = cv2.INTER_CUBIC)\n",
        "                #one_map = \\\n",
        "                    #skimage.transform.pyramid_expand(one_map, sigma=20,\n",
        "                     #                                upscale=vis_size // att_sze)\n",
        "            row_beforeNorm.append(one_map)\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            if minVglobal > minV:\n",
        "                minVglobal = minV\n",
        "            if maxVglobal < maxV:\n",
        "                maxVglobal = maxV\n",
        "        for j in range(seq_len + 1):\n",
        "            if j < num_attn:\n",
        "                one_map = row_beforeNorm[j]\n",
        "                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
        "                one_map *= 255\n",
        "                #\n",
        "                PIL_im = Image.fromarray(np.uint8(img))\n",
        "                PIL_att = Image.fromarray(np.uint8(one_map))\n",
        "                merged = \\\n",
        "                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "                mask = Image.new('L', (vis_size, vis_size), (210))\n",
        "                merged.paste(PIL_im, (0, 0))\n",
        "                merged.paste(PIL_att, (0, 0), mask)\n",
        "                merged = np.array(merged)[:, :, :3]\n",
        "            else:\n",
        "                one_map = post_pad\n",
        "                merged = post_pad\n",
        "            row.append(one_map)\n",
        "            row.append(middle_pad)\n",
        "            #\n",
        "            row_merge.append(merged)\n",
        "            row_merge.append(middle_pad)\n",
        "        row = np.concatenate(row, 1)\n",
        "        row_merge = np.concatenate(row_merge, 1)\n",
        "        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('txt', txt.shape, 'row', row.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        row = np.concatenate([txt, row, row_merge], 0)\n",
        "        img_set.append(row)\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def build_super_images2(real_imgs, captions, cap_lens, ixtoword,\n",
        "                        attn_maps, att_sze, vis_size=256, topK=5):\n",
        "    batch_size = real_imgs.size(0)\n",
        "    max_word_num = np.max(cap_lens)\n",
        "    text_convas = np.ones([batch_size * FONT_MAX,\n",
        "                           max_word_num * (vis_size + 2), 3],\n",
        "                           dtype=np.uint8)\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    img_set = []\n",
        "    num = len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size, off1=0)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = cap_lens[i]\n",
        "        thresh = 2./float(num_attn)\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        row = []\n",
        "        row_merge = []\n",
        "        row_txt = []\n",
        "        row_beforeNorm = []\n",
        "        conf_score = []\n",
        "        for j in range(num_attn):\n",
        "            one_map = attn[j]\n",
        "            mask0 = one_map > (2. * thresh)\n",
        "            conf_score.append(np.sum(one_map * mask0))\n",
        "            mask = one_map > thresh\n",
        "            one_map = one_map * mask\n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map = cv2.resize(one_map,(vis_size,vis_size),interpolation = cv2.INTER_CUBIC)\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            one_map = (one_map - minV) / (maxV - minV)\n",
        "            row_beforeNorm.append(one_map)\n",
        "        sorted_indices = np.argsort(conf_score)[::-1]\n",
        "\n",
        "        for j in range(num_attn):\n",
        "            one_map = row_beforeNorm[j]\n",
        "            one_map *= 255\n",
        "            #\n",
        "            PIL_im = Image.fromarray(np.uint8(img))\n",
        "            PIL_att = Image.fromarray(np.uint8(one_map))\n",
        "            merged = \\\n",
        "                Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "            mask = Image.new('L', (vis_size, vis_size), (180))  # (210)\n",
        "            merged.paste(PIL_im, (0, 0))\n",
        "            merged.paste(PIL_att, (0, 0), mask)\n",
        "            merged = np.array(merged)[:, :, :3]\n",
        "\n",
        "            row.append(np.concatenate([one_map, middle_pad], 1))\n",
        "            #\n",
        "            row_merge.append(np.concatenate([merged, middle_pad], 1))\n",
        "            #\n",
        "            txt = text_map[i * FONT_MAX:(i + 1) * FONT_MAX,\n",
        "                           j * (vis_size + 2):(j + 1) * (vis_size + 2), :]\n",
        "            row_txt.append(txt)\n",
        "        # reorder\n",
        "        row_new = []\n",
        "        row_merge_new = []\n",
        "        txt_new = []\n",
        "        for j in range(num_attn):\n",
        "            idx = sorted_indices[j]\n",
        "            row_new.append(row[idx])\n",
        "            row_merge_new.append(row_merge[idx])\n",
        "            txt_new.append(row_txt[idx])\n",
        "        row = np.concatenate(row_new[:topK], 1)\n",
        "        row_merge = np.concatenate(row_merge_new[:topK], 1)\n",
        "        txt = np.concatenate(txt_new[:topK], 1)\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('Warnings: txt', txt.shape, 'row', row.shape,\n",
        "                  'row_merge_new', row_merge_new.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        row = np.concatenate([txt, row_merge], 0)\n",
        "        img_set.append(row)\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "####################################################################\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.orthogonal(m.weight.data, 1.0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        nn.init.orthogonal(m.weight.data, 1.0)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0.0)\n",
        "\n",
        "\n",
        "def load_params(model, new_param):\n",
        "    for p, new_p in zip(model.parameters(), new_param):\n",
        "        p.data.copy_(new_p)\n",
        "\n",
        "\n",
        "def copy_G_params(model):\n",
        "    flatten = deepcopy(list(p.data for p in model.parameters()))\n",
        "    return flatten\n",
        "\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1idyWJ85leI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_teLlla5lhM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8yiyDx15lkB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi5d4-nk0mqR"
      },
      "source": [
        "dataset.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82sHxbk33KtG"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "# from miscc.config import cfg\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy.random as random\n",
        "if sys.version_info[0] == 2:\n",
        "    import cPickle as pickle\n",
        "else:\n",
        "    import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6P1M8yO3KwM"
      },
      "source": [
        "def prepare_data(data):\n",
        "    imgs, captions, captions_lens, class_ids, keys = data\n",
        "    #print(captions)\n",
        "    # sort data by the length in a decreasing order\n",
        "    sorted_cap_lens, sorted_cap_indices = \\\n",
        "        torch.sort(captions_lens, 0, True)\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = imgs[i][sorted_cap_indices]\n",
        "        if cfg.CUDA:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    captions = captions[sorted_cap_indices].squeeze()\n",
        "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
        "    # sent_indices = sent_indices[sorted_cap_indices]\n",
        "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
        "    # print('keys', type(keys), keys[-1])  # list\n",
        "    if cfg.CUDA:\n",
        "        captions = Variable(captions).cuda()\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n",
        "    else:\n",
        "        captions = Variable(captions)\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens)\n",
        "\n",
        "    return [real_imgs, captions, sorted_cap_lens,\n",
        "            class_ids, keys]\n",
        "\n",
        "\n",
        "def get_imgs(img_path, imsize, bbox=None,\n",
        "             transform=None, normalize=None):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - r)\n",
        "        y2 = np.minimum(height, center_y + r)\n",
        "        x1 = np.maximum(0, center_x - r)\n",
        "        x2 = np.minimum(width, center_x + r)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img)\n",
        "\n",
        "    ret = []\n",
        "    if cfg.GAN.B_DCGAN:\n",
        "        ret = [normalize(img)]\n",
        "    else:\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            # print(imsize[i])\n",
        "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
        "                re_img = transforms.Scale(imsize[i])(img)\n",
        "            else:\n",
        "                re_img = img\n",
        "            ret.append(normalize(re_img))\n",
        "\n",
        "    return ret\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SngQkRYCxn5"
      },
      "source": [
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train',\n",
        "                 base_size=64,\n",
        "                 transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
        "\n",
        "        self.imsize = []\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            self.imsize.append(base_size)\n",
        "            base_size = base_size * 2\n",
        "\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        if data_dir.find('birds') != -1:\n",
        "            self.bbox = self.load_bbox()\n",
        "        else:\n",
        "            self.bbox = None\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames, self.captions, self.ixtoword, \\\n",
        "            self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
        "\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames))\n",
        "        self.number_example = len(self.filenames)\n",
        "\n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        #\n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = \\\n",
        "            pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        print('Total filenames: ', len(filenames), filenames[0])\n",
        "        #\n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        #\n",
        "        return filename_bbox\n",
        "\n",
        "    def load_captions(self, data_dir, filenames):\n",
        "        all_captions = []\n",
        "        for i in range(len(filenames)):\n",
        "            cap_path = '%s/text_c10/%s.txt' % (data_dir, filenames[i])\n",
        "            with open(cap_path, \"r\") as f:\n",
        "                captions = f.read().split('\\n')\n",
        "                cnt = 0\n",
        "                for cap in captions:\n",
        "                    if len(cap) == 0:\n",
        "                        continue\n",
        "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    # picks out sequences of alphanumeric characters as tokens\n",
        "                    # and drops everything else\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(cap.lower())\n",
        "                    # print('tokens', tokens)\n",
        "                    if len(tokens) == 0:\n",
        "                        print('cap', cap)\n",
        "                        continue\n",
        "\n",
        "                    tokens_new = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0:\n",
        "                            tokens_new.append(t)\n",
        "                    all_captions.append(tokens_new)\n",
        "                    cnt += 1\n",
        "                    if cnt == self.embeddings_num:\n",
        "                        break\n",
        "                if cnt < self.embeddings_num:\n",
        "                    print('ERROR: the captions for %s less than %d'\n",
        "                          % (filenames[i], cnt))\n",
        "        return all_captions\n",
        "\n",
        "    def build_dictionary(self, train_captions, test_captions):\n",
        "        word_counts = defaultdict(float)\n",
        "        captions = train_captions + test_captions\n",
        "        for sent in captions:\n",
        "            for word in sent:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
        "\n",
        "        ixtoword = {}\n",
        "        ixtoword[0] = '<end>'\n",
        "        wordtoix = {}\n",
        "        wordtoix['<end>'] = 0\n",
        "        ix = 1\n",
        "        for w in vocab:\n",
        "            wordtoix[w] = ix\n",
        "            ixtoword[ix] = w\n",
        "            ix += 1\n",
        "\n",
        "        train_captions_new = []\n",
        "        for t in train_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            train_captions_new.append(rev)\n",
        "\n",
        "        test_captions_new = []\n",
        "        for t in test_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            test_captions_new.append(rev)\n",
        "\n",
        "        return [train_captions_new, test_captions_new,\n",
        "                ixtoword, wordtoix, len(ixtoword)]\n",
        "\n",
        "    def load_text_data(self, data_dir, split):\n",
        "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
        "        train_names = self.load_filenames(data_dir, 'train')\n",
        "        test_names = self.load_filenames(data_dir, 'test')\n",
        "        if not os.path.isfile(filepath):\n",
        "            train_captions = self.load_captions(data_dir, train_names)\n",
        "            test_captions = self.load_captions(data_dir, test_names)\n",
        "\n",
        "            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n",
        "                self.build_dictionary(train_captions, test_captions)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump([train_captions, test_captions,\n",
        "                             ixtoword, wordtoix], f, protocol=2)\n",
        "                print('Save to: ', filepath)\n",
        "        else:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                x = pickle.load(f)\n",
        "                train_captions, test_captions = x[0], x[1]\n",
        "                ixtoword, wordtoix = x[2], x[3]\n",
        "                del x\n",
        "                n_words = len(ixtoword)\n",
        "                #print('Load from: ', filepath)\n",
        "        if split == 'train':\n",
        "            # a list of list: each list contains\n",
        "            # the indices of words in a sentence\n",
        "            captions = train_captions\n",
        "            filenames = train_names\n",
        "        else:  # split=='test'\n",
        "            captions = test_captions\n",
        "            filenames = test_names\n",
        "        return filenames, captions, ixtoword, wordtoix, n_words\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f,encoding='bytes')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir, split):\n",
        "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                filenames = pickle.load(f)\n",
        "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        else:\n",
        "            filenames = []\n",
        "        return filenames\n",
        "\n",
        "    def get_caption(self, sent_ix):\n",
        "        # a list of indices for a sentence\n",
        "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
        "        if (sent_caption == 0).sum() > 0:\n",
        "            print('ERROR: do not need END (0) token', sent_caption)\n",
        "        num_words = len(sent_caption)\n",
        "        # pad with 0s (i.e., '<end>')\n",
        "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
        "        x_len = num_words\n",
        "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
        "            x[:num_words, 0] = sent_caption\n",
        "        else:\n",
        "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
        "            np.random.shuffle(ix)\n",
        "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
        "            ix = np.sort(ix)\n",
        "            x[:, 0] = sent_caption[ix]\n",
        "            x_len = cfg.TEXT.WORDS_NUM\n",
        "        return x, x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        key = self.filenames[index]\n",
        "        cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "        #\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        imgs = get_imgs(img_name, self.imsize,\n",
        "                        bbox, self.transform, normalize=self.norm)\n",
        "        # random select a sentence\n",
        "        sent_ix = random.randint(0, self.embeddings_num)\n",
        "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
        "        caps, cap_len = self.get_caption(new_sent_ix)\n",
        "        return imgs, caps, cap_len, cls_id, key\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0KtNYKlve-b"
      },
      "source": [
        "def save_(r,f,caption,fullpath):\n",
        "#im = cv2.imwrite(fullpath,r[0])\n",
        "    fullpath=fullpath\n",
        "    title_font = ImageFont.load_default()\n",
        "    im1 = Image.fromarray(np.uint8(f[0])).convert('RGB')\n",
        "    im2 = Image.fromarray(np.uint8(r[0])).convert('RGB')\n",
        "    im1=ImageOps.expand(im1,border=10,fill='white')\n",
        "    im2=ImageOps.expand(im2,border=10,fill='white')\n",
        "    image_editable = ImageDraw.Draw(im1)\n",
        "    W,H=im1.size\n",
        "    text = 'Stage 1'\n",
        "    w,h=image_editable.textsize(text)\n",
        "    image_editable.text(((W-w)/2,(H-h)), text, (0,0,0), font=title_font)\n",
        "    image_editable = ImageDraw.Draw(im2)\n",
        "    W,H=im2.size\n",
        "    text = 'Stage 2'\n",
        "    w,h=image_editable.textsize(text)\n",
        "    image_editable.text(((W-w)/2,(H-h)), text, (0,0,0), font=title_font)\n",
        "    im=np.concatenate((np.uint8(im1),np.uint8(im2)),axis=1)\n",
        "    im = Image.fromarray(np.uint8(im)).convert('RGB')\n",
        "    im = ImageOps.expand(im,border=30,fill='white')\n",
        "    W,H=im.size\n",
        "\n",
        "    title_text = caption#\"a colorful bird with a bright yellow body, a black crown and throat, orange bill, and black primaries and secondaries.\"\n",
        "    image_editable = ImageDraw.Draw(im)\n",
        "    lines = textwrap.wrap(title_text, width=60)\n",
        "    ini = 1\n",
        "    for line in lines:\n",
        "        w, h = image_editable.textsize(line)\n",
        "        image_editable.text(((W-w)/2,ini*h), line, (0,0,0), font=title_font)\n",
        "        ini+=1\n",
        "    # w, h = image_editable.textsize(title_text)\n",
        "    #image_editable.text(((W-w)/2,(H-h)*0.1), title_text, (0,0,0), font=title_font)\n",
        "    print(fullpath)\n",
        "    im.save(fullpath)\n",
        "#caption=\"a colorful bird with a bright yellow body, a black crown and throat, orange bill, and black primaries and secondaries.\"\n",
        "\n",
        "\n",
        "def build_super_images3(real_imgs, captions, ixtoword,\n",
        "                       attn_maps, att_sze, lr_imgs=None,\n",
        "                       batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                       max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    \n",
        "    nvis = 1\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    # text_convas = \\\n",
        "    #     np.ones([batch_size * FONT_MAX,\n",
        "    #              (max_word_num + 2) * (vis_size + 2), 3],\n",
        "    #             dtype=np.uint8)\n",
        "\n",
        "    # for i in range(max_word_num):\n",
        "    #     istart = (i + 2) * (vis_size + 2)\n",
        "    #     iend = (i + 3) * (vis_size + 2)\n",
        "    #     text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "    return [real_imgs,lr_imgs]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yRaNsSDV7EM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCLjvmeK024s"
      },
      "source": [
        "model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0curPjZ_2rBL"
      },
      "source": [
        "import torch.nn.parallel\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# from miscc.config import cfg\n",
        "# from GlobalAttention import GlobalAttentionGeneral as ATT_NET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6Fgconm2rEM"
      },
      "source": [
        "def conv1x1(in_planes, out_planes, bias=False):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "# Upsale the spatial size by a factor of 2\n",
        "def upBlock(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "        conv3x3(in_planes, out_planes * 2),\n",
        "        nn.BatchNorm2d(out_planes * 2),\n",
        "        GLU())\n",
        "    return block\n",
        "\n",
        "\n",
        "# Keep the spatial size\n",
        "def Block3x3_relu(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        conv3x3(in_planes, out_planes * 2),\n",
        "        nn.BatchNorm2d(out_planes * 2),\n",
        "        GLU())\n",
        "    return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABNQ5Ejg2rHa"
      },
      "source": [
        "class GLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GLU, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        nc = x.size(1)\n",
        "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
        "        nc = int(nc/2)\n",
        "        return x[:, :nc] * F.sigmoid(x[:, nc:])\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channel_num):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            conv3x3(channel_num, channel_num * 2),\n",
        "            nn.BatchNorm2d(channel_num * 2),\n",
        "            GLU(),\n",
        "            conv3x3(channel_num, channel_num),\n",
        "            nn.BatchNorm2d(channel_num))\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.block(x)\n",
        "        out += residual\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSMXgbzb2rLz"
      },
      "source": [
        "# ############## Text2Image Encoder-Decoder #######\n",
        "class RNN_ENCODER(nn.Module):\n",
        "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
        "                 nhidden=128, nlayers=1, bidirectional=True):\n",
        "        super(RNN_ENCODER, self).__init__()\n",
        "        self.n_steps = cfg.TEXT.WORDS_NUM\n",
        "        self.ntoken = ntoken  # size of the dictionary\n",
        "        self.ninput = ninput  # size of each embedding vector\n",
        "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
        "        self.nlayers = nlayers  # Number of recurrent layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn_type = cfg.RNN_TYPE\n",
        "        if bidirectional:\n",
        "            self.num_directions = 2\n",
        "        else:\n",
        "            self.num_directions = 1\n",
        "        # number of features in the hidden state\n",
        "        self.nhidden = nhidden // self.num_directions\n",
        "\n",
        "        self.define_module()\n",
        "        self.init_weights()\n",
        "\n",
        "    def define_module(self):\n",
        "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                               self.nlayers, batch_first=True,\n",
        "                               dropout=self.drop_prob,\n",
        "                               bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                              self.nlayers, batch_first=True,\n",
        "                              dropout=self.drop_prob,\n",
        "                              bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()),\n",
        "                    Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()))\n",
        "        else:\n",
        "            return Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                       bsz, self.nhidden).zero_())\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        emb = self.drop(self.encoder(captions))\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "        return words_emb, sent_emb\n",
        "# class BERT_RNN_ENCODER(RNN_ENCODER):\n",
        "#     def define_module(self):\n",
        "#         self.encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "#         for param in self.encoder.parameters():\n",
        "#             param.requires_grad = False\n",
        "#         self.bert_linear = nn.Linear(768, self.ninput)\n",
        "#         self.drop = nn.Dropout(self.drop_prob)\n",
        "#         if self.rnn_type == 'LSTM':\n",
        "#             # dropout: If non-zero, introduces a dropout layer on\n",
        "#             # the outputs of each RNN layer except the last layer\n",
        "#             self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "#                                self.nlayers, batch_first=True,\n",
        "#                                dropout=self.drop_prob,\n",
        "#                                bidirectional=self.bidirectional)\n",
        "#         elif self.rnn_type == 'GRU':\n",
        "#             self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "#                               self.nlayers, batch_first=True,\n",
        "#                               dropout=self.drop_prob,\n",
        "#                               bidirectional=self.bidirectional)\n",
        "#         else:\n",
        "#             raise NotImplementedError\n",
        "\n",
        "#     def init_weights(self):\n",
        "#         initrange = 0.1\n",
        "#         self.bert_linear.weight.data.uniform_(-initrange, initrange)\n",
        "#         # Do not need to initialize RNN parameters, which have been initialized\n",
        "#         # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "#         # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "#         # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "#     def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "#         # input: torch.LongTensor of size batch x n_steps\n",
        "#         # --> emb: batch x n_steps x ninput\n",
        "        \n",
        "#         emb, _ = self.encoder(captions, output_all_encoded_layers=False)\n",
        "#         emb = self.bert_linear(emb)\n",
        "#         emb = self.drop(emb)\n",
        "#         #\n",
        "#         # Returns: a PackedSequence object\n",
        "#         cap_lens = cap_lens.data.tolist()\n",
        "#         emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "#         # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "#         # tensor containing the initial hidden state for each element in batch.\n",
        "#         # #output (batch, seq_len, hidden_size * num_directions)\n",
        "#         # #or a PackedSequence object:\n",
        "#         # tensor containing output features (h_t) from the last layer of RNN\n",
        "#         output, hidden = self.rnn(emb, hidden)\n",
        "#         # PackedSequence object\n",
        "#         # --> (batch, seq_len, hidden_size * num_directions)\n",
        "#         output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "#         # output = self.drop(output)\n",
        "#         # --> batch x hidden_size*num_directions x seq_len\n",
        "#         words_emb = output.transpose(1, 2)\n",
        "#         # --> batch x num_directions*hidden_size\n",
        "#         if self.rnn_type == 'LSTM':\n",
        "#             sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "#         else:\n",
        "#             sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "#         sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "#         return words_emb, sent_emb\n",
        "\n",
        "class GPT2_RNN_ENCODER(RNN_ENCODER):\n",
        "    def define_module(self):\n",
        "        self.encoder = GPT2Model.from_pretrained('gpt2')\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.gpt2_linear = nn.Linear(768, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                               self.nlayers, batch_first=True,\n",
        "                               dropout=self.drop_prob,\n",
        "                               bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                              self.nlayers, batch_first=True,\n",
        "                              dropout=self.drop_prob,\n",
        "                              bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.gpt2_linear.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        \n",
        "        emb, _ = self.encoder(captions)\n",
        "        emb = self.gpt2_linear(emb)\n",
        "        emb = self.drop(emb)\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "        return words_emb, sent_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-10BLTN52rSX"
      },
      "source": [
        "class CNN_ENCODER(nn.Module):\n",
        "    def __init__(self, nef):\n",
        "        super(CNN_ENCODER, self).__init__()\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.nef = nef\n",
        "        else:\n",
        "            self.nef = 256  # define a uniform ranker\n",
        "\n",
        "        model = models.inception_v3()\n",
        "        # url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        # model.load_state_dict(model_zoo.load_url(url))\n",
        "        state_dict = torch.load('/content/drive/My Drive/data/inception_v3.pth')\n",
        "        model.load_state_dict(state_dict)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        # print('Load pretrained model from ', url)\n",
        "        # print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
        "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
        "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
        "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
        "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
        "        self.Mixed_5b = model.Mixed_5b\n",
        "        self.Mixed_5c = model.Mixed_5c\n",
        "        self.Mixed_5d = model.Mixed_5d\n",
        "        self.Mixed_6a = model.Mixed_6a\n",
        "        self.Mixed_6b = model.Mixed_6b\n",
        "        self.Mixed_6c = model.Mixed_6c\n",
        "        self.Mixed_6d = model.Mixed_6d\n",
        "        self.Mixed_6e = model.Mixed_6e\n",
        "        self.Mixed_7a = model.Mixed_7a\n",
        "        self.Mixed_7b = model.Mixed_7b\n",
        "        self.Mixed_7c = model.Mixed_7c\n",
        "\n",
        "        self.emb_features = conv1x1(768, self.nef)\n",
        "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
        "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = None\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        # image region features\n",
        "        features = x\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.Mixed_7c(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = F.avg_pool2d(x, kernel_size=8)\n",
        "        # 1 x 1 x 2048\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # 1 x 1 x 2048\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 2048\n",
        "\n",
        "        # global image features\n",
        "        cnn_code = self.emb_cnn_code(x)\n",
        "        # 512\n",
        "        if features is not None:\n",
        "            features = self.emb_features(features)\n",
        "        return features, cnn_code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN2tmBTR2bvX"
      },
      "source": [
        "# ############## G networks ###################\n",
        "class CA_NET(nn.Module):\n",
        "    # some code is modified from vae examples\n",
        "    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n",
        "    def __init__(self):\n",
        "        super(CA_NET, self).__init__()\n",
        "        self.t_dim = cfg.TEXT.EMBEDDING_DIM\n",
        "        self.c_dim = cfg.GAN.CONDITION_DIM\n",
        "        self.fc = nn.Linear(self.t_dim, self.c_dim * 4, bias=True)\n",
        "        self.relu = GLU()\n",
        "\n",
        "    def encode(self, text_embedding):\n",
        "        x = self.relu(self.fc(text_embedding))\n",
        "        mu = x[:, :self.c_dim]\n",
        "        logvar = x[:, self.c_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparametrize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        if cfg.CUDA:\n",
        "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
        "        else:\n",
        "            eps = torch.FloatTensor(std.size()).normal_()\n",
        "        eps = Variable(eps)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def forward(self, text_embedding):\n",
        "        mu, logvar = self.encode(text_embedding)\n",
        "        c_code = self.reparametrize(mu, logvar)\n",
        "        return c_code, mu, logvar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87KQKYBS2byk"
      },
      "source": [
        "class INIT_STAGE_G(nn.Module):\n",
        "    def __init__(self, ngf, ncf):\n",
        "        super(INIT_STAGE_G, self).__init__()\n",
        "        self.gf_dim = ngf\n",
        "        self.in_dim = cfg.GAN.Z_DIM + ncf  # cfg.TEXT.EMBEDDING_DIM\n",
        "\n",
        "        self.define_module()\n",
        "\n",
        "    def define_module(self):\n",
        "        nz, ngf = self.in_dim, self.gf_dim\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(nz, ngf * 4 * 4 * 2, bias=False),\n",
        "            nn.BatchNorm1d(ngf * 4 * 4 * 2),\n",
        "            GLU())\n",
        "\n",
        "        self.upsample1 = upBlock(ngf, ngf // 2)\n",
        "        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n",
        "        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n",
        "        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n",
        "\n",
        "    def forward(self, z_code, c_code):\n",
        "        \"\"\"\n",
        "        :param z_code: batch x cfg.GAN.Z_DIM\n",
        "        :param c_code: batch x cfg.TEXT.EMBEDDING_DIM\n",
        "        :return: batch x ngf/16 x 64 x 64\n",
        "        \"\"\"\n",
        "        c_z_code = torch.cat((c_code, z_code), 1)\n",
        "        # state size ngf x 4 x 4\n",
        "        out_code = self.fc(c_z_code)\n",
        "        out_code = out_code.view(-1, self.gf_dim, 4, 4)\n",
        "        # state size ngf/3 x 8 x 8\n",
        "        out_code = self.upsample1(out_code)\n",
        "        # state size ngf/4 x 16 x 16\n",
        "        out_code = self.upsample2(out_code)\n",
        "        # state size ngf/8 x 32 x 32\n",
        "        out_code32 = self.upsample3(out_code)\n",
        "        # state size ngf/16 x 64 x 64\n",
        "        out_code64 = self.upsample4(out_code32)\n",
        "\n",
        "        return out_code64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbmfLNPt2b1S"
      },
      "source": [
        "class NEXT_STAGE_G(nn.Module):\n",
        "    def __init__(self, ngf, nef, ncf):\n",
        "        super(NEXT_STAGE_G, self).__init__()\n",
        "        self.gf_dim = ngf\n",
        "        self.ef_dim = nef\n",
        "        self.cf_dim = ncf\n",
        "        self.num_residual = cfg.GAN.R_NUM\n",
        "        self.define_module()\n",
        "\n",
        "    def _make_layer(self, block, channel_num):\n",
        "        layers = []\n",
        "        for i in range(cfg.GAN.R_NUM):\n",
        "            layers.append(block(channel_num))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def define_module(self):\n",
        "        ngf = self.gf_dim\n",
        "        # ATT_NET = GlobalAttentionGeneral()\n",
        "        self.att = GlobalAttentionGeneral(ngf, self.ef_dim)\n",
        "        self.residual = self._make_layer(ResBlock, ngf * 2)\n",
        "        self.upsample = upBlock(ngf * 2, ngf)\n",
        "\n",
        "    def forward(self, h_code, c_code, word_embs, mask):\n",
        "        \"\"\"\n",
        "            h_code1(query):  batch x idf x ih x iw (queryL=ihxiw)\n",
        "            word_embs(context): batch x cdf x sourceL (sourceL=seq_len)\n",
        "            c_code1: batch x idf x queryL\n",
        "            att1: batch x sourceL x queryL\n",
        "        \"\"\"\n",
        "        self.att.applyMask(mask)\n",
        "        c_code, att = self.att(h_code, word_embs)\n",
        "        h_c_code = torch.cat((h_code, c_code), 1)\n",
        "        out_code = self.residual(h_c_code)\n",
        "\n",
        "        # state size ngf/2 x 2in_size x 2in_size\n",
        "        out_code = self.upsample(out_code)\n",
        "\n",
        "        return out_code, att"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy8ZFi1S2cXm"
      },
      "source": [
        "class GET_IMAGE_G(nn.Module):\n",
        "    def __init__(self, ngf):\n",
        "        super(GET_IMAGE_G, self).__init__()\n",
        "        self.gf_dim = ngf\n",
        "        self.img = nn.Sequential(\n",
        "            conv3x3(ngf, 3),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, h_code):\n",
        "        out_img = self.img(h_code)\n",
        "        return out_img\n",
        "\n",
        "\n",
        "class G_NET(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(G_NET, self).__init__()\n",
        "        ngf = cfg.GAN.GF_DIM\n",
        "        nef = cfg.TEXT.EMBEDDING_DIM\n",
        "        ncf = cfg.GAN.CONDITION_DIM\n",
        "        self.ca_net = CA_NET()\n",
        "\n",
        "        if cfg.TREE.BRANCH_NUM > 0:\n",
        "            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf)\n",
        "            self.img_net1 = GET_IMAGE_G(ngf)\n",
        "        # gf x 64 x 64\n",
        "        if cfg.TREE.BRANCH_NUM > 1:\n",
        "            self.h_net2 = NEXT_STAGE_G(ngf, nef, ncf)\n",
        "            self.img_net2 = GET_IMAGE_G(ngf)\n",
        "        if cfg.TREE.BRANCH_NUM > 2:\n",
        "            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf)\n",
        "            self.img_net3 = GET_IMAGE_G(ngf)\n",
        "\n",
        "    def forward(self, z_code, sent_emb, word_embs, mask):\n",
        "        \"\"\"\n",
        "            :param z_code: batch x cfg.GAN.Z_DIM\n",
        "            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n",
        "            :param word_embs: batch x cdf x seq_len\n",
        "            :param mask: batch x seq_len\n",
        "            :return:\n",
        "        \"\"\"\n",
        "        fake_imgs = []\n",
        "        att_maps = []\n",
        "        c_code, mu, logvar = self.ca_net(sent_emb)\n",
        "\n",
        "        if cfg.TREE.BRANCH_NUM > 0:\n",
        "            h_code1 = self.h_net1(z_code, c_code)\n",
        "            fake_img1 = self.img_net1(h_code1)\n",
        "            fake_imgs.append(fake_img1)\n",
        "        if cfg.TREE.BRANCH_NUM > 1:\n",
        "            h_code2, att1 = \\\n",
        "                self.h_net2(h_code1, c_code, word_embs, mask)\n",
        "            fake_img2 = self.img_net2(h_code2)\n",
        "            fake_imgs.append(fake_img2)\n",
        "            if att1 is not None:\n",
        "                att_maps.append(att1)\n",
        "        if cfg.TREE.BRANCH_NUM > 2:\n",
        "            h_code3, att2 = \\\n",
        "                self.h_net3(h_code2, c_code, word_embs, mask)\n",
        "            fake_img3 = self.img_net3(h_code3)\n",
        "            fake_imgs.append(fake_img3)\n",
        "            if att2 is not None:\n",
        "                att_maps.append(att2)\n",
        "\n",
        "        return fake_imgs, att_maps, mu, logvar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV2dxetJ2HTv"
      },
      "source": [
        "class G_DCGAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(G_DCGAN, self).__init__()\n",
        "        ngf = cfg.GAN.GF_DIM\n",
        "        nef = cfg.TEXT.EMBEDDING_DIM\n",
        "        ncf = cfg.GAN.CONDITION_DIM\n",
        "        self.ca_net = CA_NET()\n",
        "\n",
        "        # 16gf x 64 x 64 --> gf x 64 x 64 --> 3 x 64 x 64\n",
        "        if cfg.TREE.BRANCH_NUM > 0:\n",
        "            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf)\n",
        "        # gf x 64 x 64\n",
        "        if cfg.TREE.BRANCH_NUM > 1:\n",
        "            self.h_net2 = NEXT_STAGE_G(ngf, nef, ncf)\n",
        "        if cfg.TREE.BRANCH_NUM > 2:\n",
        "            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf)\n",
        "        self.img_net = GET_IMAGE_G(ngf)\n",
        "\n",
        "    def forward(self, z_code, sent_emb, word_embs, mask):\n",
        "        \"\"\"\n",
        "            :param z_code: batch x cfg.GAN.Z_DIM\n",
        "            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n",
        "            :param word_embs: batch x cdf x seq_len\n",
        "            :param mask: batch x seq_len\n",
        "            :return:\n",
        "        \"\"\"\n",
        "        att_maps = []\n",
        "        c_code, mu, logvar = self.ca_net(sent_emb)\n",
        "        if cfg.TREE.BRANCH_NUM > 0:\n",
        "            h_code = self.h_net1(z_code, c_code)\n",
        "        if cfg.TREE.BRANCH_NUM > 1:\n",
        "            h_code, att1 = self.h_net2(h_code, c_code, word_embs, mask)\n",
        "            if att1 is not None:\n",
        "                att_maps.append(att1)\n",
        "        if cfg.TREE.BRANCH_NUM > 2:\n",
        "            h_code, att2 = self.h_net3(h_code, c_code, word_embs, mask)\n",
        "            if att2 is not None:\n",
        "                att_maps.append(att2)\n",
        "\n",
        "        fake_imgs = self.img_net(h_code)\n",
        "        return [fake_imgs], att_maps, mu, logvar\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYzyjDAc2HaM"
      },
      "source": [
        "\n",
        "# ############## D networks ##########################\n",
        "def Block3x3_leakRelu(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        conv3x3(in_planes, out_planes),\n",
        "        nn.BatchNorm2d(out_planes),\n",
        "        nn.LeakyReLU(0.2, inplace=True)\n",
        "    )\n",
        "    return block\n",
        "\n",
        "\n",
        "# Downsale the spatial size by a factor of 2\n",
        "def downBlock(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        nn.Conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_planes),\n",
        "        nn.LeakyReLU(0.2, inplace=True)\n",
        "    )\n",
        "    return block\n",
        "\n",
        "\n",
        "# Downsale the spatial size by a factor of 16\n",
        "def encode_image_by_16times(ndf):\n",
        "    encode_img = nn.Sequential(\n",
        "        # --> state size. ndf x in_size/2 x in_size/2\n",
        "        nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        # --> state size 2ndf x x in_size/4 x in_size/4\n",
        "        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(ndf * 2),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        # --> state size 4ndf x in_size/8 x in_size/8\n",
        "        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(ndf * 4),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        # --> state size 8ndf x in_size/16 x in_size/16\n",
        "        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(ndf * 8),\n",
        "        nn.LeakyReLU(0.2, inplace=True)\n",
        "    )\n",
        "    return encode_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgYm9o8m2HeF"
      },
      "source": [
        "class D_GET_LOGITS(nn.Module):\n",
        "    def __init__(self, ndf, nef, bcondition=False):\n",
        "        super(D_GET_LOGITS, self).__init__()\n",
        "        self.df_dim = ndf\n",
        "        self.ef_dim = nef\n",
        "        self.bcondition = bcondition\n",
        "        if self.bcondition:\n",
        "            self.jointConv = Block3x3_leakRelu(ndf * 8 + nef, ndf * 8)\n",
        "\n",
        "        self.outlogits = nn.Sequential(\n",
        "            nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, h_code, c_code=None):\n",
        "        if self.bcondition and c_code is not None:\n",
        "            # conditioning output\n",
        "            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
        "            c_code = c_code.repeat(1, 1, 4, 4)\n",
        "            # state size (ngf+egf) x 4 x 4\n",
        "            h_c_code = torch.cat((h_code, c_code), 1)\n",
        "            # state size ngf x in_size x in_size\n",
        "            h_c_code = self.jointConv(h_c_code)\n",
        "        else:\n",
        "            h_c_code = h_code\n",
        "\n",
        "        output = self.outlogits(h_c_code)\n",
        "        return output.view(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDvpPIGv2Hht"
      },
      "source": [
        "# For 64 x 64 images\n",
        "class D_NET64(nn.Module):\n",
        "    def __init__(self, b_jcu=True):\n",
        "        super(D_NET64, self).__init__()\n",
        "        ndf = cfg.GAN.DF_DIM\n",
        "        nef = cfg.TEXT.EMBEDDING_DIM\n",
        "        self.img_code_s16 = encode_image_by_16times(ndf)\n",
        "        if b_jcu:\n",
        "            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
        "        else:\n",
        "            self.UNCOND_DNET = None\n",
        "        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
        "\n",
        "    def forward(self, x_var):\n",
        "        x_code4 = self.img_code_s16(x_var)  # 4 x 4 x 8df\n",
        "        return x_code4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRqev0vV2HuN"
      },
      "source": [
        "# For 128 x 128 images\n",
        "class D_NET128(nn.Module):\n",
        "    def __init__(self, b_jcu=True):\n",
        "        super(D_NET128, self).__init__()\n",
        "        ndf = cfg.GAN.DF_DIM\n",
        "        nef = cfg.TEXT.EMBEDDING_DIM\n",
        "        self.img_code_s16 = encode_image_by_16times(ndf)\n",
        "        self.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n",
        "        self.img_code_s32_1 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n",
        "        #\n",
        "        if b_jcu:\n",
        "            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
        "        else:\n",
        "            self.UNCOND_DNET = None\n",
        "        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
        "\n",
        "    def forward(self, x_var):\n",
        "        x_code8 = self.img_code_s16(x_var)   # 8 x 8 x 8df\n",
        "        x_code4 = self.img_code_s32(x_code8)   # 4 x 4 x 16df\n",
        "        x_code4 = self.img_code_s32_1(x_code4)  # 4 x 4 x 8df\n",
        "        return x_code4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ABG5XVa2HzL"
      },
      "source": [
        "# For 256 x 256 images\n",
        "class D_NET256(nn.Module):\n",
        "    def __init__(self, b_jcu=True):\n",
        "        super(D_NET256, self).__init__()\n",
        "        ndf = cfg.GAN.DF_DIM\n",
        "        nef = cfg.TEXT.EMBEDDING_DIM\n",
        "        self.img_code_s16 = encode_image_by_16times(ndf)\n",
        "        self.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n",
        "        self.img_code_s64 = downBlock(ndf * 16, ndf * 32)\n",
        "        self.img_code_s64_1 = Block3x3_leakRelu(ndf * 32, ndf * 16)\n",
        "        self.img_code_s64_2 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n",
        "        if b_jcu:\n",
        "            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
        "        else:\n",
        "            self.UNCOND_DNET = None\n",
        "        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
        "\n",
        "    def forward(self, x_var):\n",
        "        x_code16 = self.img_code_s16(x_var)\n",
        "        x_code8 = self.img_code_s32(x_code16)\n",
        "        x_code4 = self.img_code_s64(x_code8)\n",
        "        x_code4 = self.img_code_s64_1(x_code4)\n",
        "        x_code4 = self.img_code_s64_2(x_code4)\n",
        "        return x_code4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iByj0nS1A3G"
      },
      "source": [
        "trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vaFFVVI_iEX"
      },
      "source": [
        "from __future__ import print_function\n",
        "from six.moves import range\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD6CtKg_PUIW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-b-vl1z0_LW"
      },
      "source": [
        "# ################# Text to image task############################ #\n",
        "class condGANTrainer(object):\n",
        "    def __init__(self, output_dir, data_loader, n_words, ixtoword):\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.model_dir = os.path.join(output_dir, 'Model')\n",
        "            self.image_dir = os.path.join(output_dir, 'Image')\n",
        "            mkdir_p(self.model_dir)\n",
        "            mkdir_p(self.image_dir)\n",
        "\n",
        "        torch.cuda.set_device(cfg.GPU_ID)\n",
        "        cudnn.benchmark = True\n",
        "        self.model_dir = os.path.join(output_dir, 'Model')\n",
        "        self.image_dir = os.path.join(output_dir, 'Image')\n",
        "        self.batch_size = cfg.TRAIN.BATCH_SIZE\n",
        "        self.max_epoch = cfg.TRAIN.MAX_EPOCH\n",
        "        self.snapshot_interval = cfg.TRAIN.SNAPSHOT_INTERVAL\n",
        "        self.predictions=[]\n",
        "        self.models_loaded = False\n",
        "        self.n_words = n_words\n",
        "        self.ixtoword = ixtoword\n",
        "        self.data_loader = data_loader\n",
        "        self.num_batches = len(self.data_loader)\n",
        "        #self.text_encoder, self.image_encoder, self.netG, self.netsD, self.start_epoch = self.build_models()\n",
        "        \n",
        "\n",
        "    def build_models(self):\n",
        "        # ###################encoders######################################## #\n",
        "        \n",
        "        if cfg.TRAIN.NET_E == '':\n",
        "            print('Error: no pretrained text-image encoders')\n",
        "            return\n",
        "        \n",
        "        image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "        cfg.TRAIN.NET_E = '/content/data/DAMSMencoders/bird/gpt2_text_encoder200.pth'\n",
        "        img_encoder_path = cfg.TRAIN.NET_E.replace('gpt2_text_encoder', 'image_encoder')\n",
        "        # img_encoder_path = '/content/data/DAMSMencoders/bird/image_encoder200.pth'\n",
        "      \n",
        "        state_dict = \\\n",
        "            torch.load(img_encoder_path, map_location=lambda storage, loc: storage)\n",
        "        image_encoder.load_state_dict(state_dict)\n",
        "        for p in image_encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "        print('Load image encoder from:', img_encoder_path)\n",
        "        image_encoder.eval()\n",
        "\n",
        "        text_encoder = \\\n",
        "            GPT2_RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "        state_dict = \\\n",
        "            torch.load(cfg.TRAIN.NET_E,\n",
        "                       map_location=lambda storage, loc: storage)\n",
        "        text_encoder.load_state_dict(state_dict)\n",
        "        for p in text_encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "        print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
        "        text_encoder.eval()\n",
        "\n",
        "        # #######################generator and discriminators############## #\n",
        "        netsD = []\n",
        "        if cfg.GAN.B_DCGAN:\n",
        "            if cfg.TREE.BRANCH_NUM ==1:\n",
        "                # from model import D_NET64 as D_NET\n",
        "                D_NET = D_NET64\n",
        "            elif cfg.TREE.BRANCH_NUM == 2:\n",
        "                # from model import D_NET128 as D_NET\n",
        "                D_NET = D_NET128\n",
        "            else:  # cfg.TREE.BRANCH_NUM == 3:\n",
        "                # from model import D_NET256 as D_NET\n",
        "                D_NET = D_NET256\n",
        "            # TODO: elif cfg.TREE.BRANCH_NUM > 3:\n",
        "            netG = G_DCGAN()\n",
        "            netsD = [D_NET(b_jcu=False)]\n",
        "        else:\n",
        "            # from model import D_NET64, D_NET128, D_NET256\n",
        "            netG = G_NET()\n",
        "            if cfg.TREE.BRANCH_NUM > 0:\n",
        "                netsD.append(D_NET64())\n",
        "            if cfg.TREE.BRANCH_NUM > 1:\n",
        "                netsD.append(D_NET128())\n",
        "            if cfg.TREE.BRANCH_NUM > 2:\n",
        "                netsD.append(D_NET256())\n",
        "            # TODO: if cfg.TREE.BRANCH_NUM > 3:\n",
        "        netG.apply(weights_init)\n",
        "        # print(netG)\n",
        "        for i in range(len(netsD)):\n",
        "            netsD[i].apply(weights_init)\n",
        "            # print(netsD[i])\n",
        "        print('# of netsD', len(netsD))\n",
        "        #\n",
        "        epoch = 0\n",
        "        if cfg.TRAIN.NET_G != '':\n",
        "            state_dict = \\\n",
        "                torch.load(cfg.TRAIN.NET_G, map_location=lambda storage, loc: storage)\n",
        "            netG.load_state_dict(state_dict)\n",
        "            print('Load G from: ', cfg.TRAIN.NET_G)\n",
        "            istart = cfg.TRAIN.NET_G.rfind('_') + 1\n",
        "            iend = cfg.TRAIN.NET_G.rfind('.')\n",
        "            epoch = cfg.TRAIN.NET_G[istart:iend]\n",
        "            epoch = int(epoch) + 1\n",
        "            if cfg.TRAIN.B_NET_D:\n",
        "                Gname = cfg.TRAIN.NET_G\n",
        "                for i in range(len(netsD)):\n",
        "                    s_tmp = Gname[:Gname.rfind('/')]\n",
        "                    Dname = '%s/netD%d.pth' % (s_tmp, i)\n",
        "                    print('Load D from: ', Dname)\n",
        "                    state_dict = \\\n",
        "                        torch.load(Dname, map_location=lambda storage, loc: storage)\n",
        "                    netsD[i].load_state_dict(state_dict)\n",
        "        # ########################################################### #\n",
        "        if cfg.CUDA:\n",
        "            text_encoder = text_encoder.cuda()\n",
        "            image_encoder = image_encoder.cuda()\n",
        "            netG.cuda()\n",
        "            for i in range(len(netsD)):\n",
        "                netsD[i].cuda()\n",
        "        self.models_loaded=True\n",
        "        return [text_encoder, image_encoder, netG, netsD, epoch]\n",
        "\n",
        "    def define_optimizers(self, netG, netsD):\n",
        "        optimizersD = []\n",
        "        num_Ds = len(netsD)\n",
        "        for i in range(num_Ds):\n",
        "            opt = optim.Adam(netsD[i].parameters(),\n",
        "                             lr=cfg.TRAIN.DISCRIMINATOR_LR,\n",
        "                             betas=(0.5, 0.999))\n",
        "            optimizersD.append(opt)\n",
        "\n",
        "        optimizerG = optim.Adam(netG.parameters(),\n",
        "                                lr=cfg.TRAIN.GENERATOR_LR,\n",
        "                                betas=(0.5, 0.999))\n",
        "\n",
        "        return optimizerG, optimizersD\n",
        "\n",
        "    def prepare_labels(self):\n",
        "        batch_size = self.batch_size\n",
        "        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n",
        "        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n",
        "        match_labels = Variable(torch.LongTensor(range(batch_size)))\n",
        "        if cfg.CUDA:\n",
        "            real_labels = real_labels.cuda()\n",
        "            fake_labels = fake_labels.cuda()\n",
        "            match_labels = match_labels.cuda()\n",
        "\n",
        "        return real_labels, fake_labels, match_labels\n",
        "\n",
        "    def save_model(self, netG, avg_param_G, netsD, epoch):\n",
        "        backup_para = copy_G_params(netG)\n",
        "        load_params(netG, avg_param_G)\n",
        "        torch.save(netG.state_dict(),\n",
        "            '%s/netG_epoch_%d.pth' % (self.model_dir, epoch))\n",
        "        load_params(netG, backup_para)\n",
        "        #\n",
        "        for i in range(len(netsD)):\n",
        "            netD = netsD[i]\n",
        "            torch.save(netD.state_dict(),\n",
        "                '%s/netD%d.pth' % (self.model_dir, i))\n",
        "        print('Save G/Ds models.')\n",
        "\n",
        "    def set_requires_grad_value(self, models_list, brequires):\n",
        "        for i in range(len(models_list)):\n",
        "            for p in models_list[i].parameters():\n",
        "                p.requires_grad = brequires\n",
        "\n",
        "    def save_img_results(self, netG, noise, sent_emb, words_embs, mask,\n",
        "                         image_encoder, captions, cap_lens,\n",
        "                         gen_iterations, name='current'):\n",
        "        # Save images\n",
        "        fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
        "        for i in range(len(attention_maps)):\n",
        "            if len(fake_imgs) > 1:\n",
        "                img = fake_imgs[i + 1].detach().cpu()\n",
        "                lr_img = fake_imgs[i].detach().cpu()\n",
        "            else:\n",
        "                img = fake_imgs[0].detach().cpu()\n",
        "                lr_img = None\n",
        "            attn_maps = attention_maps[i]\n",
        "            att_sze = attn_maps.size(2)\n",
        "            img_set, _ = \\\n",
        "                build_super_images(img, captions, self.ixtoword,\n",
        "                                   attn_maps, att_sze, lr_imgs=lr_img)\n",
        "            if not os.path.exists('/content/drive/My Drive/data/gpt2_output_attn1/Model/naveen/'):#\n",
        "              os.makedirs('/content/drive/My Drive/data/gpt2_output_attn1/Model/naveen/')\n",
        "            if img_set is not None:\n",
        "                im = Image.fromarray(img_set)\n",
        "                fullpath = '%s/G_%s_epoch-%d_%d.png'\\\n",
        "                    % ('/content/drive/My Drive/data/gpt2_output_attn1/Model/naveen/', name, gen_iterations, i)\n",
        "                im.save(fullpath)\n",
        "\n",
        "        # for i in range(len(netsD)):\n",
        "        # i = -1\n",
        "        # img = fake_imgs[i].detach()\n",
        "        # region_features, _ = image_encoder(img)\n",
        "        # att_sze = region_features.size(2)\n",
        "        # _, _, att_maps = words_loss(region_features.detach(),\n",
        "        #                             words_embs.detach(),\n",
        "        #                             None, cap_lens,\n",
        "        #                             None, self.batch_size)\n",
        "        # img_set, _ = \\\n",
        "        #     build_super_images(fake_imgs[i].detach().cpu(),\n",
        "        #                        captions, self.ixtoword, att_maps, att_sze)\n",
        "        # if img_set is not None:\n",
        "        #     im = Image.fromarray(img_set)\n",
        "        #     fullpath = '%s/D_%s_epoch-%d.png'\\\n",
        "        #         % (self.image_dir, name, gen_iterations)\n",
        "        #     im.save(fullpath)\n",
        "    def fid(self):\n",
        "        batch_size =self.batch_size\n",
        "        text_encoder, image_encoder, netG, netsD, start_epoch = self.build_models()\n",
        "        lis = []\n",
        "        nz = cfg.GAN.Z_DIM\n",
        "        noise = Variable(torch.FloatTensor(batch_size, nz))\n",
        "        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1))\n",
        "        if cfg.CUDA:\n",
        "            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
        "        for data in tqdm(iter(dataloader)):\n",
        "          real_imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
        "          real_labels, fake_labels, match_labels = self.prepare_labels()\n",
        "          hidden = text_encoder.init_hidden(batch_size)\n",
        "          words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "          mask = (captions == 0)\n",
        "          num_words = words_embs.size(2)\n",
        "          if mask.size(1) > num_words:\n",
        "              mask = mask[:, :num_words]\n",
        "\n",
        "          #######################################################\n",
        "          # (2) Generate fake images\n",
        "          ######################################################\n",
        "          noise.data.normal_(0, 1)\n",
        "          fake_imgs, _, mu, logvar = netG(noise, sent_emb, words_embs, mask)\n",
        "          for i in range(len(netsD)):\n",
        "            img_features, cnn_code = image_encoder(fake_imgs[i])\n",
        "            fid=Compute_FID(img_features, words_embs, match_labels,\n",
        "                        cap_lens, class_ids, batch_size)\n",
        "            lis.append(fid)\n",
        "          return lis\n",
        "\n",
        "    def train(self):\n",
        "        text_encoder, image_encoder, netG, netsD, start_epoch = self.build_models()\n",
        "        avg_param_G = copy_G_params(netG)\n",
        "        optimizerG, optimizersD = self.define_optimizers(netG, netsD)\n",
        "        real_labels, fake_labels, match_labels = self.prepare_labels()\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        nz = cfg.GAN.Z_DIM\n",
        "        noise = Variable(torch.FloatTensor(batch_size, nz))\n",
        "        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1))\n",
        "        if cfg.CUDA:\n",
        "            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
        "\n",
        "        gen_iterations = 0\n",
        "        # gen_iterations = start_epoch * self.num_batches\n",
        "        for epoch in trange(start_epoch, self.max_epoch):\n",
        "            start_t = time.time()\n",
        "            # print(start_time,\"######################################################################################\")\n",
        "            data_iter = iter(self.data_loader)\n",
        "            \n",
        "            for step in trange(self.num_batches):\n",
        "                # print(step,\"######################################################################################\")\n",
        "                # reset requires_grad to be trainable for all Ds\n",
        "                # self.set_requires_grad_value(netsD, True)\n",
        "\n",
        "                ######################################################\n",
        "                # (1) Prepare training data and Compute text embeddings\n",
        "                ######################################################\n",
        "                data = data_iter.next()\n",
        "                imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
        "\n",
        "                hidden = text_encoder.init_hidden(batch_size)\n",
        "                # words_embs: batch_size x nef x seq_len\n",
        "                # sent_emb: batch_size x nef\n",
        "                words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "                words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n",
        "                mask = (captions == 0)\n",
        "                num_words = words_embs.size(2)\n",
        "                if mask.size(1) > num_words:\n",
        "                    mask = mask[:, :num_words]\n",
        "\n",
        "                #######################################################\n",
        "                # (2) Generate fake images\n",
        "                ######################################################\n",
        "                noise.data.normal_(0, 1)\n",
        "                fake_imgs, _, mu, logvar = netG(noise, sent_emb, words_embs, mask)\n",
        "\n",
        "                #######################################################\n",
        "                # (3) Update D network\n",
        "                ######################################################\n",
        "                errD_total = 0\n",
        "                D_logs = ''\n",
        "                for i in range(len(netsD)):\n",
        "                    netsD[i].zero_grad()\n",
        "                    errD = discriminator_loss(netsD[i], imgs[i], fake_imgs[i],\n",
        "                                              sent_emb, real_labels, fake_labels)\n",
        "                    # backward and update parameters\n",
        "                    errD.backward()\n",
        "                    optimizersD[i].step()\n",
        "                    errD_total += errD\n",
        "                    D_logs += 'errD%d: %.2f ' % (i, errD)\n",
        "\n",
        "                #######################################################\n",
        "                # (4) Update G network: maximize log(D(G(z)))\n",
        "                ######################################################\n",
        "                # compute total loss for training G\n",
        "                \n",
        "                gen_iterations += 1\n",
        "\n",
        "                # do not need to compute gradient for Ds\n",
        "                # self.set_requires_grad_value(netsD, False)\n",
        "                netG.zero_grad()\n",
        "                errG_total, G_logs = \\\n",
        "                    generator_loss(netsD, image_encoder, fake_imgs, real_labels,\n",
        "                                   words_embs, sent_emb, match_labels, cap_lens, class_ids)\n",
        "                kl_loss = KL_loss(mu, logvar)\n",
        "                errG_total += kl_loss\n",
        "                G_logs += 'kl_loss: %.2f ' % kl_loss\n",
        "                # backward and update parameters\n",
        "                errG_total.backward()\n",
        "                optimizerG.step()\n",
        "                for p, avg_p in zip(netG.parameters(), avg_param_G):\n",
        "                    avg_p.mul_(0.999).add_(0.001, p.data)\n",
        "\n",
        "                if gen_iterations % 100 == 0:\n",
        "                    print(D_logs + '\\n' + G_logs)\n",
        "                # save images\n",
        "                # if gen_iterations % 100 == 0:\n",
        "                #     backup_para = copy_G_params(netG)\n",
        "                #     load_params(netG, avg_param_G)\n",
        "                #     self.save_img_results(netG, fixed_noise, sent_emb,\n",
        "                #                           words_embs, mask, image_encoder,\n",
        "                #                           captions, cap_lens, epoch, name='average')\n",
        "                #     load_params(netG, backup_para)\n",
        "                    #\n",
        "                    # self.save_img_results(netG, fixed_noise, sent_emb,\n",
        "                    #                       words_embs, mask, image_encoder,\n",
        "                    #                       captions, cap_lens,\n",
        "                    #                       epoch, name='current')\n",
        "            end_t = time.time()\n",
        "\n",
        "            print('''[%d/%d][%d]\n",
        "                  Loss_D: %.2f Loss_G: %.2f Time: %.2fs'''\n",
        "                  % (epoch, self.max_epoch, self.num_batches,\n",
        "                     errD_total, errG_total,\n",
        "                     end_t - start_t))\n",
        "\n",
        "            #if epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0:  # and epoch != 0:\n",
        "            backup_para = copy_G_params(netG)\n",
        "            load_params(netG, avg_param_G)\n",
        "            self.save_img_results(netG, fixed_noise, sent_emb,\n",
        "                                          words_embs, mask, image_encoder,\n",
        "                                          captions, cap_lens, epoch+1, name='average')\n",
        "            load_params(netG, backup_para)\n",
        "            self.save_model(netG, avg_param_G, netsD, epoch+1)\n",
        "\n",
        "        self.save_model(netG, avg_param_G, netsD, self.max_epoch)\n",
        "\n",
        "    def save_singleimages(self, images, filenames, save_dir,\n",
        "                          split_dir, sentenceID=0):\n",
        "        for i in range(images.size(0)):\n",
        "            s_tmp = '%s/single_samples/%s/%s' %\\\n",
        "                (save_dir, split_dir, filenames[i])\n",
        "            folder = s_tmp[:s_tmp.rfind('/')]\n",
        "            if not os.path.isdir(folder):\n",
        "                print('Make a new folder: ', folder)\n",
        "                mkdir_p(folder)\n",
        "\n",
        "            fullpath = '%s_%d.jpg' % (s_tmp, sentenceID)\n",
        "            # range from [-1, 1] to [0, 1]\n",
        "            # img = (images[i] + 1.0) / 2\n",
        "            img = images[i].add(1).div(2).mul(255).clamp(0, 255).byte()\n",
        "            # range from [0, 1] to [0, 255]\n",
        "            ndarr = img.permute(1, 2, 0).data.cpu().numpy()\n",
        "            im = Image.fromarray(ndarr)\n",
        "            im.save(fullpath)\n",
        "\n",
        "    def sampling(self, split_dir):\n",
        "        if cfg.TRAIN.NET_G == '':\n",
        "            print('Error: the path for morels is not found!')\n",
        "        else:\n",
        "            if split_dir == 'test':\n",
        "                split_dir = 'valid'\n",
        "            # Build and load the generator\n",
        "            if cfg.GAN.B_DCGAN:\n",
        "                netG = G_DCGAN()\n",
        "            else:\n",
        "                netG = G_NET()\n",
        "            netG.apply(weights_init)\n",
        "            netG.cuda()\n",
        "            netG.eval()\n",
        "            #\n",
        "            text_encoder = RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "            state_dict = \\\n",
        "                torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n",
        "            text_encoder.load_state_dict(state_dict)\n",
        "            print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
        "            text_encoder = text_encoder.cuda()\n",
        "            text_encoder.eval()\n",
        "            \n",
        "            # image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "            # cfg.TRAIN.NET_E = '/content/data/DAMSMencoders/bird/text_encoder200.pth'\n",
        "            # img_encoder_path = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "            # # img_encoder_path = '/content/data/DAMSMencoders/bird/image_encoder200.pth'\n",
        "          \n",
        "            # state_dict = \\\n",
        "            #     torch.load(img_encoder_path, map_location=lambda storage, loc: storage)\n",
        "            # image_encoder.load_state_dict(state_dict)\n",
        "            # for p in image_encoder.parameters():\n",
        "            #     p.requires_grad = False\n",
        "            # print('Load image encoder from:', img_encoder_path)\n",
        "            # image_encoder.eval()\n",
        "\n",
        "            batch_size = self.batch_size\n",
        "            nz = cfg.GAN.Z_DIM\n",
        "            noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
        "            noise = noise.cuda()\n",
        "\n",
        "            model_dir = cfg.TRAIN.NET_G\n",
        "            state_dict = \\\n",
        "                torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
        "            # state_dict = torch.load(cfg.TRAIN.NET_G)\n",
        "            netG.load_state_dict(state_dict)\n",
        "            print('Load G from: ', model_dir)\n",
        "\n",
        "            # the path to save generated images\n",
        "            s_tmp = model_dir[:model_dir.rfind('.pth')]\n",
        "            save_dir = '%s/%s' % (s_tmp, split_dir)\n",
        "            mkdir_p(save_dir)\n",
        "\n",
        "            cnt = 0\n",
        "\n",
        "            for _ in range(1):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n",
        "                for step, data in enumerate(self.data_loader, 0):\n",
        "                    cnt += batch_size\n",
        "                    if step % 100 == 0:\n",
        "                        print('step: ', step)\n",
        "                    # if step > 50:\n",
        "                    #     break\n",
        "\n",
        "                    imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
        "\n",
        "                    hidden = text_encoder.init_hidden(batch_size)\n",
        "                    # words_embs: batch_size x nef x seq_len\n",
        "                    # sent_emb: batch_size x nef\n",
        "                    words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "                    words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n",
        "                    mask = (captions == 0)\n",
        "                    num_words = words_embs.size(2)\n",
        "                    if mask.size(1) > num_words:\n",
        "                        mask = mask[:, :num_words]\n",
        "\n",
        "                    #######################################################\n",
        "                    # (2) Generate fake images\n",
        "                    ######################################################\n",
        "                    # self.save_img_results(netG, noise, sent_emb, words_embs, mask,\n",
        "                    #    image_encoder, captions, cap_lens,\n",
        "                    #    step*100000, name='Naveen',directory)\n",
        "                    # continue\n",
        "                    \n",
        "                    noise.data.normal_(0, 1)\n",
        "                    fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
        "                    for i in range(len(attention_maps)):\n",
        "                        #s_tmp = '%s/naveen/%s' % (save_dir, keys[i])\n",
        "                        s_tmp = '/content/%s'%(keys[i])\n",
        "                        folder = s_tmp[:s_tmp.rfind('/')]\n",
        "                        if not os.path.isdir(folder):\n",
        "                            print('Make a new folder: ', folder)\n",
        "                            mkdir_p(folder)\n",
        "                        if len(fake_imgs) > 1:\n",
        "                            img = fake_imgs[i + 1].detach().cpu()\n",
        "                            lr_img = fake_imgs[i].detach().cpu()\n",
        "                        else:\n",
        "                            img = fake_imgs[0].detach().cpu()\n",
        "                            lr_img = None\n",
        "                        attn_maps = attention_maps[i]\n",
        "                        att_sze = attn_maps.size(2)\n",
        "                        img_set, _ = \\\n",
        "                            build_super_images(img, captions, self.ixtoword,\n",
        "                                              attn_maps, att_sze, lr_imgs=lr_img)\n",
        "                        if img_set is not None:\n",
        "                            im = Image.fromarray(img_set)\n",
        "                            fullpath = '%s_s%d.png'\\\n",
        "                                % (s_tmp, i)\n",
        "                            im.save(fullpath)\n",
        "                    #    k = -1\n",
        "                    #     # for k in range(len(fake_imgs)):\n",
        "                    #     im = fake_imgs[k][j].data.cpu().numpy()\n",
        "                    #     # [-1, 1] --> [0, 255]\n",
        "                    #     im = (im + 1.0) * 127.5\n",
        "                    #     im = im.astype(np.uint8)\n",
        "                    #     im = np.transpose(im, (1, 2, 0))\n",
        "                    #     im = Image.fromarray(im)\n",
        "                    #     fullpath = '%s_s%d.png' % (s_tmp, k)\n",
        "                    #     im.save(fullpath)\n",
        "\n",
        "    def gen_example2(self, data_dic):\n",
        "        if cfg.TRAIN.NET_G == '':\n",
        "            print('Error: the path for morels is not found!')\n",
        "        else:\n",
        "            # Build and load the generator\n",
        "            text_encoder = \\\n",
        "                RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "            state_dict = \\\n",
        "                torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n",
        "            text_encoder.load_state_dict(state_dict)\n",
        "            print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
        "            text_encoder = text_encoder.cuda()\n",
        "            text_encoder.eval()\n",
        "\n",
        "            # the path to save generated images\n",
        "            if cfg.GAN.B_DCGAN:\n",
        "                netG = G_DCGAN()\n",
        "            else:\n",
        "                netG = G_NET()\n",
        "            s_tmp = cfg.TRAIN.NET_G[:cfg.TRAIN.NET_G.rfind('.pth')]\n",
        "            model_dir = cfg.TRAIN.NET_G\n",
        "            state_dict = \\\n",
        "                torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
        "            netG.load_state_dict(state_dict)\n",
        "            print('Load G from: ', model_dir)\n",
        "            netG.cuda()\n",
        "            netG.eval()\n",
        "            image = 1\n",
        "            print(len(data_dic))\n",
        "            for key in data_dic:\n",
        "                save_dir = '%s/%s' % (s_tmp, key)\n",
        "                mkdir_p(save_dir)\n",
        "                captions, cap_lens, sorted_indices,texts = data_dic[key]\n",
        "                #print(captions)\n",
        "                #print(data_dic[key])\n",
        "\n",
        "                batch_size = captions.shape[0]\n",
        "                nz = cfg.GAN.Z_DIM\n",
        "                captions = Variable(torch.from_numpy(captions), volatile=True)\n",
        "                cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n",
        "\n",
        "                captions = captions.cuda()\n",
        "                cap_lens = cap_lens.cuda()\n",
        "                \n",
        "                for _ in range(1):  # 16\n",
        "                    noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
        "                    noise = noise.cuda()\n",
        "                    #######################################################\n",
        "                    # (1) Extract text embeddings\n",
        "                    ######################################################\n",
        "                    hidden = text_encoder.init_hidden(batch_size)\n",
        "                    # words_embs: batch_size x nef x seq_len\n",
        "                    # sent_emb: batch_size x nef\n",
        "                    words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "                    mask = (captions == 0)\n",
        "                    #######################################################\n",
        "                    # (2) Generate fake images\n",
        "                    ######################################################\n",
        "                    noise.data.normal_(0, 1)\n",
        "                    fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
        "                    # G attention\n",
        "                    cap_lens_np = cap_lens.cpu().data.numpy()\n",
        "                    for k in range(batch_size):\n",
        "                        for i in range(len(attention_maps)):\n",
        "                            s_tmp = '%s/trails/%s' % (save_dir,'examples')\n",
        "                            folder = s_tmp[:s_tmp.rfind('/')]\n",
        "                            if not os.path.isdir(folder):\n",
        "                                print('Make a new folder: ', folder)\n",
        "                                mkdir_p(folder)\n",
        "                            if len(fake_imgs) > 1:\n",
        "                                img = fake_imgs[i + 1].detach().cpu()\n",
        "                                lr_img = fake_imgs[i].detach().cpu()\n",
        "                            else:\n",
        "                                img = fake_imgs[0].detach().cpu()\n",
        "                                lr_img = None\n",
        "                            attn_maps = attention_maps[i]\n",
        "                            #img = Image.fromarray(img)\n",
        "                            to_pil_image = transforms.ToPILImage()\n",
        "                            print(img.shape)\n",
        "                            img = to_pil_image(img[0])\n",
        "                            draw = ImageDraw.Draw(img)\n",
        "                            font = ImageFont.load_default()\n",
        "                            draw.text((0, 0), texts[i], (255, 255, 255), font=font)\n",
        "\n",
        "                            fullpath = '%s_s%d.png' % (s_tmp, image)\n",
        "                            self.image+=1\n",
        "                            #print(fullpath)\n",
        "                            img.save(fullpath)\n",
        "                            # img_set, _ = \\\n",
        "                            #     build_super_images(img, captions, self.ixtoword,\n",
        "                            #                       attn_maps, att_sze, lr_imgs=lr_img)\n",
        "                            # if img_set is not None:\n",
        "                            #     im = Image.fromarray(img_set)\n",
        "                            #     fullpath = '%s_s%d.png'\\\n",
        "                            #         % (s_tmp, image)\n",
        "                            #     image+=1\n",
        "                            #     print(fullpath)\n",
        "                            #     im.save(fullpath)\n",
        "                 \n",
        "    def get_inception(self,data_dic,inception_model):\n",
        "        \n",
        "        if cfg.TRAIN.NET_G == '':\n",
        "            print('Error: the path for morels is not found!')\n",
        "        else:\n",
        "            # Build and load the generator\n",
        "            text_encoder = \\\n",
        "                RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "            state_dict = \\\n",
        "                torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n",
        "            text_encoder.load_state_dict(state_dict)\n",
        "            #print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
        "            text_encoder = text_encoder.cuda()\n",
        "            text_encoder.eval()\n",
        "\n",
        "            # the path to save generated images\n",
        "            if cfg.GAN.B_DCGAN:\n",
        "                netG = G_DCGAN()\n",
        "            else:\n",
        "                netG = G_NET()\n",
        "            s_tmp = cfg.TRAIN.NET_G[:cfg.TRAIN.NET_G.rfind('.pth')]\n",
        "            model_dir = cfg.TRAIN.NET_G\n",
        "            state_dict = \\\n",
        "                torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
        "            netG.load_state_dict(state_dict)\n",
        "            #print('Load G from: ', model_dir)\n",
        "            netG.cuda()\n",
        "            netG.eval()\n",
        "            #image = 1\n",
        "            #print(len(data_dic))\n",
        "            for key in data_dic:\n",
        "                lis = key.split('_')\n",
        "                save_dir = '%s/%s/%s' % (s_tmp,'naveen','_'.join(lis[:-2]))\n",
        "                mkdir_p(save_dir)\n",
        "                captions, cap_lens, sorted_indices,texts = data_dic[key]\n",
        "                #print(captions)\n",
        "                #print(data_dic[key])\n",
        "\n",
        "                batch_size = captions.shape[0]\n",
        "                nz = cfg.GAN.Z_DIM\n",
        "                captions = Variable(torch.from_numpy(captions), volatile=True)\n",
        "                cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n",
        "\n",
        "                captions = captions.cuda()\n",
        "                cap_lens = cap_lens.cuda()\n",
        "                num = captions.size(0)\n",
        "                sentence_list = []\n",
        "                v_size=128\n",
        "                for i in range(num):\n",
        "                    cap = captions[i].data.cpu().numpy()\n",
        "                    #print(cap)\n",
        "                    sentence = []\n",
        "                    for j in range(len(cap)):\n",
        "                        if cap[j] == 0:\n",
        "                            break\n",
        "                        word = dataset.ixtoword[cap[j]]#.encode('ascii', 'ignore').decode('ascii')\n",
        "                        sentence.append(word)\n",
        "                    \n",
        "                    sentence_list.append(' '.join(sentence))\n",
        "                sentence_list\n",
        "                for _ in range(1):  # 16\n",
        "                    noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
        "                    noise = noise.cuda()\n",
        "                    #######################################################\n",
        "                    # (1) Extract text embeddings\n",
        "                    ######################################################\n",
        "                    hidden = text_encoder.init_hidden(batch_size)\n",
        "                    # words_embs: batch_size x nef x seq_len\n",
        "                    # sent_emb: batch_size x nef\n",
        "                    words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "                    mask = (captions == 0)\n",
        "                    #######################################################\n",
        "                    # (2) Generate fake images\n",
        "                    ######################################################\n",
        "                    noise.data.normal_(0, 1)\n",
        "                    fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
        "                    # G attention\n",
        "                    #self.image=1\n",
        "                    cap_lens_np = cap_lens.cpu().data.numpy()\n",
        "                    pred = inception_model(fake_imgs[-1].detach())\n",
        "                    self.predictions.append(pred.data.cpu().numpy())\n",
        "                    if len(self.predictions) > 100:\n",
        "                        self.predictions = np.concatenate(self.predictions, 0)\n",
        "                        mean, std = compute_inception_score(self.predictions, 10)\n",
        "                        # print('mean:', mean, 'std', std)\n",
        "                        # m_incep = summary.scalar('Inception_mean', mean)\n",
        "                        print('Inception mean',mean,'Inception std',std)\n",
        "                        fields=['Mean','Std']\n",
        "                        mydict={'Mean':mean,'Std':std}\n",
        "                        file_exists = os.path.isfile('%s/log.csv'% s_tmp)\n",
        "                        with open('%s/log.csv'% s_tmp,'a') as csvfile:\n",
        "                            writer = csv.DictWriter(csvfile, fieldnames = fields)  \n",
        "                            # writing headers (field names) \n",
        "                            if not  file_exists:\n",
        "                                print('File saved at %s/log.csv'% s_tmp)\n",
        "                                writer.writeheader()  \n",
        "                                \n",
        "                            # writing data rows  \n",
        "                            writer.writerow(mydict)\n",
        "                            csvfile.close()\n",
        "                        \n",
        "                        # self.summary_writer.add_summary(m_incep, count)\n",
        "                        # #\n",
        "                        # mean_nlpp, std_nlpp = \\\n",
        "                        #     negative_log_posterior_probability(predictions, 10)\n",
        "                        # m_nlpp = summary.scalar('NLPP_mean', mean_nlpp)\n",
        "                        # self.summary_writer.add_summary(m_nlpp, count)\n",
        "                        #\n",
        "                        self.predictions = []\n",
        "\n",
        "    def gen_example(self, data_dic,text_encoder,netG):\n",
        "        \n",
        "\n",
        "            #image = 1\n",
        "            #print(len(data_dic))\n",
        "            # epoch=int(cfg.TRAIN.NET_G[:cfg.TRAIN.NET_G.rfind('.pth')][-3:])\n",
        "            for key in data_dic:\n",
        "                lis = key.split('_')\n",
        "                #save_dir = '%s/epoch_%s/%s' % (self.image_dir,epoch,'_'.join(lis[:-2]))\n",
        "                save_dir = '%s/epoch_%s/%s' % ('/content',epoch,'_'.join(lis[:-2]))\n",
        "                mkdir_p(save_dir)\n",
        "                captions, cap_lens, sorted_indices,texts = data_dic[key]\n",
        "                #print(captions)\n",
        "                #print(data_dic[key])\n",
        "\n",
        "                batch_size = captions.shape[0]\n",
        "                nz = cfg.GAN.Z_DIM\n",
        "                captions = Variable(torch.from_numpy(captions), volatile=True)\n",
        "                cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n",
        "\n",
        "                captions = captions.cuda()\n",
        "                cap_lens = cap_lens.cuda()\n",
        "                num = captions.size(0)\n",
        "                sentence_list = []\n",
        "                v_size=128\n",
        "                for i in range(num):\n",
        "                    cap = captions[i].data.cpu().numpy()\n",
        "                    #print(cap)\n",
        "                    sentence = []\n",
        "                    for j in range(len(cap)):\n",
        "                        if cap[j] == 0:\n",
        "                            break\n",
        "                        word = dataset.ixtoword[cap[j]]#.encode('ascii', 'ignore').decode('ascii')\n",
        "                        sentence.append(word)\n",
        "                    \n",
        "                    sentence_list.append(' '.join(sentence))\n",
        "                sentence_list\n",
        "                for _ in range(1):  # 16\n",
        "                    noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
        "                    noise = noise.cuda()\n",
        "                    #######################################################\n",
        "                    # (1) Extract text embeddings\n",
        "                    ######################################################\n",
        "                    hidden = text_encoder.init_hidden(batch_size)\n",
        "                    # words_embs: batch_size x nef x seq_len\n",
        "                    # sent_emb: batch_size x nef\n",
        "                    words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "                    mask = (captions == 0)\n",
        "                    #######################################################\n",
        "                    # (2) Generate fake images\n",
        "                    ######################################################\n",
        "                    noise.data.normal_(0, 1)\n",
        "                    fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
        "                    # G attention\n",
        "                    #self.image=1\n",
        "                    cap_lens_np = cap_lens.cpu().data.numpy()\n",
        "                    #for k in range(batch_size):\n",
        "                    k = random.randint(0,batch_size-1)\n",
        "                        #print(len(fake_imgs))\n",
        "                        #for i in range(len(attention_maps)):\n",
        "                    s_tmp = '%s/%s' % (save_dir,'_'.join(lis[-2:]))\n",
        "                    folder = s_tmp[:s_tmp.rfind('/')]\n",
        "                    if not os.path.isdir(folder):\n",
        "                        print('Make a new folder: ', folder)\n",
        "                        mkdir_p(folder)\n",
        "                    if len(fake_imgs) > 1:\n",
        "                        img = fake_imgs[-1].detach().cpu()\n",
        "                        lr_img = fake_imgs[-2].detach().cpu()\n",
        "                    else:\n",
        "                        img = fake_imgs[0].detach().cpu()\n",
        "                        lr_img = None\n",
        "\n",
        "                    fullpath = '%s.png'\\\n",
        "                            % (s_tmp)\n",
        "                    #trans=transforms.ToTensor()\n",
        "                    vutils.save_image(img[k],fullpath, normalize=True)\n",
        "                    #print(fullpath)\n",
        "                \n",
        "                    # attn_maps = attention_maps[-1]\n",
        "                    # att_sze = attn_maps.size(2)\n",
        "                    # r,f= \\\n",
        "                    #     build_super_images3(img[k].unsqueeze(0), captions[k].unsqueeze(0), self.ixtoword,\n",
        "                    #                       [attn_maps[k]], att_sze, lr_imgs=lr_img[k].unsqueeze(0))\n",
        "                    \n",
        "                    # if r is not None:\n",
        "                        \n",
        "                        \n",
        "                        #save_(r,f,sentence_list[k],fullpath)\n",
        "                        #self.image+=1\n",
        "                                \n",
        "                    # for j in range(batch_size):\n",
        "                    #     save_name = '%s/%d_s_%d' % (save_dir, i, sorted_indices[j])\n",
        "                    #     print(len(fake_imgs))\n",
        "                    #     print(fake_imgs[0].shape)\n",
        "                    #     for k in range(len(fake_imgs)):\n",
        "                    #         im = fake_imgs[k][j].data.cpu().numpy()\n",
        "                    #         im = (im + 1.0) * 127.5\n",
        "                    #         im = im.astype(np.uint8)\n",
        "                    #         # print('im', im.shape)\n",
        "                    #         im = np.transpose(im, (1, 2, 0))\n",
        "                    #         # print('im', im.shape)\n",
        "                    #         im = Image.fromarray(im)\n",
        "                    #         fullpath = '%s_g%d.png' % (save_name, k)\n",
        "                    #         im.save(fullpath)\n",
        "                    #     print(len(attention_maps))\n",
        "                    #     print(attention_maps[0].shape)\n",
        "                    #     for k in range(len(attention_maps)):\n",
        "                    #         if len(fake_imgs) > 1:\n",
        "                    #             im = fake_imgs[k + 1].detach().cpu()\n",
        "                    #         else:\n",
        "                    #             im = fake_imgs[0].detach().cpu()\n",
        "                    #         attn_maps = attention_maps[k]\n",
        "                    #         att_sze = attn_maps.size(2)\n",
        "                    #         img_set, sentences = \\\n",
        "                    #             build_super_images2(im[j].unsqueeze(0),\n",
        "                    #                                 captions[j].unsqueeze(0),\n",
        "                    #                                 [cap_lens_np[j]], self.ixtoword,\n",
        "                    #                                 [attn_maps[j]], att_sze)\n",
        "                    #         if img_set is not None:\n",
        "                    #             im = Image.fromarray(img_set)\n",
        "                    #             fullpath = '%s_a%d.png' % (save_name, k)\n",
        "                    #             im.save(fullpath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCekK3st00N1"
      },
      "source": [
        "main.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbydfT8B6_5J"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "# from miscc.config import cfg, cfg_from_file\n",
        "# from datasets import TextDataset\n",
        "# from trainer import condGANTrainer as trainer\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZh0VIl27I7h"
      },
      "source": [
        "def gen_example(wordtoix, algo,inception=False):\n",
        "    '''generate images from example sentences'''\n",
        "    if cfg.TRAIN.NET_G == '':\n",
        "        print('Error: the path for morels is not found!')\n",
        "    else:\n",
        "        # Build and load the generator\n",
        "        text_encoder = \\\n",
        "            GPT2_RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "        state_dict = \\\n",
        "            torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n",
        "        text_encoder.load_state_dict(state_dict)\n",
        "        print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
        "        text_encoder = text_encoder.cuda()\n",
        "        text_encoder.eval()\n",
        "\n",
        "        # the path to save generated images\n",
        "        if cfg.GAN.B_DCGAN:\n",
        "            netG = G_DCGAN()\n",
        "        else:\n",
        "            netG = G_NET()\n",
        "        s_tmp = cfg.TRAIN.NET_G[:cfg.TRAIN.NET_G.rfind('.pth')]\n",
        "        model_dir = cfg.TRAIN.NET_G\n",
        "        state_dict = \\\n",
        "            torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
        "        netG.load_state_dict(state_dict)\n",
        "        print('Load G from: ', model_dir)\n",
        "        netG.cuda()\n",
        "        netG.eval()\n",
        "    from nltk.tokenize import RegexpTokenizer\n",
        "    filepath = '%s/example_filenames.txt' % (cfg.DATA_DIR)\n",
        "    #filepath = '/content/data/birds/'\n",
        "    data_dic = {}\n",
        "    with open(filepath, \"r\") as f:\n",
        "        filenames = f.read().split('\\n')\n",
        "        for name in filenames:\n",
        "            if len(name) == 0:\n",
        "                continue\n",
        "            filepath = '%s/%s.txt' % (cfg.DATA_DIR, name)\n",
        "            with open(filepath, \"r\") as f:\n",
        "                #print('Load from:', name)\n",
        "                sentences = f.read().split('\\n')\n",
        "                # a list of indices for a sentence\n",
        "                texts=[]\n",
        "                captions = []\n",
        "                cap_lens = []\n",
        "                for sent in sentences:\n",
        "                    #print(sent)\n",
        "                    if len(sent) == 0:\n",
        "                        continue\n",
        "                    sent = sent.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(sent.lower())\n",
        "                    if len(tokens) == 0:\n",
        "                        print('sent', sent)\n",
        "                        continue\n",
        "\n",
        "                    rev = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0 and t in wordtoix:\n",
        "                            rev.append(wordtoix[t])\n",
        "                    captions.append(rev)\n",
        "                    cap_lens.append(len(rev))\n",
        "                    texts.append(sent)\n",
        "            max_len = np.max(cap_lens)\n",
        "\n",
        "            sorted_indices = np.argsort(cap_lens)[::-1]\n",
        "            cap_lens = np.asarray(cap_lens)\n",
        "            cap_lens = cap_lens[sorted_indices]\n",
        "            cap_array = np.zeros((len(captions), max_len), dtype='int64')\n",
        "            for i in range(len(captions)):\n",
        "                idx = sorted_indices[i]\n",
        "                cap = captions[idx]\n",
        "                c_len = len(cap)\n",
        "                cap_array[i, :c_len] = cap\n",
        "            key = name[(name.rfind('/') + 1):]\n",
        "            data_dic[key] = [cap_array, cap_lens, sorted_indices,texts]\n",
        "    \n",
        "    if inception:\n",
        "        inception_model = INCEPTION_V3()\n",
        "        if cfg.CUDA:\n",
        "            inception_model = inception_model.cuda()\n",
        "        inception_model.eval()\n",
        "    for k,v in data_dic.items():\n",
        "      if inception:\n",
        "        algo.get_inception(dict([(k,v)]),inception_model)\n",
        "      else:\n",
        "        algo.gen_example(dict([(k,v)]),text_encoder,netG)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6RknCop6_wp"
      },
      "source": [
        "__file__ = '/content/data/'\n",
        "dir_path = (os.path.abspath(os.path.join(os.path.realpath(__file__),'./')))\n",
        "sys.path.append(dir_path)\n",
        "from torchvision import transforms\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Train a AttnGAN network')\n",
        "    parser.add_argument('--cfg', dest='cfg_file',\n",
        "                        help='optional config file',\n",
        "                        default='/content/data/cfg/bird_attn2.yml', type=str)\n",
        "    parser.add_argument('--gpu', dest='gpu_id', type=int, default=-1)\n",
        "    parser.add_argument('--data_dir', dest='data_dir', type=str, default='/content/data/birds')\n",
        "    parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
        "    parser.add_argument('--session',type=int,default=1)\n",
        "    # args = parser.parse_args()\n",
        "    args = parser.parse_known_args()[0]\n",
        "    return args\n",
        "\n",
        "def zipdir(path, ziph):\n",
        "    # ziph is zipfile handle\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            ziph.write(os.path.join(root, file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP1XjgVp01w9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ca798e-ef0c-4264-bfb5-c921875cf411"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    if args.cfg_file is not None:\n",
        "        cfg_from_file(args.cfg_file)\n",
        "\n",
        "    if args.gpu_id != -1:\n",
        "        cfg.GPU_ID = args.gpu_id\n",
        "    else:\n",
        "        cfg.CUDA = False\n",
        "    cfg.CUDA =True\n",
        "    cfg.TRAIN.FLAG = False\n",
        "    if args.data_dir != '':\n",
        "        cfg.DATA_DIR = args.data_dir\n",
        "    print('Using config:')\n",
        "    cfg.TRAIN.NET_E='/content/data/DAMSMencoders/bird/gpt2_text_encoder200.pth'\n",
        "    cfg.TRAIN.NET_G = '/content/drive/My Drive/data/gpt2_output_attn1/Model/netG_epoch_1.pth'\n",
        "    pprint.pprint(cfg)\n",
        "    if not cfg.TRAIN.FLAG:\n",
        "        args.manualSeed = 100\n",
        "    elif args.manualSeed is None:\n",
        "        args.manualSeed = random.randint(1, 10000)\n",
        "    random.seed(args.manualSeed)\n",
        "    np.random.seed(args.manualSeed)\n",
        "    torch.manual_seed(args.manualSeed)\n",
        "    if cfg.CUDA:\n",
        "        torch.cuda.manual_seed_all(args.manualSeed)\n",
        "\n",
        "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
        "    #timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "    output_dir = '/content/drive/My Drive/data/gpt2_output_attn%d' %(args.session)\n",
        "    cfg.DATA_DIR = '/content/data/birds/'\n",
        "    split_dir, bshuffle = 'train', True\n",
        "    if not cfg.TRAIN.FLAG:\n",
        "        # bshuffle = False\n",
        "        split_dir = 'test'\n",
        "    #cfg.TRAIN.FLAG=False\n",
        "    # Get data loader\n",
        "    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM - 1))\n",
        "    image_transform = transforms.Compose([\n",
        "        transforms.Scale(int(imsize * 76 / 64)),\n",
        "        transforms.RandomCrop(imsize),\n",
        "        transforms.RandomHorizontalFlip()])\n",
        "    dataset = TextDataset(cfg.DATA_DIR, split_dir,\n",
        "                          base_size=cfg.TREE.BASE_SIZE,\n",
        "                          transform=image_transform)\n",
        "    assert dataset\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "        drop_last=True, shuffle=bshuffle, num_workers=int(cfg.WORKERS))\n",
        "\n",
        "    # Define models and go to train/evaluate\n",
        "\n",
        "    algo = condGANTrainer(output_dir, dataloader, dataset.n_words, dataset.ixtoword)\n",
        "    \n",
        "   \n",
        "    for epoch in [695]:\n",
        "        start_t = time.time()\n",
        "        cfg.TRAIN.NET_G = f'/content/drive/My Drive/data/gpt2_output_attn1/Model/netG_epoch_{epoch}.pth'\n",
        "        #pprint.pprint(cfg)\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            algo.train()\n",
        "        else:\n",
        "            '''generate images from pre-extracted embeddings'''\n",
        "            if cfg.B_VALIDATION:\n",
        "                algo.sampling(split_dir)  # generate images for the whole valid dataset\n",
        "            else:\n",
        "                d=gen_example(dataset.wordtoix, algo,False)  # generate images for customized captions\n",
        "        end_t = time.time()\n",
        "        zipf = zipfile.ZipFile('%s/Image/epoch_%d.zip'%(output_dir,epoch), 'w', zipfile.ZIP_DEFLATED)\n",
        "        zipdir('/content/epoch_%d/'%(epoch), zipf)\n",
        "        zipf.close()\n",
        "        !rm -r /content/epoch_{epoch}/\n",
        "        print('Total time for training of epoch %d:'%(epoch), end_t - start_t)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '/content/data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': False,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '/content/data/DAMSMencoders/bird/gpt2_text_encoder200.pth',\n",
            "           'NET_G': '/content/drive/My '\n",
            "                    'Drive/data/gpt2_output_attn1/Model/netG_epoch_1.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 50},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load filenames from: /content/data/birds//train/filenames.pickle (8855)\n",
            "Load filenames from: /content/data/birds//test/filenames.pickle (2933)\n",
            "Save to:  /content/data/birds/captions.pickle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 548118077/548118077 [00:07<00:00, 73875648.77B/s]\n",
            "100%|██████████| 665/665 [00:00<00:00, 373688.66B/s]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load text encoder from: /content/data/DAMSMencoders/bird/gpt2_text_encoder200.pth\n",
            "Load G from:  /content/drive/My Drive/data/gpt2_output_attn1/Model/netG_epoch_695.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/data/:699: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/content/data/:700: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/content/data/:720: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/data/:89: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total time for training of epoch 695: 327.92475390434265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFAQEY7b5YnL"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CVAQ0sUVkeV",
        "outputId": "c2c917db-200c-439a-f674-4cd5e17d4635"
      },
      "source": [
        "x = random.randn(1)\n",
        "print(x.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.17878332  0.76165335 -0.97806076 -0.09357969 -0.84370479 -0.03809951\n",
            " -0.14946979 -1.88089799 -2.42448574 -0.28907052]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPrmUnBPVq5X",
        "outputId": "8571bf97-ae22-4e6d-fb5a-bc710478fa10"
      },
      "source": [
        "for i in range(10):\n",
        "  x = random.randn(1)\n",
        "  print(x.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.11156600692865758\n",
            "0.14866506064264803\n",
            "-0.5070839889889001\n",
            "-0.43502337300022165\n",
            "0.3301877636124858\n",
            "-0.6971720090888385\n",
            "-0.5373199340295148\n",
            "0.04477994948977754\n",
            "-0.5834190452458582\n",
            "1.493224135042995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fxi3VLFWLd_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}