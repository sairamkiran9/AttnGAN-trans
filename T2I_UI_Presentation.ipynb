{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "T2I UI Presentation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "707ced6c67094a46bf00297bd5635dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a191364c6f1d4b9e8f4755514ff4c622",
              "IPY_MODEL_0fa90f7ab9284b8ab5637ecb4be9dfc4",
              "IPY_MODEL_46144499eaf84459997b1f58b1f72dd5"
            ],
            "layout": "IPY_MODEL_79be8d08e4544025aaae73e010af3bf3"
          }
        },
        "a191364c6f1d4b9e8f4755514ff4c622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd249ab53d9a4b09b74f5feba8116a50",
            "placeholder": "​",
            "style": "IPY_MODEL_af20c8fa5cd84d5fb17b656b7e13f1f0",
            "value": "Downloading: 100%"
          }
        },
        "0fa90f7ab9284b8ab5637ecb4be9dfc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3d586c225664db0ae2824dc087e4439",
            "max": 760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7dd4d676ed1454b811436a5ee0f8231",
            "value": 760
          }
        },
        "46144499eaf84459997b1f58b1f72dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_123130aa2f804b39800936ac1b9305f1",
            "placeholder": "​",
            "style": "IPY_MODEL_5a9a82bdc5a84514a0b742503118ef75",
            "value": " 760/760 [00:00&lt;00:00, 16.7kB/s]"
          }
        },
        "79be8d08e4544025aaae73e010af3bf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd249ab53d9a4b09b74f5feba8116a50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af20c8fa5cd84d5fb17b656b7e13f1f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3d586c225664db0ae2824dc087e4439": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7dd4d676ed1454b811436a5ee0f8231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "123130aa2f804b39800936ac1b9305f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a9a82bdc5a84514a0b742503118ef75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00b0aa70cf564b26ab15d6ed3fcbd857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_870b45c2668b4ac4affcb498c73bbed2",
              "IPY_MODEL_46aba3b6957c4849bf83e164cc9b5fa6",
              "IPY_MODEL_6d6eea7dfa684dd5aee60aed90a6b089"
            ],
            "layout": "IPY_MODEL_af8118d460ae447c99e0d903a8ef62bb"
          }
        },
        "870b45c2668b4ac4affcb498c73bbed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47367f4e0c0b4b80ba99345192d3b630",
            "placeholder": "​",
            "style": "IPY_MODEL_07a9eb02b5b34feeaebe80da37de1ecb",
            "value": "Downloading: 100%"
          }
        },
        "46aba3b6957c4849bf83e164cc9b5fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dbfd05f5abb4b97a221a22f3c0096e5",
            "max": 467042463,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7bd8df18a5547af9ef0ce23c18849f8",
            "value": 467042463
          }
        },
        "6d6eea7dfa684dd5aee60aed90a6b089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_930f26bf24144118939980b0e2968d59",
            "placeholder": "​",
            "style": "IPY_MODEL_cb0fe75f67894cbfb40c3604c40d47c8",
            "value": " 445M/445M [00:17&lt;00:00, 29.1MB/s]"
          }
        },
        "af8118d460ae447c99e0d903a8ef62bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47367f4e0c0b4b80ba99345192d3b630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07a9eb02b5b34feeaebe80da37de1ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dbfd05f5abb4b97a221a22f3c0096e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7bd8df18a5547af9ef0ce23c18849f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "930f26bf24144118939980b0e2968d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0fe75f67894cbfb40c3604c40d47c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sairamkiran9/AttnGAN-trans/blob/structure/T2I_UI_Presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm4T8mB56t0S",
        "outputId": "687f71f0-17f1-428d-d926-fef26322afb9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSM4aQIGTnKX",
        "outputId": "fead7b7c-9f43-43bb-f594-de8ea63162a4"
      },
      "source": [
        "!pip install pytorch_pretrained_bert\n",
        "!pip install flask-ngrok\n",
        "!pip install transformers\n",
        "#!pip install SentencePiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.62.3)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.9.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.19.2-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 31.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Collecting botocore<1.23.0,>=1.22.2\n",
            "  Downloading botocore-1.22.2-py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 45.9 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.23.0,>=1.22.2->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.0,>=1.22.2->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.19.2 botocore-1.22.2 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.0 urllib3-1.25.11\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K95dpQYnIwF8",
        "outputId": "9ff84859-7cd1-4fd8-9541-2ccc5223985e"
      },
      "source": [
        "!pip uninstall pyyaml -y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: PyYAML 6.0\n",
            "Uninstalling PyYAML-6.0:\n",
            "  Successfully uninstalled PyYAML-6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40OlteXvIWWo",
        "outputId": "6780e27b-09e7-40d2-cba3-93ba02ab9108"
      },
      "source": [
        "!pip install pyyaml==5.4.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 225 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 307 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 368 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 389 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 419 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 450 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 481 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 501 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 532 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 563 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 583 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 593 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 614 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 636 kB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyyaml\n",
            "Successfully installed pyyaml-5.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc8y1Ab_ZXqz",
        "outputId": "ac5655e9-29b3-4431-b692-dda7457e75e9"
      },
      "source": [
        "cd '/content/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utOKqtKHhC9T"
      },
      "source": [
        "# rm -r '/content/data/DAMSMencoders'\n",
        "!mkdir ./models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anHPrm3ITjXE"
      },
      "source": [
        "#!rm -r /content/html_files  /content/static"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFLU1QeteYoW"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/data/html_files /content/html_files\n",
        "!cp -r /content/drive/MyDrive/data/static /content/static\n",
        "#!mkdir /content/static/results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M6vwg2lga3b"
      },
      "source": [
        "!mkdir /content/models/xlnet/\n",
        "!mkdir /content/models/bert/\n",
        "!mkdir /content/models/gpt2/\n",
        "!mkdir /content/models/attngan/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLtvO9nNVjta"
      },
      "source": [
        "import cv2\n",
        "from tqdm.notebook import tqdm,trange\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "import textwrap,csv\n",
        "from flask import Flask, request, render_template  \n",
        "#from eval import eval_main\n",
        "import os\n",
        "import time\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from transformers import XLNetTokenizer, XLNetModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PMuN7FG7K4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caab62ca-3325-4d95-a98e-85d8967f5d74"
      },
      "source": [
        "!rm -r '/content/data/birds'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/data/birds': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFBMekLk1F0S"
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/data/birds.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/data\")\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud2Hagi_vnEp"
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/data/cfg.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/data/\")\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVpxhj0WrrwH"
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/data/XLNet.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/data/DAMSMencoders/xlnet\")\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJQIxEOabZWk"
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/data/bird.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/data/DAMSMencoders/attngan\")\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx1gkHpuzAOc"
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/data/gpt2.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/data/DAMSMencoders/gpt2\")\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JJfxcxiy78l"
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/data/bert.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/data/DAMSMencoders/bert\")\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8jJe_TzsXj2"
      },
      "source": [
        "import tarfile\n",
        "fname = \"/content/drive/My Drive/data/CUB_200_2011.tar.gz\"\n",
        "if fname.endswith(\"tar.gz\"):\n",
        "    tar = tarfile.open(fname, \"r:gz\")\n",
        "    tar.extractall(\"/content/data/birds\")\n",
        "    tar.close()\n",
        "elif fname.endswith(\"tar\"):\n",
        "    tar = tarfile.open(fname, \"r:\")\n",
        "    tar.extractall(\"/content/data/birds\")\n",
        "    tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaoem0aQhLTK"
      },
      "source": [
        "#!rm -r /content/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unVdw3YoeMSJ"
      },
      "source": [
        "!cp /content/data/DAMSMencoders/xlnet/XLNet_text_encoder200.pth /content/models/xlnet/\n",
        "!cp /content/drive/MyDrive/data/XLNet_output_attn1/Model/netG_epoch_630.pth /content/models/xlnet/\n",
        "!cp /content/data/DAMSMencoders/bert/bert_text_encoder200.pth /content/models/bert/\n",
        "!cp /content/drive/MyDrive/data/bert_output_attn1/Model/netG_epoch_658.pth /content/models/bert/\n",
        "!cp /content/data/DAMSMencoders/gpt2/gpt2_text_encoder200.pth /content/models/gpt2/\n",
        "!cp /content/drive/MyDrive/data/gpt2_output_attn1/Model/netG_epoch_695.pth /content/models/gpt2/\n",
        "!cp /content/data/DAMSMencoders/attngan/bird/text_encoder200.pth /content/models/attngan/\n",
        "!cp /content/drive/MyDrive/data/output_attn1/Model/netG_epoch_620.pth /content/models/attngan/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew14GdTeXP60"
      },
      "source": [
        "import pickle\n",
        "with open('/content/data/birds/example_filenames.txt','w') as file:\n",
        "  file.write(\"example_captions\")\n",
        "  # with open('/content/data/birds/test/filenames.pickle','rb') as f:\n",
        "  #   dir=pickle.load(f)\n",
        "  #   for d in dir:\n",
        "  #     d = 'text_c10/'+d\n",
        "  #     file.write(d)\n",
        "  #     file.write('\\n')\n",
        "  #   f.close()\n",
        "  file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvQ0c03kmTDh"
      },
      "source": [
        "bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps0xSaw8mWj5"
      },
      "source": [
        "from pytorch_pretrained_bert import BertModel\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import GPT2Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twh4FQC650yf"
      },
      "source": [
        "code/miscc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo3MsrQ15inx"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import calendar\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "from easydict import EasyDict as edict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95SszhEs5lMm"
      },
      "source": [
        "__C = edict()\n",
        "cfg = __C\n",
        "\n",
        "# Dataset name: flowers, birds\n",
        "__C.DATASET_NAME = 'birds'\n",
        "__C.CONFIG_NAME = ''\n",
        "__C.DATA_DIR = ''\n",
        "__C.GPU_ID = 0\n",
        "__C.CUDA = True\n",
        "__C.WORKERS = 6\n",
        "\n",
        "__C.RNN_TYPE = 'LSTM'   # 'GRU'\n",
        "__C.B_VALIDATION = False\n",
        "\n",
        "__C.TREE = edict()\n",
        "__C.TREE.BRANCH_NUM = 3\n",
        "__C.TREE.BASE_SIZE = 64\n",
        "\n",
        "\n",
        "# Training options\n",
        "__C.TRAIN = edict()\n",
        "__C.TRAIN.BATCH_SIZE = 64\n",
        "__C.TRAIN.MAX_EPOCH = 600\n",
        "__C.TRAIN.SNAPSHOT_INTERVAL = 1000\n",
        "__C.TRAIN.DISCRIMINATOR_LR = 2e-4\n",
        "__C.TRAIN.GENERATOR_LR = 2e-4\n",
        "__C.TRAIN.ENCODER_LR = 2e-4\n",
        "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
        "__C.TRAIN.FLAG = True\n",
        "__C.TRAIN.NET_E = ''\n",
        "__C.TRAIN.NET_G = '/content/drive/My Drive/data/output_attn1/Model/netG_epoch_8.pth'\n",
        "__C.TRAIN.B_NET_D = True\n",
        "\n",
        "__C.TRAIN.SMOOTH = edict()\n",
        "__C.TRAIN.SMOOTH.GAMMA1 = 5.0\n",
        "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
        "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
        "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
        "\n",
        "\n",
        "# Modal options\n",
        "__C.GAN = edict()\n",
        "__C.GAN.DF_DIM = 64\n",
        "__C.GAN.GF_DIM = 128\n",
        "__C.GAN.Z_DIM = 100\n",
        "__C.GAN.CONDITION_DIM = 100\n",
        "__C.GAN.R_NUM = 2\n",
        "__C.GAN.B_ATTENTION = True\n",
        "__C.GAN.B_DCGAN = False\n",
        "\n",
        "\n",
        "__C.TEXT = edict()\n",
        "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
        "__C.TEXT.EMBEDDING_DIM = 256\n",
        "__C.TEXT.WORDS_NUM = 18\n",
        "\n",
        "\n",
        "def _merge_a_into_b(a, b):\n",
        "    \"\"\"Merge config dictionary a into config dictionary b, clobbering the\n",
        "    options in b whenever they are also specified in a.\n",
        "    \"\"\"\n",
        "    if type(a) is not edict:\n",
        "        return\n",
        "\n",
        "    for k, v in a.items():\n",
        "        # a must specify keys that are in b\n",
        "        if k not in b:\n",
        "            raise KeyError('{} is not a valid config key'.format(k))\n",
        "\n",
        "        # the types must match, too\n",
        "        old_type = type(b[k])\n",
        "        if old_type is not type(v):\n",
        "            if isinstance(b[k], np.ndarray):\n",
        "                v = np.array(v, dtype=b[k].dtype)\n",
        "            else:\n",
        "                raise ValueError(('Type mismatch ({} vs. {}) '\n",
        "                                  'for config key: {}').format(type(b[k]),\n",
        "                                                               type(v), k))\n",
        "\n",
        "        # recursively merge dicts\n",
        "        if type(v) is edict:\n",
        "            try:\n",
        "                _merge_a_into_b(a[k], b[k])\n",
        "            except:\n",
        "                print('Error under config key: {}'.format(k))\n",
        "                raise\n",
        "        else:\n",
        "            b[k] = v\n",
        "\n",
        "\n",
        "def cfg_from_file(filename):\n",
        "    \"\"\"Load a config file and merge it into the default options.\"\"\"\n",
        "    # print(filename)\n",
        "    import yaml\n",
        "    with open(filename, 'r') as f:\n",
        "        yaml_cfg = edict(yaml.load(f))\n",
        "        # print(\"success\")\n",
        "    _merge_a_into_b(yaml_cfg, __C)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZwoZApa_HdU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=False)\n",
        "\n",
        "\n",
        "def func_attention(query, context, gamma1):\n",
        "    \"\"\"\n",
        "    query: batch x ndf x queryL\n",
        "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
        "    mask: batch_size x sourceL\n",
        "    \"\"\"\n",
        "    batch_size, queryL = query.size(0), query.size(2)\n",
        "    ih, iw = context.size(2), context.size(3)\n",
        "    sourceL = ih * iw\n",
        "\n",
        "    # --> batch x sourceL x ndf\n",
        "    context = context.view(batch_size, -1, sourceL)\n",
        "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
        "\n",
        "    # Get attention\n",
        "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
        "    # -->batch x sourceL x queryL\n",
        "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
        "    # --> batch*sourceL x queryL\n",
        "    attn = attn.view(batch_size*sourceL, queryL)\n",
        "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
        "\n",
        "    # --> batch x sourceL x queryL\n",
        "    attn = attn.view(batch_size, sourceL, queryL)\n",
        "    # --> batch*queryL x sourceL\n",
        "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "    attn = attn.view(batch_size*queryL, sourceL)\n",
        "    #  Eq. (9)\n",
        "    attn = attn * gamma1\n",
        "    attn = nn.Softmax()(attn)\n",
        "    attn = attn.view(batch_size, queryL, sourceL)\n",
        "    # --> batch x sourceL x queryL\n",
        "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
        "    # --> batch x ndf x queryL\n",
        "    weightedContext = torch.bmm(context, attnT)\n",
        "\n",
        "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "\n",
        "class GlobalAttentionGeneral(nn.Module):\n",
        "    def __init__(self, idf, cdf):\n",
        "        super(GlobalAttentionGeneral, self).__init__()\n",
        "        self.conv_context = conv1x1(cdf, idf)\n",
        "        self.sm = nn.Softmax()\n",
        "        self.mask = None\n",
        "\n",
        "    def applyMask(self, mask):\n",
        "        self.mask = mask  # batch x sourceL\n",
        "\n",
        "    def forward(self, input, context):\n",
        "        \"\"\"\n",
        "            input: batch x idf x ih x iw (queryL=ihxiw)\n",
        "            context: batch x cdf x sourceL\n",
        "        \"\"\"\n",
        "        ih, iw = input.size(2), input.size(3)\n",
        "        queryL = ih * iw\n",
        "        batch_size, sourceL = context.size(0), context.size(2)\n",
        "\n",
        "        # --> batch x queryL x idf\n",
        "        target = input.view(batch_size, -1, queryL)\n",
        "        targetT = torch.transpose(target, 1, 2).contiguous()\n",
        "        # batch x cdf x sourceL --> batch x cdf x sourceL x 1\n",
        "        sourceT = context.unsqueeze(3)\n",
        "        # --> batch x idf x sourceL\n",
        "        sourceT = self.conv_context(sourceT).squeeze(3)\n",
        "\n",
        "        # Get attention\n",
        "        # (batch x queryL x idf)(batch x idf x sourceL)\n",
        "        # -->batch x queryL x sourceL\n",
        "        attn = torch.bmm(targetT, sourceT)\n",
        "        # --> batch*queryL x sourceL\n",
        "        attn = attn.view(batch_size*queryL, sourceL)\n",
        "        if self.mask is not None:\n",
        "            # batch_size x sourceL --> batch_size*queryL x sourceL\n",
        "            mask = self.mask.repeat(queryL, 1)\n",
        "            attn.data.masked_fill_(mask.data, -float('inf'))\n",
        "        attn = self.sm(attn)  # Eq. (2)\n",
        "        # --> batch x queryL x sourceL\n",
        "        attn = attn.view(batch_size, queryL, sourceL)\n",
        "        # --> batch x sourceL x queryL\n",
        "        attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "        # (batch x idf x sourceL)(batch x sourceL x queryL)\n",
        "        # --> batch x idf x queryL\n",
        "        weightedContext = torch.bmm(sourceT, attn)\n",
        "        weightedContext = weightedContext.view(batch_size, -1, ih, iw)\n",
        "        attn = attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "        return weightedContext, attn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3usYkyhk57SZ"
      },
      "source": [
        "import numpy as np\n",
        "# from miscc.config import cfg\n",
        "\n",
        "# from GlobalAttention import func_attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFhKtbs7hdRY"
      },
      "source": [
        "def cal_activations(act):\n",
        "  mu = np.mean(act, axis=0)\n",
        "  sigma = np.cov(act, rowvar=False)\n",
        "  return mu, sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roXxI7rxf484"
      },
      "source": [
        "\n",
        "def FID(x1, x2, dim=1, eps=1e-8):\n",
        "    \"\"\"Returns FID between x1 and x2, computed along dim.\n",
        "    \"\"\"\n",
        "    mu1 = np.atleast_1d(cal_activations(x1))\n",
        "    mu2 = np.atleast_1d(cal_activations(x2))\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    # Product might be almost singular\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) +\n",
        "            np.trace(sigma2) - 2 * tr_covmean)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVKvWe415lPo"
      },
      "source": [
        " ##################Loss for matching text-image###################\n",
        "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
        "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\n",
        "    \"\"\"\n",
        "    w12 = torch.sum(x1 * x2, dim)\n",
        "    w1 = torch.norm(x1, 2, dim)\n",
        "    w2 = torch.norm(x2, 2, dim)\n",
        "    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n",
        "\n",
        "\n",
        "def sent_loss(cnn_code, rnn_code, labels, class_ids,\n",
        "              batch_size, eps=1e-8):\n",
        "    # ### Mask mis-match samples  ###\n",
        "    # that come from the same class as the real sample ###\n",
        "    masks = []\n",
        "    if class_ids is not None:\n",
        "        for i in range(batch_size):\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.ByteTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    # --> seq_len x batch_size x nef\n",
        "    if cnn_code.dim() == 2:\n",
        "        cnn_code = cnn_code.unsqueeze(0)\n",
        "        rnn_code = rnn_code.unsqueeze(0)\n",
        "\n",
        "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
        "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
        "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
        "    # scores* / norm*: seq_len x batch_size x batch_size\n",
        "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
        "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
        "    scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "\n",
        "    # --> batch_size x batch_size\n",
        "    scores0 = scores0.squeeze()\n",
        "    if class_ids is not None:\n",
        "        scores0.data.masked_fill_(masks, -float('inf'))\n",
        "    scores1 = scores0.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQvmXR8I5lSa"
      },
      "source": [
        "def words_loss(img_features, words_emb, labels,\n",
        "               cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "    for i in range(batch_size):\n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = cosine_similarity(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.ByteTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8xph8OVfRc9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkynFFjQfIbF"
      },
      "source": [
        "def Compute_FID(img_features, words_emb, labels,\n",
        "               cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "    for i in range(batch_size):\n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = FID(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.ByteTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luEeSiq25lYQ"
      },
      "source": [
        "# ##################Loss for G and Ds##############################\n",
        "def discriminator_loss(netD, real_imgs, fake_imgs, conditions,\n",
        "                       real_labels, fake_labels):\n",
        "    # Forward\n",
        "    real_features = netD(real_imgs)\n",
        "    fake_features = netD(fake_imgs.detach())\n",
        "    # loss\n",
        "    #\n",
        "    cond_real_logits = netD.COND_DNET(real_features, conditions)\n",
        "    cond_real_errD = nn.BCELoss()(cond_real_logits, real_labels)\n",
        "    cond_fake_logits = netD.COND_DNET(fake_features, conditions)\n",
        "    cond_fake_errD = nn.BCELoss()(cond_fake_logits, fake_labels)\n",
        "    #\n",
        "    batch_size = real_features.size(0)\n",
        "    cond_wrong_logits = netD.COND_DNET(real_features[:(batch_size - 1)], conditions[1:batch_size])\n",
        "    cond_wrong_errD = nn.BCELoss()(cond_wrong_logits, fake_labels[1:batch_size])\n",
        "\n",
        "    if netD.UNCOND_DNET is not None:\n",
        "        real_logits = netD.UNCOND_DNET(real_features)\n",
        "        fake_logits = netD.UNCOND_DNET(fake_features)\n",
        "        real_errD = nn.BCELoss()(real_logits, real_labels)\n",
        "        fake_errD = nn.BCELoss()(fake_logits, fake_labels)\n",
        "        errD = ((real_errD + cond_real_errD) / 2. +\n",
        "                (fake_errD + cond_fake_errD + cond_wrong_errD) / 3.)\n",
        "    else:\n",
        "        errD = cond_real_errD + (cond_fake_errD + cond_wrong_errD) / 2.\n",
        "    return errD\n",
        "\n",
        "\n",
        "def generator_loss(netsD, image_encoder, fake_imgs, real_labels,\n",
        "                   words_embs, sent_emb, match_labels,\n",
        "                   cap_lens, class_ids):\n",
        "    numDs = len(netsD)\n",
        "    batch_size = real_labels.size(0)\n",
        "    logs = ''\n",
        "    # Forward\n",
        "    errG_total = 0\n",
        "    for i in range(numDs):\n",
        "        features = netsD[i](fake_imgs[i])\n",
        "        cond_logits = netsD[i].COND_DNET(features, sent_emb)\n",
        "        cond_errG = nn.BCELoss()(cond_logits, real_labels)\n",
        "        if netsD[i].UNCOND_DNET is  not None:\n",
        "            logits = netsD[i].UNCOND_DNET(features)\n",
        "            errG = nn.BCELoss()(logits, real_labels)\n",
        "            g_loss = errG + cond_errG\n",
        "        else:\n",
        "            g_loss = cond_errG\n",
        "        errG_total += g_loss\n",
        "        # err_img = errG_total.data[0]\n",
        "        logs += 'g_loss%d: %.2f ' % (i, g_loss)\n",
        "\n",
        "        # Ranking loss\n",
        "        if i == (numDs - 1):\n",
        "            # words_features: batch_size x nef x 17 x 17\n",
        "            # sent_code: batch_size x nef\n",
        "            region_features, cnn_code = image_encoder(fake_imgs[i])\n",
        "            w_loss0, w_loss1, _ = words_loss(region_features, words_embs,\n",
        "                                             match_labels, cap_lens,\n",
        "                                             class_ids, batch_size)\n",
        "            w_loss = (w_loss0 + w_loss1) * \\\n",
        "                cfg.TRAIN.SMOOTH.LAMBDA\n",
        "            # err_words = err_words + w_loss.data[0]\n",
        "\n",
        "            s_loss0, s_loss1 = sent_loss(cnn_code, sent_emb,\n",
        "                                         match_labels, class_ids, batch_size)\n",
        "            s_loss = (s_loss0 + s_loss1) * \\\n",
        "                cfg.TRAIN.SMOOTH.LAMBDA\n",
        "            # err_sent = err_sent + s_loss.data[0]\n",
        "\n",
        "            errG_total += w_loss + s_loss\n",
        "            logs += 'w_loss: %.2f s_loss: %.2f ' % (w_loss, s_loss)\n",
        "    return errG_total, logs\n",
        "\n",
        "\n",
        "##################################################################\n",
        "def KL_loss(mu, logvar):\n",
        "    # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
        "    KLD = torch.mean(KLD_element).mul_(-0.5)\n",
        "    return KLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upHx7d-36Qlj"
      },
      "source": [
        "miscc/utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOZc_mSe6UNv"
      },
      "source": [
        "import os\n",
        "import errno\n",
        "import numpy as np\n",
        "from torch.nn import init\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from copy import deepcopy\n",
        "import skimage.transform\n",
        "\n",
        "# from miscc.config import cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNKfEefWs86n"
      },
      "source": [
        "class INCEPTION_V3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(INCEPTION_V3, self).__init__()\n",
        "        self.model = models.inception_v3()\n",
        "        # url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        ## print(next(model.parameters()).data)\n",
        "        # state_dict = \\\n",
        "        #     model_zoo.load_url(url, map_location=lambda storage, loc: storage)\n",
        "        state_dict = torch.load('/content/drive/My Drive/data/inception_v3.pth')\n",
        "        self.model.load_state_dict(state_dict)\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        # print('Load pretrained model from ', url)\n",
        "        # print(next(self.model.parameters()).data)\n",
        "        # print(self.model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # [-1.0, 1.0] --> [0, 1.0]\n",
        "        x = input * 0.5 + 0.5\n",
        "        # mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]\n",
        "        # --> mean = 0, std = 1\n",
        "        x[:, 0] = (x[:, 0] - 0.485) / 0.229\n",
        "        x[:, 1] = (x[:, 1] - 0.456) / 0.224\n",
        "        x[:, 2] = (x[:, 2] - 0.406) / 0.225\n",
        "        #\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.model(x)\n",
        "        x = nn.Softmax()(x)\n",
        "        return x\n",
        "        \n",
        "def compute_inception_score(predictions, num_splits=1):\n",
        "    # print('predictions', predictions.shape)\n",
        "    scores = []\n",
        "    for i in range(num_splits):\n",
        "        istart = i * predictions.shape[0] // num_splits\n",
        "        iend = (i + 1) * predictions.shape[0] // num_splits\n",
        "        part = predictions[istart:iend, :]\n",
        "        kl = part * \\\n",
        "            (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
        "        kl = np.mean(np.sum(kl, 1))\n",
        "        scores.append(np.exp(kl))\n",
        "    #print(\"Inception Score: \",np.mean(scores), np.std(scores))\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "\n",
        "def negative_log_posterior_probability(predictions, num_splits=1):\n",
        "    # print('predictions', predictions.shape)\n",
        "    scores = []\n",
        "    for i in range(num_splits):\n",
        "        istart = i * predictions.shape[0] // num_splits\n",
        "        iend = (i + 1) * predictions.shape[0] // num_splits\n",
        "        part = predictions[istart:iend, :]\n",
        "        result = -1. * np.log(np.max(part, 1))\n",
        "        result = np.mean(result)\n",
        "        scores.append(result)\n",
        "    return np.mean(scores), np.std(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_IOksS75lbd"
      },
      "source": [
        "# For visualization ################################################\n",
        "COLOR_DIC = {0:[128,64,128],  1:[244, 35,232],\n",
        "             2:[70, 70, 70],  3:[102,102,156],\n",
        "             4:[190,153,153], 5:[153,153,153],\n",
        "             6:[250,170, 30], 7:[220, 220, 0],\n",
        "             8:[107,142, 35], 9:[152,251,152],\n",
        "             10:[70,130,180], 11:[220,20, 60],\n",
        "             12:[255, 0, 0],  13:[0, 0, 142],\n",
        "             14:[119,11, 32], 15:[0, 60,100],\n",
        "             16:[0, 80, 100], 17:[0, 0, 230],\n",
        "             18:[0,  0, 70],  19:[0, 0,  0]}\n",
        "FONT_MAX = 50\n",
        "\n",
        "\n",
        "def drawCaption(convas, captions, ixtoword, vis_size, off1=2, off2=2):\n",
        "    num = captions.size(0)\n",
        "    img_txt = Image.fromarray(convas)\n",
        "    # get a font\n",
        "    # fnt = None  # ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
        "    # fnt = ImageFont.truetype('arial.ttf', 50)\n",
        "    fnt = ImageFont.load_default()\n",
        "    # get a drawing context\n",
        "    d = ImageDraw.Draw(img_txt)\n",
        "    sentence_list = []\n",
        "    for i in range(num):\n",
        "        cap = captions[i].data.cpu().numpy()\n",
        "        sentence = []\n",
        "        for j in range(len(cap)):\n",
        "            if cap[j] == 0:\n",
        "                break\n",
        "            word = ixtoword[cap[j]].encode('ascii', 'ignore').decode('ascii')\n",
        "            d.text(((j + off1) * (vis_size + off2), i * FONT_MAX), '%d:%s' % (j, word[:6]),\n",
        "                   font=fnt, fill=(255, 255, 255, 255))\n",
        "            sentence.append(word)\n",
        "        sentence_list.append(sentence)\n",
        "    return img_txt, sentence_list\n",
        "\n",
        "\n",
        "def build_super_images(real_imgs, captions, ixtoword,\n",
        "                       attn_maps, att_sze, lr_imgs=None,\n",
        "                       batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                       max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    \n",
        "    nvis = 1\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    text_convas = \\\n",
        "        np.ones([batch_size * FONT_MAX,\n",
        "                 (max_word_num + 2) * (vis_size + 2), 3],\n",
        "                dtype=np.uint8)\n",
        "\n",
        "    for i in range(max_word_num):\n",
        "        istart = (i + 2) * (vis_size + 2)\n",
        "        iend = (i + 3) * (vis_size + 2)\n",
        "        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    seq_len = max_word_num\n",
        "    img_set = []\n",
        "    num = nvis  # len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        # --> 1 x 1 x 17 x 17\n",
        "        attn_max = attn.max(dim=1, keepdim=True)\n",
        "        attn = torch.cat([attn_max[0], attn], 1)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = attn.shape[0]\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        if lr_imgs is None:\n",
        "            lrI = img\n",
        "        else:\n",
        "            lrI = lr_imgs[i]\n",
        "        row = [lrI, middle_pad]\n",
        "        row_merge = [img, middle_pad]\n",
        "        row_beforeNorm = []\n",
        "        minVglobal, maxVglobal = 1, 0\n",
        "        for j in range(num_attn):\n",
        "            one_map = attn[j]\n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map=cv2.resize(one_map,(vis_size,vis_size),interpolation = cv2.INTER_CUBIC)\n",
        "                #one_map = \\\n",
        "                    #skimage.transform.pyramid_expand(one_map, sigma=20,\n",
        "                     #                                upscale=vis_size // att_sze)\n",
        "            row_beforeNorm.append(one_map)\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            if minVglobal > minV:\n",
        "                minVglobal = minV\n",
        "            if maxVglobal < maxV:\n",
        "                maxVglobal = maxV\n",
        "        for j in range(seq_len + 1):\n",
        "            if j < num_attn:\n",
        "                one_map = row_beforeNorm[j]\n",
        "                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
        "                one_map *= 255\n",
        "                #\n",
        "                PIL_im = Image.fromarray(np.uint8(img))\n",
        "                PIL_att = Image.fromarray(np.uint8(one_map))\n",
        "                merged = \\\n",
        "                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "                mask = Image.new('L', (vis_size, vis_size), (210))\n",
        "                merged.paste(PIL_im, (0, 0))\n",
        "                merged.paste(PIL_att, (0, 0), mask)\n",
        "                merged = np.array(merged)[:, :, :3]\n",
        "            else:\n",
        "                one_map = post_pad\n",
        "                merged = post_pad\n",
        "            row.append(one_map)\n",
        "            row.append(middle_pad)\n",
        "            #\n",
        "            row_merge.append(merged)\n",
        "            row_merge.append(middle_pad)\n",
        "        row = np.concatenate(row, 1)\n",
        "        row_merge = np.concatenate(row_merge, 1)\n",
        "        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('txt', txt.shape, 'row', row.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        row = np.concatenate([txt, row, row_merge], 0)\n",
        "        img_set.append(row)\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def build_super_images2(real_imgs, captions, cap_lens, ixtoword,\n",
        "                        attn_maps, att_sze, vis_size=256, topK=5):\n",
        "    batch_size = real_imgs.size(0)\n",
        "    max_word_num = np.max(cap_lens)\n",
        "    text_convas = np.ones([batch_size * FONT_MAX,\n",
        "                           max_word_num * (vis_size + 2), 3],\n",
        "                           dtype=np.uint8)\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    img_set = []\n",
        "    num = len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size, off1=0)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = cap_lens[i]\n",
        "        thresh = 2./float(num_attn)\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        row = []\n",
        "        row_merge = []\n",
        "        row_txt = []\n",
        "        row_beforeNorm = []\n",
        "        conf_score = []\n",
        "        for j in range(num_attn):\n",
        "            one_map = attn[j]\n",
        "            mask0 = one_map > (2. * thresh)\n",
        "            conf_score.append(np.sum(one_map * mask0))\n",
        "            mask = one_map > thresh\n",
        "            one_map = one_map * mask\n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map = cv2.resize(one_map,(vis_size,vis_size),interpolation = cv2.INTER_CUBIC)\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            one_map = (one_map - minV) / (maxV - minV)\n",
        "            row_beforeNorm.append(one_map)\n",
        "        sorted_indices = np.argsort(conf_score)[::-1]\n",
        "\n",
        "        for j in range(num_attn):\n",
        "            one_map = row_beforeNorm[j]\n",
        "            one_map *= 255\n",
        "            #\n",
        "            PIL_im = Image.fromarray(np.uint8(img))\n",
        "            PIL_att = Image.fromarray(np.uint8(one_map))\n",
        "            merged = \\\n",
        "                Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "            mask = Image.new('L', (vis_size, vis_size), (180))  # (210)\n",
        "            merged.paste(PIL_im, (0, 0))\n",
        "            merged.paste(PIL_att, (0, 0), mask)\n",
        "            merged = np.array(merged)[:, :, :3]\n",
        "\n",
        "            row.append(np.concatenate([one_map, middle_pad], 1))\n",
        "            #\n",
        "            row_merge.append(np.concatenate([merged, middle_pad], 1))\n",
        "            #\n",
        "            txt = text_map[i * FONT_MAX:(i + 1) * FONT_MAX,\n",
        "                           j * (vis_size + 2):(j + 1) * (vis_size + 2), :]\n",
        "            row_txt.append(txt)\n",
        "        # reorder\n",
        "        row_new = []\n",
        "        row_merge_new = []\n",
        "        txt_new = []\n",
        "        for j in range(num_attn):\n",
        "            idx = sorted_indices[j]\n",
        "            row_new.append(row[idx])\n",
        "            row_merge_new.append(row_merge[idx])\n",
        "            txt_new.append(row_txt[idx])\n",
        "        row = np.concatenate(row_new[:topK], 1)\n",
        "        row_merge = np.concatenate(row_merge_new[:topK], 1)\n",
        "        txt = np.concatenate(txt_new[:topK], 1)\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('Warnings: txt', txt.shape, 'row', row.shape,\n",
        "                  'row_merge_new', row_merge_new.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        row = np.concatenate([txt, row_merge], 0)\n",
        "        img_set.append(row)\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "####################################################################\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.orthogonal(m.weight.data, 1.0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        nn.init.orthogonal(m.weight.data, 1.0)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0.0)\n",
        "\n",
        "\n",
        "def load_params(model, new_param):\n",
        "    for p, new_p in zip(model.parameters(), new_param):\n",
        "        p.data.copy_(new_p)\n",
        "\n",
        "\n",
        "def copy_G_params(model):\n",
        "    flatten = deepcopy(list(p.data for p in model.parameters()))\n",
        "    return flatten\n",
        "\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1idyWJ85leI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_teLlla5lhM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8yiyDx15lkB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi5d4-nk0mqR"
      },
      "source": [
        "dataset.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82sHxbk33KtG"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "# from miscc.config import cfg\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy.random as random\n",
        "if sys.version_info[0] == 2:\n",
        "    import cPickle as pickle\n",
        "else:\n",
        "    import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6P1M8yO3KwM"
      },
      "source": [
        "def prepare_data(data):\n",
        "    imgs, captions, captions_lens, class_ids, keys = data\n",
        "    #print(captions)\n",
        "    # sort data by the length in a decreasing order\n",
        "    sorted_cap_lens, sorted_cap_indices = \\\n",
        "        torch.sort(captions_lens, 0, True)\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = imgs[i][sorted_cap_indices]\n",
        "        if cfg.CUDA:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    captions = captions[sorted_cap_indices].squeeze()\n",
        "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
        "    # sent_indices = sent_indices[sorted_cap_indices]\n",
        "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
        "    # print('keys', type(keys), keys[-1])  # list\n",
        "    if cfg.CUDA:\n",
        "        captions = Variable(captions).cuda()\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n",
        "    else:\n",
        "        captions = Variable(captions)\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens)\n",
        "\n",
        "    return [real_imgs, captions, sorted_cap_lens,\n",
        "            class_ids, keys]\n",
        "\n",
        "\n",
        "def get_imgs(img_path, imsize, bbox=None,\n",
        "             transform=None, normalize=None):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - r)\n",
        "        y2 = np.minimum(height, center_y + r)\n",
        "        x1 = np.maximum(0, center_x - r)\n",
        "        x2 = np.minimum(width, center_x + r)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img)\n",
        "\n",
        "    ret = []\n",
        "    if cfg.GAN.B_DCGAN:\n",
        "        ret = [normalize(img)]\n",
        "    else:\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            # print(imsize[i])\n",
        "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
        "                re_img = transforms.Scale(imsize[i])(img)\n",
        "            else:\n",
        "                re_img = img\n",
        "            ret.append(normalize(re_img))\n",
        "\n",
        "    return ret\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SngQkRYCxn5"
      },
      "source": [
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train',\n",
        "                 base_size=64,\n",
        "                 transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
        "\n",
        "        self.imsize = []\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            self.imsize.append(base_size)\n",
        "            base_size = base_size * 2\n",
        "\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        if data_dir.find('birds') != -1:\n",
        "            self.bbox = self.load_bbox()\n",
        "        else:\n",
        "            self.bbox = None\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames, self.captions, self.ixtoword, \\\n",
        "            self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
        "\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames))\n",
        "        self.number_example = len(self.filenames)\n",
        "\n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        #\n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = \\\n",
        "            pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        print('Total filenames: ', len(filenames), filenames[0])\n",
        "        #\n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        #\n",
        "        return filename_bbox\n",
        "\n",
        "    def load_captions(self, data_dir, filenames):\n",
        "        all_captions = []\n",
        "        for i in range(len(filenames)):\n",
        "            cap_path = '%s/text_c10/%s.txt' % (data_dir, filenames[i])\n",
        "            with open(cap_path, \"r\") as f:\n",
        "                captions = f.read().split('\\n')\n",
        "                cnt = 0\n",
        "                for cap in captions:\n",
        "                    if len(cap) == 0:\n",
        "                        continue\n",
        "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    # picks out sequences of alphanumeric characters as tokens\n",
        "                    # and drops everything else\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(cap.lower())\n",
        "                    # print('tokens', tokens)\n",
        "                    if len(tokens) == 0:\n",
        "                        print('cap', cap)\n",
        "                        continue\n",
        "\n",
        "                    tokens_new = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0:\n",
        "                            tokens_new.append(t)\n",
        "                    all_captions.append(tokens_new)\n",
        "                    cnt += 1\n",
        "                    if cnt == self.embeddings_num:\n",
        "                        break\n",
        "                if cnt < self.embeddings_num:\n",
        "                    print('ERROR: the captions for %s less than %d'\n",
        "                          % (filenames[i], cnt))\n",
        "        return all_captions\n",
        "\n",
        "    def build_dictionary(self, train_captions, test_captions):\n",
        "        word_counts = defaultdict(float)\n",
        "        captions = train_captions + test_captions\n",
        "        for sent in captions:\n",
        "            for word in sent:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
        "\n",
        "        ixtoword = {}\n",
        "        ixtoword[0] = '<end>'\n",
        "        wordtoix = {}\n",
        "        wordtoix['<end>'] = 0\n",
        "        ix = 1\n",
        "        for w in vocab:\n",
        "            wordtoix[w] = ix\n",
        "            ixtoword[ix] = w\n",
        "            ix += 1\n",
        "\n",
        "        train_captions_new = []\n",
        "        for t in train_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            train_captions_new.append(rev)\n",
        "\n",
        "        test_captions_new = []\n",
        "        for t in test_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            test_captions_new.append(rev)\n",
        "\n",
        "        return [train_captions_new, test_captions_new,\n",
        "                ixtoword, wordtoix, len(ixtoword)]\n",
        "\n",
        "    def load_text_data(self, data_dir, split):\n",
        "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
        "        train_names = self.load_filenames(data_dir, 'train')\n",
        "        test_names = self.load_filenames(data_dir, 'test')\n",
        "        if not os.path.isfile(filepath):\n",
        "            train_captions = self.load_captions(data_dir, train_names)\n",
        "            test_captions = self.load_captions(data_dir, test_names)\n",
        "\n",
        "            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n",
        "                self.build_dictionary(train_captions, test_captions)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump([train_captions, test_captions,\n",
        "                             ixtoword, wordtoix], f, protocol=2)\n",
        "                print('Save to: ', filepath)\n",
        "        else:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                x = pickle.load(f)\n",
        "                train_captions, test_captions = x[0], x[1]\n",
        "                ixtoword, wordtoix = x[2], x[3]\n",
        "                del x\n",
        "                n_words = len(ixtoword)\n",
        "                print('Load from: ', filepath)\n",
        "        if split == 'train':\n",
        "            # a list of list: each list contains\n",
        "            # the indices of words in a sentence\n",
        "            captions = train_captions\n",
        "            filenames = train_names\n",
        "        else:  # split=='test'\n",
        "            captions = test_captions\n",
        "            filenames = test_names\n",
        "        return filenames, captions, ixtoword, wordtoix, n_words\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f,encoding='bytes')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir, split):\n",
        "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                filenames = pickle.load(f)\n",
        "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        else:\n",
        "            filenames = []\n",
        "        return filenames\n",
        "\n",
        "    def get_caption(self, sent_ix):\n",
        "        # a list of indices for a sentence\n",
        "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
        "        if (sent_caption == 0).sum() > 0:\n",
        "            print('ERROR: do not need END (0) token', sent_caption)\n",
        "        num_words = len(sent_caption)\n",
        "        # pad with 0s (i.e., '<end>')\n",
        "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
        "        x_len = num_words\n",
        "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
        "            x[:num_words, 0] = sent_caption\n",
        "        else:\n",
        "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
        "            np.random.shuffle(ix)\n",
        "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
        "            ix = np.sort(ix)\n",
        "            x[:, 0] = sent_caption[ix]\n",
        "            x_len = cfg.TEXT.WORDS_NUM\n",
        "        return x, x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        key = self.filenames[index]\n",
        "        cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "        #\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        imgs = get_imgs(img_name, self.imsize,\n",
        "                        bbox, self.transform, normalize=self.norm)\n",
        "        # random select a sentence\n",
        "        sent_ix = random.randint(0, self.embeddings_num)\n",
        "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
        "        caps, cap_len = self.get_caption(new_sent_ix)\n",
        "        return imgs, caps, cap_len, cls_id, key\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0KtNYKlve-b"
      },
      "source": [
        "def save_(r,f,caption,fullpath):\n",
        "#im = cv2.imwrite(fullpath,r[0])\n",
        "    fullpath=fullpath\n",
        "    title_font = ImageFont.load_default()\n",
        "    im1 = Image.fromarray(np.uint8(f[0])).convert('RGB')\n",
        "    im2 = Image.fromarray(np.uint8(r[0])).convert('RGB')\n",
        "    im1=ImageOps.expand(im1,border=10,fill='white')\n",
        "    im2=ImageOps.expand(im2,border=10,fill='white')\n",
        "    #image_editable = ImageDraw.Draw(im1)\n",
        "    # W,H=im1.size\n",
        "    # text = 'Stage 1'\n",
        "    # w,h=image_editable.textsize(text)\n",
        "    # image_editable.text(((W-w)/2,(H-h)), text, (0,0,0), font=title_font)\n",
        "    image_editable = ImageDraw.Draw(im2)\n",
        "    W,H=im2.size\n",
        "    text = model_name\n",
        "    w,h=image_editable.textsize(text)\n",
        "    image_editable.text(((W-w)/2,(H-h)), text, (0,0,0), font=title_font)\n",
        "    # im=np.concatenate((np.uint8(im1),np.uint8(im2)),axis=1)\n",
        "    im = np.uint8(im2)\n",
        "    im = Image.fromarray(np.uint8(im)).convert('RGB')\n",
        "    im = ImageOps.expand(im,border=5,fill='white')\n",
        "    W,H=im.size\n",
        "\n",
        "    # title_text = caption#\"a colorful bird with a bright yellow body, a black crown and throat, orange bill, and black primaries and secondaries.\"\n",
        "    # image_editable = ImageDraw.Draw(im)\n",
        "    # lines = textwrap.wrap(title_text, width=60)\n",
        "    # ini = 1\n",
        "    # for line in lines:\n",
        "    #     w, h = image_editable.textsize(line)\n",
        "    #     image_editable.text(((W-w)/2,ini*h), line, (0,0,0), font=title_font)\n",
        "    #     ini+=1\n",
        "    # w, h = image_editable.textsize(title_text)\n",
        "    #image_editable.text(((W-w)/2,(H-h)*0.1), title_text, (0,0,0), font=title_font)\n",
        "    print(fullpath)\n",
        "    # im.show()\n",
        "    im.save('/content/static/results/{}.png'.format(image_name))\n",
        "    im.save(fullpath)\n",
        "#caption=\"a colorful bird with a bright yellow body, a black crown and throat, orange bill, and black primaries and secondaries.\"\n",
        "\n",
        "\n",
        "def build_super_images3(real_imgs, captions, ixtoword,\n",
        "                       attn_maps, att_sze, lr_imgs=None,\n",
        "                       batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                       max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    \n",
        "    nvis = 1\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    # text_convas = \\\n",
        "    #     np.ones([batch_size * FONT_MAX,\n",
        "    #              (max_word_num + 2) * (vis_size + 2), 3],\n",
        "    #             dtype=np.uint8)\n",
        "\n",
        "    # for i in range(max_word_num):\n",
        "    #     istart = (i + 2) * (vis_size + 2)\n",
        "    #     iend = (i + 3) * (vis_size + 2)\n",
        "    #     text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "    return [real_imgs,lr_imgs]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yRaNsSDV7EM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCLjvmeK024s"
      },
      "source": [
        "model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0curPjZ_2rBL"
      },
      "source": [
        "import torch.nn.parallel\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# from miscc.config import cfg\n",
        "# from GlobalAttention import GlobalAttentionGeneral as ATT_NET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6Fgconm2rEM"
      },
      "source": [
        "def conv1x1(in_planes, out_planes, bias=False):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "# Upsale the spatial size by a factor of 2\n",
        "def upBlock(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "        conv3x3(in_planes, out_planes * 2),\n",
        "        nn.BatchNorm2d(out_planes * 2),\n",
        "        GLU())\n",
        "    return block\n",
        "\n",
        "\n",
        "# Keep the spatial size\n",
        "def Block3x3_relu(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        conv3x3(in_planes, out_planes * 2),\n",
        "        nn.BatchNorm2d(out_planes * 2),\n",
        "        GLU())\n",
        "    return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABNQ5Ejg2rHa"
      },
      "source": [
        "class GLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GLU, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        nc = x.size(1)\n",
        "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
        "        nc = int(nc/2)\n",
        "        return x[:, :nc] * F.sigmoid(x[:, nc:])\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channel_num):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            conv3x3(channel_num, channel_num * 2),\n",
        "            nn.BatchNorm2d(channel_num * 2),\n",
        "            GLU(),\n",
        "            conv3x3(channel_num, channel_num),\n",
        "            nn.BatchNorm2d(channel_num))\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.block(x)\n",
        "        out += residual\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz-jVPOwmvGo"
      },
      "source": [
        "# ############## Text2Image Encoder-Decoder #######\n",
        "class RNN_ENCODER(nn.Module):\n",
        "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
        "                 nhidden=128, nlayers=1, bidirectional=True):\n",
        "        super(RNN_ENCODER, self).__init__()\n",
        "        self.n_steps = cfg.TEXT.WORDS_NUM\n",
        "        self.ntoken = ntoken  # size of the dictionary\n",
        "        self.ninput = ninput  # size of each embedding vector\n",
        "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
        "        self.nlayers = nlayers  # Number of recurrent layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn_type = cfg.RNN_TYPE\n",
        "        if bidirectional:\n",
        "            self.num_directions = 2\n",
        "        else:\n",
        "            self.num_directions = 1\n",
        "        # number of features in the hidden state\n",
        "        self.nhidden = nhidden // self.num_directions\n",
        "\n",
        "        self.define_module()\n",
        "        self.init_weights()\n",
        "\n",
        "    def define_module(self):\n",
        "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                               self.nlayers, batch_first=True,\n",
        "                               dropout=self.drop_prob,\n",
        "                               bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                              self.nlayers, batch_first=True,\n",
        "                              dropout=self.drop_prob,\n",
        "                              bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()),\n",
        "                    Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()))\n",
        "        else:\n",
        "            return Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                       bsz, self.nhidden).zero_())\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        emb = self.drop(self.encoder(captions))\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "        return words_emb, sent_emb\n",
        "\n",
        "class BERT_RNN_ENCODER(RNN_ENCODER):\n",
        "    def define_module(self):\n",
        "        self.encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.bert_linear = nn.Linear(768, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                               self.nlayers, batch_first=True,\n",
        "                               dropout=self.drop_prob,\n",
        "                               bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                              self.nlayers, batch_first=True,\n",
        "                              dropout=self.drop_prob,\n",
        "                              bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.bert_linear.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        \n",
        "        emb, _ = self.encoder(captions, output_all_encoded_layers=False)\n",
        "        emb = self.bert_linear(emb)\n",
        "        emb = self.drop(emb)\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "        return words_emb, sent_emb\n",
        "\n",
        "class GPT2_RNN_ENCODER(RNN_ENCODER):\n",
        "    def define_module(self):\n",
        "        self.encoder = GPT2Model.from_pretrained('gpt2')\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.gpt2_linear = nn.Linear(768, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                               self.nlayers, batch_first=True,\n",
        "                               dropout=self.drop_prob,\n",
        "                               bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                              self.nlayers, batch_first=True,\n",
        "                              dropout=self.drop_prob,\n",
        "                              bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.gpt2_linear.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        \n",
        "        emb, _ = self.encoder(captions)\n",
        "        emb = self.gpt2_linear(emb)\n",
        "        emb = self.drop(emb)\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "        return words_emb, sent_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp06MmvfrdlZ"
      },
      "source": [
        "class XLNet_RNN_ENCODER(RNN_ENCODER):\n",
        "    def define_module(self):\n",
        "        #from transformers import GPT2Model\n",
        "        self.encoder = XLNetModel.from_pretrained('xlnet-base-cased')\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.XLNet_linear = nn.Linear(768, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                               self.nlayers, batch_first=True,\n",
        "                               dropout=self.drop_prob,\n",
        "                               bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                              self.nlayers, batch_first=True,\n",
        "                              dropout=self.drop_prob,\n",
        "                              bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.XLNet_linear.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        \n",
        "        emb = self.encoder(captions,output_hidden_states =False)\n",
        "        emb= emb.last_hidden_state\n",
        "        #print(emb.shape)\n",
        "        emb = self.XLNet_linear(emb)\n",
        "        emb = self.drop(emb)\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "        return words_emb, sent_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-10BLTN52rSX"
      },
      "source": [
        "class CNN_ENCODER(nn.Module):\n",
        "    def __init__(self, nef):\n",
        "        super(CNN_ENCODER, self).__init__()\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.nef = nef\n",
        "        else:\n",
        "            self.nef = 256  # define a uniform ranker\n",
        "\n",
        "        model = models.inception_v3()\n",
        "        # url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        # model.load_state_dict(model_zoo.load_url(url))\n",
        "        state_dict = torch.load('/content/drive/My Drive/data/inception_v3.pth')\n",
        "        model.load_state_dict(state_dict)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        # print('Load pretrained model from ', url)\n",
        "        # print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
        "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
        "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
        "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
        "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
        "        self.Mixed_5b = model.Mixed_5b\n",
        "        self.Mixed_5c = model.Mixed_5c\n",
        "        self.Mixed_5d = model.Mixed_5d\n",
        "        self.Mixed_6a = model.Mixed_6a\n",
        "        self.Mixed_6b = model.Mixed_6b\n",
        "        self.Mixed_6c = model.Mixed_6c\n",
        "        self.Mixed_6d = model.Mixed_6d\n",
        "        self.Mixed_6e = model.Mixed_6e\n",
        "        self.Mixed_7a = model.Mixed_7a\n",
        "        self.Mixed_7b = model.Mixed_7b\n",
        "        self.Mixed_7c = model.Mixed_7c\n",
        "\n",
        "        self.emb_features = conv1x1(768, self.nef)\n",
        "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
        "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = None\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        # image region features\n",
        "        features = x\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.Mixed_7c(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = F.avg_pool2d(x, kernel_size=8)\n",
        "        # 1 x 1 x 2048\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # 1 x 1 x 2048\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 2048\n",
        "\n",
        "        # global image features\n",
        "        cnn_code = self.emb_cnn_code(x)\n",
        "        # 512\n",
        "        if features is not None:\n",
        "            features = self.emb_features(features)\n",
        "        return features, cnn_code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN2tmBTR2bvX"
      },
      "source": [
        "# ############## G networks ###################\n",
        "class CA_NET(nn.Module):\n",
        "    # some code is modified from vae examples\n",
        "    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n",
        "    def __init__(self):\n",
        "        super(CA_NET, self).__init__()\n",
        "        self.t_dim = cfg.TEXT.EMBEDDING_DIM\n",
        "        self.c_dim = cfg.GAN.CONDITION_DIM\n",
        "        self.fc = nn.Linear(self.t_dim, self.c_dim * 4, bias=True)\n",
        "        self.relu = GLU()\n",
        "\n",
        "    def encode(self, text_embedding):\n",
        "        x = self.relu(self.fc(text_embedding))\n",
        "        mu = x[:, :self.c_dim]\n",
        "        logvar = x[:, self.c_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparametrize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        if cfg.CUDA:\n",
        "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
        "        else:\n",
        "            eps = torch.FloatTensor(std.size()).normal_()\n",
        "        eps = Variable(eps)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def forward(self, text_embedding):\n",
        "        mu, logvar = self.encode(text_embedding)\n",
        "        c_code = self.reparametrize(mu, logvar)\n",
        "        return c_code, mu, logvar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87KQKYBS2byk"
      },
      "source": [
        "class INIT_STAGE_G(nn.Module):\n",
        "    def __init__(self, ngf, ncf):\n",
        "        super(INIT_STAGE_G, self).__init__()\n",
        "        self.gf_dim = ngf\n",
        "        self.in_dim = cfg.GAN.Z_DIM + ncf  # cfg.TEXT.EMBEDDING_DIM\n",
        "\n",
        "        self.define_module()\n",
        "\n",
        "    def define_module(self):\n",
        "        nz, ngf = self.in_dim, self.gf_dim\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(nz, ngf * 4 * 4 * 2, bias=False),\n",
        "            nn.BatchNorm1d(ngf * 4 * 4 * 2),\n",
        "            GLU())\n",
        "\n",
        "        self.upsample1 = upBlock(ngf, ngf // 2)\n",
        "        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n",
        "        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n",
        "        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n",
        "\n",
        "    def forward(self, z_code, c_code):\n",
        "        \"\"\"\n",
        "        :param z_code: batch x cfg.GAN.Z_DIM\n",
        "        :param c_code: batch x cfg.TEXT.EMBEDDING_DIM\n",
        "        :return: batch x ngf/16 x 64 x 64\n",
        "        \"\"\"\n",
        "        c_z_code = torch.cat((c_code, z_code), 1)\n",
        "        # state size ngf x 4 x 4\n",
        "        out_code = self.fc(c_z_code)\n",
        "        out_code = out_code.view(-1, self.gf_dim, 4, 4)\n",
        "        # state size ngf/3 x 8 x 8\n",
        "        out_code = self.upsample1(out_code)\n",
        "        # state size ngf/4 x 16 x 16\n",
        "        out_code = self.upsample2(out_code)\n",
        "        # state size ngf/8 x 32 x 32\n",
        "        out_code32 = self.upsample3(out_code)\n",
        "        # state size ngf/16 x 64 x 64\n",
        "        out_code64 = self.upsample4(out_code32)\n",
        "\n",
        "        return out_code64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbmfLNPt2b1S"
      },
      "source": [
        "class NEXT_STAGE_G(nn.Module):\n",
        "    def __init__(self, ngf, nef, ncf):\n",
        "        super(NEXT_STAGE_G, self).__init__()\n",
        "        self.gf_dim = ngf\n",
        "        self.ef_dim = nef\n",
        "        self.cf_dim = ncf\n",
        "        self.num_residual = cfg.GAN.R_NUM\n",
        "        self.define_module()\n",
        "\n",
        "    def _make_layer(self, block, channel_num):\n",
        "        layers = []\n",
        "        for i in range(cfg.GAN.R_NUM):\n",
        "            layers.append(block(channel_num))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def define_module(self):\n",
        "        ngf = self.gf_dim\n",
        "        # ATT_NET = GlobalAttentionGeneral()\n",
        "        self.att = GlobalAttentionGeneral(ngf, self.ef_dim)\n",
        "        self.residual = self._make_layer(ResBlock, ngf * 2)\n",
        "        self.upsample = upBlock(ngf * 2, ngf)\n",
        "\n",
        "    def forward(self, h_code, c_code, word_embs, mask):\n",
        "        \"\"\"\n",
        "            h_code1(query):  batch x idf x ih x iw (queryL=ihxiw)\n",
        "            word_embs(context): batch x cdf x sourceL (sourceL=seq_len)\n",
        "            c_code1: batch x idf x queryL\n",
        "            att1: batch x sourceL x queryL\n",
        "        \"\"\"\n",
        "        self.att.applyMask(mask)\n",
        "        c_code, att = self.att(h_code, word_embs)\n",
        "        h_c_code = torch.cat((h_code, c_code), 1)\n",
        "        out_code = self.residual(h_c_code)\n",
        "\n",
        "        # state size ngf/2 x 2in_size x 2in_size\n",
        "        out_code = self.upsample(out_code)\n",
        "\n",
        "        return out_code, att"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy8ZFi1S2cXm"
      },
      "source": [
        "class GET_IMAGE_G(nn.Module):\n",
        "    def __init__(self, ngf):\n",
        "        super(GET_IMAGE_G, self).__init__()\n",
        "        self.gf_dim = ngf\n",
        "        self.img = nn.Sequential(\n",
        "            conv3x3(ngf, 3),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, h_code):\n",
        "        out_img = self.img(h_code)\n",
        "        return out_img\n",
        "\n",
        "\n",
        "class G_NET(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(G_NET, self).__init__()\n",
        "        ngf = cfg.GAN.GF_DIM\n",
        "        nef = cfg.TEXT.EMBEDDING_DIM\n",
        "        ncf = cfg.GAN.CONDITION_DIM\n",
        "        self.ca_net = CA_NET()\n",
        "\n",
        "        if cfg.TREE.BRANCH_NUM > 0:\n",
        "            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf)\n",
        "            self.img_net1 = GET_IMAGE_G(ngf)\n",
        "        # gf x 64 x 64\n",
        "        if cfg.TREE.BRANCH_NUM > 1:\n",
        "            self.h_net2 = NEXT_STAGE_G(ngf, nef, ncf)\n",
        "            self.img_net2 = GET_IMAGE_G(ngf)\n",
        "        if cfg.TREE.BRANCH_NUM > 2:\n",
        "            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf)\n",
        "            self.img_net3 = GET_IMAGE_G(ngf)\n",
        "\n",
        "    def forward(self, z_code, sent_emb, word_embs, mask):\n",
        "        \"\"\"\n",
        "            :param z_code: batch x cfg.GAN.Z_DIM\n",
        "            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n",
        "            :param word_embs: batch x cdf x seq_len\n",
        "            :param mask: batch x seq_len\n",
        "            :return:\n",
        "        \"\"\"\n",
        "        fake_imgs = []\n",
        "        att_maps = []\n",
        "        c_code, mu, logvar = self.ca_net(sent_emb)\n",
        "\n",
        "        if cfg.TREE.BRANCH_NUM > 0:\n",
        "            h_code1 = self.h_net1(z_code, c_code)\n",
        "            fake_img1 = self.img_net1(h_code1)\n",
        "            fake_imgs.append(fake_img1)\n",
        "        if cfg.TREE.BRANCH_NUM > 1:\n",
        "            h_code2, att1 = \\\n",
        "                self.h_net2(h_code1, c_code, word_embs, mask)\n",
        "            fake_img2 = self.img_net2(h_code2)\n",
        "            fake_imgs.append(fake_img2)\n",
        "            if att1 is not None:\n",
        "                att_maps.append(att1)\n",
        "        if cfg.TREE.BRANCH_NUM > 2:\n",
        "            h_code3, att2 = \\\n",
        "                self.h_net3(h_code2, c_code, word_embs, mask)\n",
        "            fake_img3 = self.img_net3(h_code3)\n",
        "            fake_imgs.append(fake_img3)\n",
        "            if att2 is not None:\n",
        "                att_maps.append(att2)\n",
        "\n",
        "        return fake_imgs, att_maps, mu, logvar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV2dxetJ2HTv"
      },
      "source": [
        "class G_DCGAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(G_DCGAN, self).__init__()\n",
        "        ngf = cfg.GAN.GF_DIM\n",
        "        nef = cfg.TEXT.EMBEDDING_DIM\n",
        "        ncf = cfg.GAN.CONDITION_DIM\n",
        "        self.ca_net = CA_NET()\n",
        "\n",
        "        # 16gf x 64 x 64 --> gf x 64 x 64 --> 3 x 64 x 64\n",
        "        if cfg.TREE.BRANCH_NUM > 0:\n",
        "            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf)\n",
        "        # gf x 64 x 64\n",
        "        if cfg.TREE.BRANCH_NUM > 1:\n",
        "            self.h_net2 = NEXT_STAGE_G(ngf, nef, ncf)\n",
        "        if cfg.TREE.BRANCH_NUM > 2:\n",
        "            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf)\n",
        "        self.img_net = GET_IMAGE_G(ngf)\n",
        "\n",
        "    def forward(self, z_code, sent_emb, word_embs, mask):\n",
        "        \"\"\"\n",
        "            :param z_code: batch x cfg.GAN.Z_DIM\n",
        "            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n",
        "            :param word_embs: batch x cdf x seq_len\n",
        "            :param mask: batch x seq_len\n",
        "            :return:\n",
        "        \"\"\"\n",
        "        att_maps = []\n",
        "        c_code, mu, logvar = self.ca_net(sent_emb)\n",
        "        if cfg.TREE.BRANCH_NUM > 0:\n",
        "            h_code = self.h_net1(z_code, c_code)\n",
        "        if cfg.TREE.BRANCH_NUM > 1:\n",
        "            h_code, att1 = self.h_net2(h_code, c_code, word_embs, mask)\n",
        "            if att1 is not None:\n",
        "                att_maps.append(att1)\n",
        "        if cfg.TREE.BRANCH_NUM > 2:\n",
        "            h_code, att2 = self.h_net3(h_code, c_code, word_embs, mask)\n",
        "            if att2 is not None:\n",
        "                att_maps.append(att2)\n",
        "\n",
        "        fake_imgs = self.img_net(h_code)\n",
        "        return [fake_imgs], att_maps, mu, logvar\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYzyjDAc2HaM"
      },
      "source": [
        "\n",
        "# ############## D networks ##########################\n",
        "def Block3x3_leakRelu(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        conv3x3(in_planes, out_planes),\n",
        "        nn.BatchNorm2d(out_planes),\n",
        "        nn.LeakyReLU(0.2, inplace=True)\n",
        "    )\n",
        "    return block\n",
        "\n",
        "\n",
        "# Downsale the spatial size by a factor of 2\n",
        "def downBlock(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        nn.Conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_planes),\n",
        "        nn.LeakyReLU(0.2, inplace=True)\n",
        "    )\n",
        "    return block\n",
        "\n",
        "\n",
        "# Downsale the spatial size by a factor of 16\n",
        "def encode_image_by_16times(ndf):\n",
        "    encode_img = nn.Sequential(\n",
        "        # --> state size. ndf x in_size/2 x in_size/2\n",
        "        nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        # --> state size 2ndf x x in_size/4 x in_size/4\n",
        "        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(ndf * 2),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        # --> state size 4ndf x in_size/8 x in_size/8\n",
        "        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(ndf * 4),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        # --> state size 8ndf x in_size/16 x in_size/16\n",
        "        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(ndf * 8),\n",
        "        nn.LeakyReLU(0.2, inplace=True)\n",
        "    )\n",
        "    return encode_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgYm9o8m2HeF"
      },
      "source": [
        "class D_GET_LOGITS(nn.Module):\n",
        "    def __init__(self, ndf, nef, bcondition=False):\n",
        "        super(D_GET_LOGITS, self).__init__()\n",
        "        self.df_dim = ndf\n",
        "        self.ef_dim = nef\n",
        "        self.bcondition = bcondition\n",
        "        if self.bcondition:\n",
        "            self.jointConv = Block3x3_leakRelu(ndf * 8 + nef, ndf * 8)\n",
        "\n",
        "        self.outlogits = nn.Sequential(\n",
        "            nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, h_code, c_code=None):\n",
        "        if self.bcondition and c_code is not None:\n",
        "            # conditioning output\n",
        "            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
        "            c_code = c_code.repeat(1, 1, 4, 4)\n",
        "            # state size (ngf+egf) x 4 x 4\n",
        "            h_c_code = torch.cat((h_code, c_code), 1)\n",
        "            # state size ngf x in_size x in_size\n",
        "            h_c_code = self.jointConv(h_c_code)\n",
        "        else:\n",
        "            h_c_code = h_code\n",
        "\n",
        "        output = self.outlogits(h_c_code)\n",
        "        return output.view(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDvpPIGv2Hht"
      },
      "source": [
        "# For 64 x 64 images\n",
        "class D_NET64(nn.Module):\n",
        "    def __init__(self, b_jcu=True):\n",
        "        super(D_NET64, self).__init__()\n",
        "        ndf = cfg.GAN.DF_DIM\n",
        "        nef = cfg.TEXT.EMBEDDING_DIM\n",
        "        self.img_code_s16 = encode_image_by_16times(ndf)\n",
        "        if b_jcu:\n",
        "            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
        "        else:\n",
        "            self.UNCOND_DNET = None\n",
        "        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
        "\n",
        "    def forward(self, x_var):\n",
        "        x_code4 = self.img_code_s16(x_var)  # 4 x 4 x 8df\n",
        "        return x_code4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRqev0vV2HuN"
      },
      "source": [
        "# For 128 x 128 images\n",
        "class D_NET128(nn.Module):\n",
        "    def __init__(self, b_jcu=True):\n",
        "        super(D_NET128, self).__init__()\n",
        "        ndf = cfg.GAN.DF_DIM\n",
        "        nef = cfg.TEXT.EMBEDDING_DIM\n",
        "        self.img_code_s16 = encode_image_by_16times(ndf)\n",
        "        self.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n",
        "        self.img_code_s32_1 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n",
        "        #\n",
        "        if b_jcu:\n",
        "            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
        "        else:\n",
        "            self.UNCOND_DNET = None\n",
        "        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
        "\n",
        "    def forward(self, x_var):\n",
        "        x_code8 = self.img_code_s16(x_var)   # 8 x 8 x 8df\n",
        "        x_code4 = self.img_code_s32(x_code8)   # 4 x 4 x 16df\n",
        "        x_code4 = self.img_code_s32_1(x_code4)  # 4 x 4 x 8df\n",
        "        return x_code4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ABG5XVa2HzL"
      },
      "source": [
        "# For 256 x 256 images\n",
        "class D_NET256(nn.Module):\n",
        "    def __init__(self, b_jcu=True):\n",
        "        super(D_NET256, self).__init__()\n",
        "        ndf = cfg.GAN.DF_DIM\n",
        "        nef = cfg.TEXT.EMBEDDING_DIM\n",
        "        self.img_code_s16 = encode_image_by_16times(ndf)\n",
        "        self.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n",
        "        self.img_code_s64 = downBlock(ndf * 16, ndf * 32)\n",
        "        self.img_code_s64_1 = Block3x3_leakRelu(ndf * 32, ndf * 16)\n",
        "        self.img_code_s64_2 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n",
        "        if b_jcu:\n",
        "            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
        "        else:\n",
        "            self.UNCOND_DNET = None\n",
        "        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
        "\n",
        "    def forward(self, x_var):\n",
        "        x_code16 = self.img_code_s16(x_var)\n",
        "        x_code8 = self.img_code_s32(x_code16)\n",
        "        x_code4 = self.img_code_s64(x_code8)\n",
        "        x_code4 = self.img_code_s64_1(x_code4)\n",
        "        x_code4 = self.img_code_s64_2(x_code4)\n",
        "        return x_code4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iByj0nS1A3G"
      },
      "source": [
        "trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vaFFVVI_iEX"
      },
      "source": [
        "from __future__ import print_function\n",
        "from six.moves import range\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-b-vl1z0_LW"
      },
      "source": [
        "# ################# Text to image task############################ #\n",
        "class condGANTrainer(object):\n",
        "    def __init__(self, output_dir, data_loader, n_words, ixtoword, model):\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.model_dir = os.path.join(output_dir, 'Model')\n",
        "            self.image_dir = os.path.join(output_dir, 'Image')\n",
        "            mkdir_p(self.model_dir)\n",
        "            mkdir_p(self.image_dir)\n",
        "\n",
        "        torch.cuda.set_device(cfg.GPU_ID)\n",
        "        cudnn.benchmark = True\n",
        "        self.model = model\n",
        "        self.batch_size = cfg.TRAIN.BATCH_SIZE\n",
        "        self.max_epoch = cfg.TRAIN.MAX_EPOCH\n",
        "        self.snapshot_interval = cfg.TRAIN.SNAPSHOT_INTERVAL\n",
        "        self.predictions=[]\n",
        "        self.iMean = []\n",
        "        self.iStd = []\n",
        "        self.models_loaded = False\n",
        "        self.n_words = n_words\n",
        "        self.ixtoword = ixtoword\n",
        "        self.data_loader = data_loader\n",
        "        self.num_batches = len(self.data_loader)\n",
        "        self.net_e = cfg.TRAIN.NET_E\n",
        "        self.net_g = cfg.TRAIN.NET_G\n",
        "        self.models_loaded = False\n",
        "        #self.text_encoder, self.image_encoder, self.netG, self.netsD, self.start_epoch = self.build_models()\n",
        "        \n",
        "\n",
        "    def build_models(self):\n",
        "        # ###################encoders######################################## #\n",
        "        \n",
        "        if cfg.TRAIN.NET_E == '':\n",
        "            print('Error: no pretrained text-image encoders')\n",
        "            return\n",
        "        \n",
        "        image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "        cfg.TRAIN.NET_E = '/content/data/DAMSMencoders/{}/text_encoder200.pth'.format(self.model)\n",
        "        img_encoder_path = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "        # img_encoder_path = '/content/data/DAMSMencoders/bird/image_encoder200.pth'\n",
        "      \n",
        "        state_dict = \\\n",
        "            torch.load(img_encoder_path, map_location=lambda storage, loc: storage)\n",
        "        image_encoder.load_state_dict(state_dict)\n",
        "        for p in image_encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "        print('Load image encoder from:', img_encoder_path)\n",
        "        image_encoder.eval()\n",
        "\n",
        "        text_encoder = \\\n",
        "            RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "        state_dict = \\\n",
        "            torch.load(cfg.TRAIN.NET_E,\n",
        "                       map_location=lambda storage, loc: storage)\n",
        "        text_encoder.load_state_dict(state_dict)\n",
        "        for p in text_encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "        print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
        "        text_encoder.eval()\n",
        "\n",
        "        # #######################generator and discriminators############## #\n",
        "        netsD = []\n",
        "        if cfg.GAN.B_DCGAN:\n",
        "            if cfg.TREE.BRANCH_NUM ==1:\n",
        "                # from model import D_NET64 as D_NET\n",
        "                D_NET = D_NET64\n",
        "            elif cfg.TREE.BRANCH_NUM == 2:\n",
        "                # from model import D_NET128 as D_NET\n",
        "                D_NET = D_NET128\n",
        "            else:  # cfg.TREE.BRANCH_NUM == 3:\n",
        "                # from model import D_NET256 as D_NET\n",
        "                D_NET = D_NET256\n",
        "            # TODO: elif cfg.TREE.BRANCH_NUM > 3:\n",
        "            netG = G_DCGAN()\n",
        "            netsD = [D_NET(b_jcu=False)]\n",
        "        else:\n",
        "            # from model import D_NET64, D_NET128, D_NET256\n",
        "            netG = G_NET()\n",
        "            if cfg.TREE.BRANCH_NUM > 0:\n",
        "                netsD.append(D_NET64())\n",
        "            if cfg.TREE.BRANCH_NUM > 1:\n",
        "                netsD.append(D_NET128())\n",
        "            if cfg.TREE.BRANCH_NUM > 2:\n",
        "                netsD.append(D_NET256())\n",
        "            # TODO: if cfg.TREE.BRANCH_NUM > 3:\n",
        "        netG.apply(weights_init)\n",
        "        # print(netG)\n",
        "        for i in range(len(netsD)):\n",
        "            netsD[i].apply(weights_init)\n",
        "            # print(netsD[i])\n",
        "        print('# of netsD', len(netsD))\n",
        "        #\n",
        "        epoch = 0\n",
        "        if cfg.TRAIN.NET_G != '':\n",
        "            state_dict = \\\n",
        "                torch.load(cfg.TRAIN.NET_G, map_location=lambda storage, loc: storage)\n",
        "            netG.load_state_dict(state_dict)\n",
        "            print('Load G from: ', cfg.TRAIN.NET_G)\n",
        "            istart = cfg.TRAIN.NET_G.rfind('_') + 1\n",
        "            iend = cfg.TRAIN.NET_G.rfind('.')\n",
        "            epoch = cfg.TRAIN.NET_G[istart:iend]\n",
        "            epoch = int(epoch) + 1\n",
        "            if cfg.TRAIN.B_NET_D:\n",
        "                Gname = cfg.TRAIN.NET_G\n",
        "                for i in range(len(netsD)):\n",
        "                    s_tmp = Gname[:Gname.rfind('/')]\n",
        "                    Dname = '%s/netD%d.pth' % (s_tmp, i)\n",
        "                    print('Load D from: ', Dname)\n",
        "                    state_dict = \\\n",
        "                        torch.load(Dname, map_location=lambda storage, loc: storage)\n",
        "                    netsD[i].load_state_dict(state_dict)\n",
        "        # ########################################################### #\n",
        "        if cfg.CUDA:\n",
        "            text_encoder = text_encoder.cuda()\n",
        "            image_encoder = image_encoder.cuda()\n",
        "            netG.cuda()\n",
        "            for i in range(len(netsD)):\n",
        "                netsD[i].cuda()\n",
        "        self.models_loaded=True\n",
        "        return [text_encoder, image_encoder, netG, netsD, epoch]\n",
        "\n",
        "    def define_optimizers(self, netG, netsD):\n",
        "        optimizersD = []\n",
        "        num_Ds = len(netsD)\n",
        "        for i in range(num_Ds):\n",
        "            opt = optim.Adam(netsD[i].parameters(),\n",
        "                             lr=cfg.TRAIN.DISCRIMINATOR_LR,\n",
        "                             betas=(0.5, 0.999))\n",
        "            optimizersD.append(opt)\n",
        "\n",
        "        optimizerG = optim.Adam(netG.parameters(),\n",
        "                                lr=cfg.TRAIN.GENERATOR_LR,\n",
        "                                betas=(0.5, 0.999))\n",
        "\n",
        "        return optimizerG, optimizersD\n",
        "\n",
        "    def prepare_labels(self):\n",
        "        batch_size = self.batch_size\n",
        "        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n",
        "        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n",
        "        match_labels = Variable(torch.LongTensor(range(batch_size)))\n",
        "        if cfg.CUDA:\n",
        "            real_labels = real_labels.cuda()\n",
        "            fake_labels = fake_labels.cuda()\n",
        "            match_labels = match_labels.cuda()\n",
        "\n",
        "        return real_labels, fake_labels, match_labels\n",
        "\n",
        "    def save_model(self, netG, avg_param_G, netsD, epoch):\n",
        "        backup_para = copy_G_params(netG)\n",
        "        load_params(netG, avg_param_G)\n",
        "        torch.save(netG.state_dict(),\n",
        "            '%s/netG_epoch_%d.pth' % (self.model_dir, epoch))\n",
        "        load_params(netG, backup_para)\n",
        "        #\n",
        "        for i in range(len(netsD)):\n",
        "            netD = netsD[i]\n",
        "            torch.save(netD.state_dict(),\n",
        "                '%s/netD%d.pth' % (self.model_dir, i))\n",
        "        print('Save G/Ds models.')\n",
        "\n",
        "    def set_requires_grad_value(self, models_list, brequires):\n",
        "        for i in range(len(models_list)):\n",
        "            for p in models_list[i].parameters():\n",
        "                p.requires_grad = brequires\n",
        "\n",
        "    def save_img_results(self, netG, noise, sent_emb, words_embs, mask,\n",
        "                         image_encoder, captions, cap_lens,\n",
        "                         gen_iterations, name='current'):\n",
        "        # Save images\n",
        "        fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
        "        for i in range(len(attention_maps)):\n",
        "            if len(fake_imgs) > 1:\n",
        "                img = fake_imgs[i + 1].detach().cpu()\n",
        "                lr_img = fake_imgs[i].detach().cpu()\n",
        "            else:\n",
        "                img = fake_imgs[0].detach().cpu()\n",
        "                lr_img = None\n",
        "            attn_maps = attention_maps[i]\n",
        "            att_sze = attn_maps.size(2)\n",
        "            img_set, _ = \\\n",
        "                build_super_images(img, captions, self.ixtoword,\n",
        "                                   attn_maps, att_sze, lr_imgs=lr_img)\n",
        "            if not os.path.exists('/content/drive/My Drive/data/output_attn1/Model/naveen/'):#\n",
        "              os.makedirs('/content/drive/My Drive/data/output_attn1/Model/naveen/')\n",
        "            if img_set is not None:\n",
        "                im = Image.fromarray(img_set)\n",
        "                fullpath = '%s/G_%s_epoch-%d_%d.png'\\\n",
        "                    % ('/content/drive/My Drive/data/output_attn1/Model/naveen/', name, gen_iterations, i)\n",
        "                im.save(fullpath)\n",
        "\n",
        "        # for i in range(len(netsD)):\n",
        "        # i = -1\n",
        "        # img = fake_imgs[i].detach()\n",
        "        # region_features, _ = image_encoder(img)\n",
        "        # att_sze = region_features.size(2)\n",
        "        # _, _, att_maps = words_loss(region_features.detach(),\n",
        "        #                             words_embs.detach(),\n",
        "        #                             None, cap_lens,\n",
        "        #                             None, self.batch_size)\n",
        "        # img_set, _ = \\\n",
        "        #     build_super_images(fake_imgs[i].detach().cpu(),\n",
        "        #                        captions, self.ixtoword, att_maps, att_sze)\n",
        "        # if img_set is not None:\n",
        "        #     im = Image.fromarray(img_set)\n",
        "        #     fullpath = '%s/D_%s_epoch-%d.png'\\\n",
        "        #         % (self.image_dir, name, gen_iterations)\n",
        "        #     im.save(fullpath)\n",
        "\n",
        "    def train(self):\n",
        "        text_encoder, image_encoder, netG, netsD, start_epoch = self.build_models()\n",
        "        avg_param_G = copy_G_params(netG)\n",
        "        optimizerG, optimizersD = self.define_optimizers(netG, netsD)\n",
        "        real_labels, fake_labels, match_labels = self.prepare_labels()\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        nz = cfg.GAN.Z_DIM\n",
        "        noise = Variable(torch.FloatTensor(batch_size, nz))\n",
        "        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1))\n",
        "        if cfg.CUDA:\n",
        "            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
        "\n",
        "        gen_iterations = 0\n",
        "        # gen_iterations = start_epoch * self.num_batches\n",
        "        for epoch in trange(start_epoch, self.max_epoch):\n",
        "            start_t = time.time()\n",
        "            # print(start_time,\"######################################################################################\")\n",
        "            data_iter = iter(self.data_loader)\n",
        "            \n",
        "            for step in trange(self.num_batches):\n",
        "                # print(step,\"######################################################################################\")\n",
        "                # reset requires_grad to be trainable for all Ds\n",
        "                # self.set_requires_grad_value(netsD, True)\n",
        "\n",
        "                ######################################################\n",
        "                # (1) Prepare training data and Compute text embeddings\n",
        "                ######################################################\n",
        "                data = data_iter.next()\n",
        "                imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
        "\n",
        "                hidden = text_encoder.init_hidden(batch_size)\n",
        "                # words_embs: batch_size x nef x seq_len\n",
        "                # sent_emb: batch_size x nef\n",
        "                words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "                words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n",
        "                mask = (captions == 0)\n",
        "                num_words = words_embs.size(2)\n",
        "                if mask.size(1) > num_words:\n",
        "                    mask = mask[:, :num_words]\n",
        "\n",
        "                #######################################################\n",
        "                # (2) Generate fake images\n",
        "                ######################################################\n",
        "                noise.data.normal_(0, 1)\n",
        "                fake_imgs, _, mu, logvar = netG(noise, sent_emb, words_embs, mask)\n",
        "\n",
        "                #######################################################\n",
        "                # (3) Update D network\n",
        "                ######################################################\n",
        "                errD_total = 0\n",
        "                D_logs = ''\n",
        "                for i in range(len(netsD)):\n",
        "                    netsD[i].zero_grad()\n",
        "                    errD = discriminator_loss(netsD[i], imgs[i], fake_imgs[i],\n",
        "                                              sent_emb, real_labels, fake_labels)\n",
        "                    # backward and update parameters\n",
        "                    errD.backward()\n",
        "                    optimizersD[i].step()\n",
        "                    errD_total += errD\n",
        "                    D_logs += 'errD%d: %.2f ' % (i, errD)\n",
        "\n",
        "                #######################################################\n",
        "                # (4) Update G network: maximize log(D(G(z)))\n",
        "                ######################################################\n",
        "                # compute total loss for training G\n",
        "                \n",
        "                gen_iterations += 1\n",
        "\n",
        "                # do not need to compute gradient for Ds\n",
        "                # self.set_requires_grad_value(netsD, False)\n",
        "                netG.zero_grad()\n",
        "                errG_total, G_logs = \\\n",
        "                    generator_loss(netsD, image_encoder, fake_imgs, real_labels,\n",
        "                                   words_embs, sent_emb, match_labels, cap_lens, class_ids)\n",
        "                kl_loss = KL_loss(mu, logvar)\n",
        "                errG_total += kl_loss\n",
        "                G_logs += 'kl_loss: %.2f ' % kl_loss\n",
        "                # backward and update parameters\n",
        "                errG_total.backward()\n",
        "                optimizerG.step()\n",
        "                for p, avg_p in zip(netG.parameters(), avg_param_G):\n",
        "                    avg_p.mul_(0.999).add_(0.001, p.data)\n",
        "\n",
        "                if gen_iterations % 100 == 0:\n",
        "                    print(D_logs + '\\n' + G_logs)\n",
        "                # save images\n",
        "                # if gen_iterations % 100 == 0:\n",
        "                #     backup_para = copy_G_params(netG)\n",
        "                #     load_params(netG, avg_param_G)\n",
        "                #     self.save_img_results(netG, fixed_noise, sent_emb,\n",
        "                #                           words_embs, mask, image_encoder,\n",
        "                #                           captions, cap_lens, epoch, name='average')\n",
        "                #     load_params(netG, backup_para)\n",
        "                    #\n",
        "                    # self.save_img_results(netG, fixed_noise, sent_emb,\n",
        "                    #                       words_embs, mask, image_encoder,\n",
        "                    #                       captions, cap_lens,\n",
        "                    #                       epoch, name='current')\n",
        "            end_t = time.time()\n",
        "\n",
        "            print('''[%d/%d][%d]\n",
        "                  Loss_D: %.2f Loss_G: %.2f Time: %.2fs'''\n",
        "                  % (epoch, self.max_epoch, self.num_batches,\n",
        "                     errD_total, errG_total,\n",
        "                     end_t - start_t))\n",
        "\n",
        "            #if epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0:  # and epoch != 0:\n",
        "            backup_para = copy_G_params(netG)\n",
        "            load_params(netG, avg_param_G)\n",
        "            self.save_img_results(netG, fixed_noise, sent_emb,\n",
        "                                          words_embs, mask, image_encoder,\n",
        "                                          captions, cap_lens, epoch+1, name='average')\n",
        "            load_params(netG, backup_para)\n",
        "            self.save_model(netG, avg_param_G, netsD, epoch+1)\n",
        "\n",
        "        self.save_model(netG, avg_param_G, netsD, self.max_epoch)\n",
        "\n",
        "    def save_singleimages(self, images, filenames, save_dir,\n",
        "                          split_dir, sentenceID=0):\n",
        "        for i in range(images.size(0)):\n",
        "            s_tmp = '%s/single_samples/%s/%s' %\\\n",
        "                (save_dir, split_dir, filenames[i])\n",
        "            folder = s_tmp[:s_tmp.rfind('/')]\n",
        "            if not os.path.isdir(folder):\n",
        "                print('Make a new folder: ', folder)\n",
        "                mkdir_p(folder)\n",
        "\n",
        "            fullpath = '%s_%d.jpg' % (s_tmp, sentenceID)\n",
        "            # range from [-1, 1] to [0, 1]\n",
        "            # img = (images[i] + 1.0) / 2\n",
        "            img = images[i].add(1).div(2).mul(255).clamp(0, 255).byte()\n",
        "            # range from [0, 1] to [0, 255]\n",
        "            ndarr = img.permute(1, 2, 0).data.cpu().numpy()\n",
        "            im = Image.fromarray(ndarr)\n",
        "            im.save(fullpath)\n",
        "\n",
        "    def sampling(self, split_dir):\n",
        "        if cfg.TRAIN.NET_G == '':\n",
        "            print('Error: the path for morels is not found!')\n",
        "        else:\n",
        "            if split_dir == 'test':\n",
        "                split_dir = 'valid'\n",
        "            # Build and load the generator\n",
        "            if cfg.GAN.B_DCGAN:\n",
        "                netG = G_DCGAN()\n",
        "            else:\n",
        "                netG = G_NET()\n",
        "            netG.apply(weights_init)\n",
        "            netG.cuda()\n",
        "            netG.eval()\n",
        "            #\n",
        "            text_encoder = RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "            state_dict = \\\n",
        "                torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n",
        "            text_encoder.load_state_dict(state_dict)\n",
        "            print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
        "            text_encoder = text_encoder.cuda()\n",
        "            text_encoder.eval()\n",
        "            \n",
        "            # image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "            # cfg.TRAIN.NET_E = '/content/data/DAMSMencoders/bird/text_encoder200.pth'\n",
        "            # img_encoder_path = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "            # # img_encoder_path = '/content/data/DAMSMencoders/bird/image_encoder200.pth'\n",
        "          \n",
        "            # state_dict = \\\n",
        "            #     torch.load(img_encoder_path, map_location=lambda storage, loc: storage)\n",
        "            # image_encoder.load_state_dict(state_dict)\n",
        "            # for p in image_encoder.parameters():\n",
        "            #     p.requires_grad = False\n",
        "            # print('Load image encoder from:', img_encoder_path)\n",
        "            # image_encoder.eval()\n",
        "\n",
        "            batch_size = self.batch_size\n",
        "            nz = cfg.GAN.Z_DIM\n",
        "            noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
        "            noise = noise.cuda()\n",
        "\n",
        "            model_dir = cfg.TRAIN.NET_G\n",
        "            state_dict = \\\n",
        "                torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
        "            # state_dict = torch.load(cfg.TRAIN.NET_G)\n",
        "            netG.load_state_dict(state_dict)\n",
        "            print('Load G from: ', model_dir)\n",
        "\n",
        "            # the path to save generated images\n",
        "            s_tmp = model_dir[:model_dir.rfind('.pth')]\n",
        "            save_dir = '%s/%s' % (s_tmp, split_dir)\n",
        "            mkdir_p(save_dir)\n",
        "\n",
        "            cnt = 0\n",
        "\n",
        "            for _ in range(1):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n",
        "                for step, data in enumerate(self.data_loader, 0):\n",
        "                    cnt += batch_size\n",
        "                    if step % 100 == 0:\n",
        "                        print('step: ', step)\n",
        "                    # if step > 50:\n",
        "                    #     break\n",
        "\n",
        "                    imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
        "\n",
        "                    hidden = text_encoder.init_hidden(batch_size)\n",
        "                    # words_embs: batch_size x nef x seq_len\n",
        "                    # sent_emb: batch_size x nef\n",
        "                    words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "                    words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n",
        "                    mask = (captions == 0)\n",
        "                    num_words = words_embs.size(2)\n",
        "                    if mask.size(1) > num_words:\n",
        "                        mask = mask[:, :num_words]\n",
        "\n",
        "                    #######################################################\n",
        "                    # (2) Generate fake images\n",
        "                    ######################################################\n",
        "                    # self.save_img_results(netG, noise, sent_emb, words_embs, mask,\n",
        "                    #    image_encoder, captions, cap_lens,\n",
        "                    #    step*100000, name='Naveen',directory)\n",
        "                    # continue\n",
        "                    \n",
        "                    noise.data.normal_(0, 1)\n",
        "                    fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
        "                    for i in range(len(attention_maps)):\n",
        "                        s_tmp = '%s/naveen/%s' % (save_dir, keys[i])\n",
        "                        folder = s_tmp[:s_tmp.rfind('/')]\n",
        "                        if not os.path.isdir(folder):\n",
        "                            print('Make a new folder: ', folder)\n",
        "                            mkdir_p(folder)\n",
        "                        if len(fake_imgs) > 1:\n",
        "                            img = fake_imgs[i + 1].detach().cpu()\n",
        "                            lr_img = fake_imgs[i].detach().cpu()\n",
        "                        else:\n",
        "                            img = fake_imgs[0].detach().cpu()\n",
        "                            lr_img = None\n",
        "                        attn_maps = attention_maps[i]\n",
        "                        att_sze = attn_maps.size(2)\n",
        "                        img_set, _ = \\\n",
        "                            build_super_images(img, captions, self.ixtoword,\n",
        "                                              attn_maps, att_sze, lr_imgs=lr_img)\n",
        "                        if img_set is not None:\n",
        "                            im = Image.fromarray(img_set)\n",
        "                            fullpath = '%s_s%d.png'\\\n",
        "                                % (s_tmp, i)\n",
        "                            im.save(fullpath)\n",
        "                    #    k = -1\n",
        "                    #     # for k in range(len(fake_imgs)):\n",
        "                    #     im = fake_imgs[k][j].data.cpu().numpy()\n",
        "                    #     # [-1, 1] --> [0, 255]\n",
        "                    #     im = (im + 1.0) * 127.5\n",
        "                    #     im = im.astype(np.uint8)\n",
        "                    #     im = np.transpose(im, (1, 2, 0))\n",
        "                    #     im = Image.fromarray(im)\n",
        "                    #     fullpath = '%s_s%d.png' % (s_tmp, k)\n",
        "                    #     im.save(fullpath)\n",
        "\n",
        "    def gen_example2(self, data_dic):\n",
        "        if cfg.TRAIN.NET_G == '':\n",
        "            print('Error: the path for morels is not found!')\n",
        "        else:\n",
        "            # Build and load the generator\n",
        "            text_encoder = \\\n",
        "                RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "            state_dict = \\\n",
        "                torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n",
        "            text_encoder.load_state_dict(state_dict)\n",
        "            print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
        "            text_encoder = text_encoder.cuda()\n",
        "            text_encoder.eval()\n",
        "\n",
        "            # the path to save generated images\n",
        "            if cfg.GAN.B_DCGAN:\n",
        "                netG = G_DCGAN()\n",
        "            else:\n",
        "                netG = G_NET()\n",
        "            s_tmp = cfg.TRAIN.NET_G[:cfg.TRAIN.NET_G.rfind('.pth')]\n",
        "            model_dir = cfg.TRAIN.NET_G\n",
        "            state_dict = \\\n",
        "                torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
        "            netG.load_state_dict(state_dict)\n",
        "            print('Load G from: ', model_dir)\n",
        "            netG.cuda()\n",
        "            netG.eval()\n",
        "            image = 1\n",
        "            print(len(data_dic))\n",
        "            for key in data_dic:\n",
        "                save_dir = '%s/%s' % (s_tmp, key)\n",
        "                mkdir_p(save_dir)\n",
        "                captions, cap_lens, sorted_indices,texts = data_dic[key]\n",
        "                #print(captions)\n",
        "                #print(data_dic[key])\n",
        "\n",
        "                batch_size = captions.shape[0]\n",
        "                nz = cfg.GAN.Z_DIM\n",
        "                captions = Variable(torch.from_numpy(captions), volatile=True)\n",
        "                cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n",
        "\n",
        "                captions = captions.cuda()\n",
        "                cap_lens = cap_lens.cuda()\n",
        "                \n",
        "                for _ in range(1):  # 16\n",
        "                    noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
        "                    noise = noise.cuda()\n",
        "                    #######################################################\n",
        "                    # (1) Extract text embeddings\n",
        "                    ######################################################\n",
        "                    hidden = text_encoder.init_hidden(batch_size)\n",
        "                    # words_embs: batch_size x nef x seq_len\n",
        "                    # sent_emb: batch_size x nef\n",
        "                    words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "                    mask = (captions == 0)\n",
        "                    #######################################################\n",
        "                    # (2) Generate fake images\n",
        "                    ######################################################\n",
        "                    noise.data.normal_(0, 1)\n",
        "                    fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
        "                    # G attention\n",
        "                    cap_lens_np = cap_lens.cpu().data.numpy()\n",
        "                    for k in range(batch_size):\n",
        "                        for i in range(len(attention_maps)):\n",
        "                            s_tmp = '%s/trails/%s' % (save_dir,'examples')\n",
        "                            folder = s_tmp[:s_tmp.rfind('/')]\n",
        "                            if not os.path.isdir(folder):\n",
        "                                print('Make a new folder: ', folder)\n",
        "                                mkdir_p(folder)\n",
        "                            if len(fake_imgs) > 1:\n",
        "                                img = fake_imgs[i + 1].detach().cpu()\n",
        "                                lr_img = fake_imgs[i].detach().cpu()\n",
        "                            else:\n",
        "                                img = fake_imgs[0].detach().cpu()\n",
        "                                lr_img = None\n",
        "                            attn_maps = attention_maps[i]\n",
        "                            #img = Image.fromarray(img)\n",
        "                            to_pil_image = transforms.ToPILImage()\n",
        "                            print(img.shape)\n",
        "                            img = to_pil_image(img[0])\n",
        "                            draw = ImageDraw.Draw(img)\n",
        "                            font = ImageFont.load_default()\n",
        "                            draw.text((0, 0), texts[i], (255, 255, 255), font=font)\n",
        "\n",
        "                            fullpath = '%s_s%d.png' % (s_tmp, image)\n",
        "                            self.image+=1\n",
        "                            #print(fullpath)\n",
        "                            img.save(fullpath)\n",
        "                            # img_set, _ = \\\n",
        "                            #     build_super_images(img, captions, self.ixtoword,\n",
        "                            #                       attn_maps, att_sze, lr_imgs=lr_img)\n",
        "                            # if img_set is not None:\n",
        "                            #     im = Image.fromarray(img_set)\n",
        "                            #     fullpath = '%s_s%d.png'\\\n",
        "                            #         % (s_tmp, image)\n",
        "                            #     image+=1\n",
        "                            #     print(fullpath)\n",
        "                            #     im.save(fullpath)\n",
        "                 \n",
        "    def get_inception(self,epoch, data_dic,inception_model):\n",
        "        \n",
        "        if cfg.TRAIN.NET_G == '':\n",
        "            print('Error: the path for morels is not found!')\n",
        "        else:\n",
        "            # Build and load the generator\n",
        "            text_encoder = \\\n",
        "                RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "            state_dict = \\\n",
        "                torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n",
        "            text_encoder.load_state_dict(state_dict)\n",
        "            #print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
        "            text_encoder = text_encoder.cuda()\n",
        "            text_encoder.eval()\n",
        "\n",
        "            # the path to save generated images\n",
        "            if cfg.GAN.B_DCGAN:\n",
        "                netG = G_DCGAN()\n",
        "            else:\n",
        "                netG = G_NET()\n",
        "            s_tmp = cfg.TRAIN.NET_G[:cfg.TRAIN.NET_G.rfind('.pth')]\n",
        "            model_dir = cfg.TRAIN.NET_G\n",
        "            state_dict = \\\n",
        "                torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
        "            netG.load_state_dict(state_dict)\n",
        "            #print('Load G from: ', model_dir)\n",
        "            netG.cuda()\n",
        "            netG.eval()\n",
        "            #image = 1\n",
        "            #print(len(data_dic))\n",
        "            for key in data_dic:\n",
        "                lis = key.split('_')\n",
        "                save_dir = '%s/%s/%s' % (s_tmp,'naveen','_'.join(lis[:-2]))\n",
        "                mkdir_p(save_dir)\n",
        "                captions, cap_lens, sorted_indices,texts = data_dic[key]\n",
        "                #print(captions)\n",
        "                #print(data_dic[key])\n",
        "\n",
        "                batch_size = captions.shape[0]\n",
        "                nz = cfg.GAN.Z_DIM\n",
        "                captions = Variable(torch.from_numpy(captions), volatile=True)\n",
        "                cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n",
        "\n",
        "                captions = captions.cuda()\n",
        "                cap_lens = cap_lens.cuda()\n",
        "                num = captions.size(0)\n",
        "                sentence_list = []\n",
        "                v_size=128\n",
        "                for i in range(num):\n",
        "                    cap = captions[i].data.cpu().numpy()\n",
        "                    #print(cap)\n",
        "                    sentence = []\n",
        "                    for j in range(len(cap)):\n",
        "                        if cap[j] == 0:\n",
        "                            break\n",
        "                        word = dataset.ixtoword[cap[j]]#.encode('ascii', 'ignore').decode('ascii')\n",
        "                        sentence.append(word)\n",
        "                    \n",
        "                    sentence_list.append(' '.join(sentence))\n",
        "                sentence_list\n",
        "                for _ in range(1):  # 16\n",
        "                    noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
        "                    noise = noise.cuda()\n",
        "                    #######################################################\n",
        "                    # (1) Extract text embeddings\n",
        "                    ######################################################\n",
        "                    hidden = text_encoder.init_hidden(batch_size)\n",
        "                    # words_embs: batch_size x nef x seq_len\n",
        "                    # sent_emb: batch_size x nef\n",
        "                    words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "                    mask = (captions == 0)\n",
        "                    #######################################################\n",
        "                    # (2) Generate fake images\n",
        "                    ######################################################\n",
        "                    noise.data.normal_(0, 1)\n",
        "                    fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
        "                    # G attention\n",
        "                    #self.image=1\n",
        "                    cap_lens_np = cap_lens.cpu().data.numpy()\n",
        "                    pred = inception_model(fake_imgs[-1].detach())\n",
        "                    self.predictions.append(pred.data.cpu().numpy())\n",
        "                    s_tmp = '/content/drive/MyDrive/data/output_attn1/Log'\n",
        "                    if len(self.predictions) >= 419:\n",
        "                        self.predictions = np.concatenate(self.predictions, 0)\n",
        "                        mean, std = compute_inception_score(self.predictions, 10)\n",
        "                        # print('mean:', mean, 'std', std)\n",
        "                        # m_incep = summary.scalar('Inception_mean', mean)\n",
        "\n",
        "                        # self.iMean += [mean]\n",
        "                        # self.iStd += [std]\n",
        "\n",
        "                        # if(len(self.iMean)==29):\n",
        "                        #   self.iMean = np.array(self.iMean)\n",
        "                        #   self.iStd = np.array(self.iStd)\n",
        "                        print('Inception mean',mean,'Inception std',std,f'of epoch {epoch}')\n",
        "                        fields=['Epoch','Mean','Std']\n",
        "                        mydict={'Epoch':epoch,'Mean':mean,'Std':std}\n",
        "                        file_exists = os.path.isfile('%s/log_IS1.csv'% s_tmp)\n",
        "                        with open('%s/log_IS1.csv'% s_tmp,'a') as csvfile:\n",
        "                            writer = csv.DictWriter(csvfile, fieldnames = fields)  \n",
        "                            # writing headers (field names) \n",
        "                            if not  file_exists:\n",
        "                                print('File saved at %s/log_IS1.csv'% s_tmp)\n",
        "                                writer.writeheader()  \n",
        "                                \n",
        "                            # writing data rows  \n",
        "                            writer.writerow(mydict)\n",
        "                            csvfile.close()\n",
        "                            # self.iMean = []\n",
        "                            # self.iStd = []\n",
        "                        \n",
        "                        # self.summary_writer.add_summary(m_incep, count)\n",
        "                        # #\n",
        "                        # mean_nlpp, std_nlpp = \\\n",
        "                        #     negative_log_posterior_probability(predictions, 10)\n",
        "                        # m_nlpp = summary.scalar('NLPP_mean', mean_nlpp)\n",
        "                        # self.summary_writer.add_summary(m_nlpp, count)\n",
        "                        #\n",
        "                        self.predictions = []\n",
        "\n",
        "    def gen_example(self, epoch,data_dic):\n",
        "        \n",
        "        if cfg.TRAIN.NET_G == '':\n",
        "            print('Error: the path for morels is not found!')\n",
        "        else:\n",
        "            # Build and load the generator\n",
        "          \n",
        "            start = time.time()\n",
        "            if self.model == 'gpt2':\n",
        "              text_encoder = \\\n",
        "                  GPT2_RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "            elif self.model == 'xlnet':\n",
        "              text_encoder = \\\n",
        "                  XLNet_RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "            elif self.model == 'bert':\n",
        "              text_encoder = \\\n",
        "                  BERT_RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "            elif self.model == 'attngan':\n",
        "              text_encoder = \\\n",
        "                  RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "            else:\n",
        "              print('Select correct model')\n",
        "              return\n",
        "            end = time.time()\n",
        "            print(\"text_encoder\",end-start)\n",
        "            start = time.time()\n",
        "  \n",
        "            state_dict = \\\n",
        "                torch.load(self.net_e, map_location=lambda storage, loc: storage)\n",
        "            text_encoder.load_state_dict(state_dict)\n",
        "\n",
        "            end = time.time()\n",
        "            print(\"text_encoder1\",end-start)\n",
        "            start = time.time()\n",
        "\n",
        "            print('Load text encoder from:',self.net_e)\n",
        "            if cfg.CUDA:\n",
        "              text_encoder = text_encoder.cuda()\n",
        "            text_encoder.eval()\n",
        "\n",
        "            end = time.time()\n",
        "            print(\"text_encoder2\",end-start)\n",
        "            start = time.time()\n",
        "\n",
        "            # the path to save generated images\n",
        "            if cfg.GAN.B_DCGAN:\n",
        "                netG = G_DCGAN()\n",
        "            else:\n",
        "                netG = G_NET()\n",
        "            \n",
        "            model_dir = self.net_g\n",
        "            state_dict = \\\n",
        "                torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
        "            netG.load_state_dict(state_dict)\n",
        "\n",
        "            end = time.time()\n",
        "            print(\"model\",end-start)\n",
        "            start = time.time()\n",
        "\n",
        "\n",
        "            print('Load G from: ', model_dir)\n",
        "            if cfg.CUDA:\n",
        "              netG.cuda()\n",
        "            netG.eval()\n",
        "\n",
        "            end = time.time()\n",
        "            print(\"model2\",end-start)\n",
        "            start = time.time()\n",
        "            \n",
        "\n",
        "            #image = 1\n",
        "            #print(len(data_dic))\n",
        "            s_tmp = '/content/drive/My Drive/data/output_attn1'\n",
        "            for key in data_dic:\n",
        "                lis = key.split('_')\n",
        "                save_dir = '%s/%s%d/%s' % (s_tmp,'Eval/1504/epoch',epoch,'_'.join(lis[:-2]))\n",
        "                mkdir_p(save_dir)\n",
        "                captions, cap_lens, sorted_indices,texts = data_dic[key]\n",
        "                #print(captions)\n",
        "                #print(data_dic[key])\n",
        "\n",
        "                batch_size = captions.shape[0]\n",
        "                nz = cfg.GAN.Z_DIM\n",
        "                captions = Variable(torch.from_numpy(captions), volatile=True)\n",
        "                cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n",
        "                if cfg.CUDA:\n",
        "                  captions = captions.cuda()\n",
        "                  cap_lens = cap_lens.cuda()\n",
        "                num = captions.size(0)\n",
        "                sentence_list = []\n",
        "                v_size=128\n",
        "                for i in range(num):\n",
        "                    cap = captions[i].data.cpu().numpy()\n",
        "                    #print(cap)\n",
        "                    sentence = []\n",
        "                    for j in range(len(cap)):\n",
        "                        if cap[j] == 0:\n",
        "                            break\n",
        "                        word = dataset.ixtoword[cap[j]]#.encode('ascii', 'ignore').decode('ascii')\n",
        "                        sentence.append(word)\n",
        "                    \n",
        "                    sentence_list.append(' '.join(sentence))\n",
        "                sentence_list\n",
        "                for _ in range(1):  # 16\n",
        "                    noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
        "                    if cfg.CUDA:\n",
        "                      noise = noise.cuda()\n",
        "                    #######################################################\n",
        "                    # (1) Extract text embeddings\n",
        "                    ######################################################\n",
        "                    hidden = text_encoder.init_hidden(batch_size)\n",
        "                    # words_embs: batch_size x nef x seq_len\n",
        "                    # sent_emb: batch_size x nef\n",
        "                    words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "                    mask = (captions == 0)\n",
        "                    #######################################################\n",
        "                    # (2) Generate fake images\n",
        "                    ######################################################\n",
        "                    noise.data.normal_(0, 1)\n",
        "                    fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
        "                    # G attention\n",
        "                    self.image=1\n",
        "                    cap_lens_np = cap_lens.cpu().data.numpy()\n",
        "                    for k in range(batch_size):\n",
        "                        #print(len(fake_imgs))\n",
        "                        #for i in range(len(attention_maps)):\n",
        "                        s_tmp = '%s/%s' % (save_dir,'_'.join(lis[-2:]))\n",
        "                        folder = s_tmp[:s_tmp.rfind('/')]\n",
        "                        if not os.path.isdir(folder):\n",
        "                            print('Make a new folder: ', folder)\n",
        "                            mkdir_p(folder)\n",
        "                        if len(fake_imgs) > 1:\n",
        "                            img = fake_imgs[-1].detach().cpu()\n",
        "                            lr_img = fake_imgs[-2].detach().cpu()\n",
        "                        else:\n",
        "                            img = fake_imgs[0].detach().cpu()\n",
        "                            lr_img = None\n",
        "                    \n",
        "                        attn_maps = attention_maps[-1]\n",
        "                        att_sze = attn_maps.size(2)\n",
        "                        r,f= \\\n",
        "                            build_super_images3(img[k].unsqueeze(0), captions[k].unsqueeze(0), self.ixtoword,\n",
        "                                              [attn_maps[k]], att_sze, lr_imgs=lr_img[k].unsqueeze(0))\n",
        "                        \n",
        "                        if r is not None:\n",
        "                            \n",
        "                            fullpath = '%s_%d%d.png'\\\n",
        "                                % (s_tmp,calendar.timegm(time.gmtime()),self.image)\n",
        "                            save_(r,f,sentence_list[k],fullpath)\n",
        "                            self.image+=1\n",
        "                            # print(calendar.timegm(time.gmtime()))\n",
        "                                \n",
        "                    # for j in range(batch_size):\n",
        "                    #     save_name = '%s/%d_s_%d' % (save_dir, i, sorted_indices[j])\n",
        "                    #     print(len(fake_imgs))\n",
        "                    #     print(fake_imgs[0].shape)\n",
        "                    #     for k in range(len(fake_imgs)):\n",
        "                    #         im = fake_imgs[k][j].data.cpu().numpy()\n",
        "                    #         im = (im + 1.0) * 127.5\n",
        "                    #         im = im.astype(np.uint8)\n",
        "                    #         # print('im', im.shape)\n",
        "                    #         im = np.transpose(im, (1, 2, 0))\n",
        "                    #         # print('im', im.shape)\n",
        "                    #         im = Image.fromarray(im)\n",
        "                    #         fullpath = '%s_g%d.png' % (save_name, k)\n",
        "                    #         im.save(fullpath)\n",
        "                    #     print(len(attention_maps))\n",
        "                    #     print(attention_maps[0].shape)\n",
        "                    #     for k in range(len(attention_maps)):\n",
        "                    #         if len(fake_imgs) > 1:\n",
        "                    #             im = fake_imgs[k + 1].detach().cpu()\n",
        "                    #         else:\n",
        "                    #             im = fake_imgs[0].detach().cpu()\n",
        "                    #         attn_maps = attention_maps[k]\n",
        "                    #         att_sze = attn_maps.size(2)\n",
        "                    #         img_set, sentences = \\\n",
        "                    #             build_super_images2(im[j].unsqueeze(0),\n",
        "                    #                                 captions[j].unsqueeze(0),\n",
        "                    #                                 [cap_lens_np[j]], self.ixtoword,\n",
        "                    #                                 [attn_maps[j]], att_sze)\n",
        "                    #         if img_set is not None:\n",
        "                    #             im = Image.fromarray(img_set)\n",
        "                    #             fullpath = '%s_a%d.png' % (save_name, k)\n",
        "                    #             im.save(fullpath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCekK3st00N1"
      },
      "source": [
        "main.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbydfT8B6_5J"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "# from miscc.config import cfg, cfg_from_file\n",
        "# from datasets import TextDataset\n",
        "# from trainer import condGANTrainer as trainer\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZh0VIl27I7h"
      },
      "source": [
        "def gen_example(epoch, wordtoix, algo,inception=False):\n",
        "    '''generate images from example sentences'''\n",
        "    from nltk.tokenize import RegexpTokenizer\n",
        "    filepath = '%s/example_filenames.txt' % (cfg.DATA_DIR)\n",
        "    #filepath = '/content/data/birds/'\n",
        "    data_dic = {}\n",
        "    with open(filepath, \"r\") as f:\n",
        "        filenames = f.read().split('\\n')\n",
        "        for name in filenames:\n",
        "            if len(name) == 0:\n",
        "                continue\n",
        "            filepath = '%s/%s.txt' % (cfg.DATA_DIR, name)\n",
        "            with open(filepath, \"r\") as f:\n",
        "                print('Load from:', name)\n",
        "                sentences = f.read().split('\\n')\n",
        "                # a list of indices for a sentence\n",
        "                texts=[]\n",
        "                captions = []\n",
        "                cap_lens = []\n",
        "                for sent in sentences:\n",
        "                    #print(sent)\n",
        "                    if len(sent) == 0:\n",
        "                        continue\n",
        "                    sent = sent.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(sent.lower())\n",
        "                    if len(tokens) == 0:\n",
        "                        print('sent', sent)\n",
        "                        continue\n",
        "\n",
        "                    rev = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0 and t in wordtoix:\n",
        "                            rev.append(wordtoix[t])\n",
        "                    captions.append(rev)\n",
        "                    cap_lens.append(len(rev))\n",
        "                    texts.append(sent)\n",
        "            max_len = np.max(cap_lens)\n",
        "\n",
        "            sorted_indices = np.argsort(cap_lens)[::-1]\n",
        "            cap_lens = np.asarray(cap_lens)\n",
        "            cap_lens = cap_lens[sorted_indices]\n",
        "            cap_array = np.zeros((len(captions), max_len), dtype='int64')\n",
        "            for i in range(len(captions)):\n",
        "                idx = sorted_indices[i]\n",
        "                cap = captions[idx]\n",
        "                c_len = len(cap)\n",
        "                cap_array[i, :c_len] = cap\n",
        "            key = name[(name.rfind('/') + 1):]\n",
        "            data_dic[key] = [cap_array, cap_lens, sorted_indices,texts]\n",
        "    # inception_model = INCEPTION_V3()\n",
        "    # if cfg.CUDA:\n",
        "    #     inception_model = inception_model.cuda()\n",
        "    # inception_model.eval()\n",
        "    for k,v in data_dic.items():\n",
        "      if inception:\n",
        "        algo.get_inception(epoch, dict([(k,v)]),inception_model)\n",
        "      else:\n",
        "        algo.gen_example(epoch, dict([(k,v)]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6RknCop6_wp"
      },
      "source": [
        "__file__ = '/content/data/'\n",
        "dir_path = (os.path.abspath(os.path.join(os.path.realpath(__file__),'./')))\n",
        "sys.path.append(dir_path)\n",
        "from torchvision import transforms\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Train a AttnGAN network')\n",
        "    parser.add_argument('--cfg', dest='cfg_file',\n",
        "                        help='optional config file',\n",
        "                        default='/content/data/cfg/bird_attn2.yml', type=str)\n",
        "    parser.add_argument('--gpu', dest='gpu_id', type=int, default=-1)\n",
        "    parser.add_argument('--data_dir', dest='data_dir', type=str, default='/content/data/birds')\n",
        "    parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
        "    parser.add_argument('--session',type=int,default=2)\n",
        "    # args = parser.parse_args()\n",
        "    args = parser.parse_known_args()[0]\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsrAm5fJ_J-I"
      },
      "source": [
        " def get_caption(caption):\n",
        "  with open('/content/data/birds/example_captions.txt','w') as file:\n",
        "    file.truncate(0)\n",
        "    #caption = input()\n",
        "    file.write(caption)\n",
        "    file.close()\n",
        "  return \"Your entered caption is: \"+caption"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QJ68qtDH07X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP1XjgVp01w9",
        "outputId": "2a145342-56d1-409a-bc91-2634ad5947ec"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    if args.cfg_file is not None:\n",
        "        cfg_from_file(args.cfg_file)\n",
        "\n",
        "    if args.gpu_id != -1:\n",
        "        cfg.GPU_ID = args.gpu_id\n",
        "    else:\n",
        "        cfg.CUDA = True\n",
        "    cfg.CUDA = True\n",
        "    cfg.TRAIN.FLAG = False\n",
        "    if args.data_dir != '':\n",
        "        cfg.DATA_DIR = args.data_dir\n",
        "    print('Using config:')\n",
        "    models = ['attngan', 'bert', 'gpt2', 'xlnet']\n",
        "\n",
        "    #pprint.pprint(cfg)\n",
        "    #print(caption())\n",
        "    if not cfg.TRAIN.FLAG:\n",
        "        args.manualSeed = 100\n",
        "    elif args.manualSeed is None:\n",
        "        args.manualSeed = random.randint(1, 10000)\n",
        "    random.seed(args.manualSeed)\n",
        "    np.random.seed(args.manualSeed)\n",
        "    torch.manual_seed(args.manualSeed)\n",
        "    if cfg.CUDA:\n",
        "        torch.cuda.manual_seed_all(args.manualSeed)\n",
        "\n",
        "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
        "    #timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "    output_dir = './output_attn%d' %(args.session)\n",
        "    cfg.DATA_DIR = '/content/data/birds/'\n",
        "    split_dir, bshuffle = 'train', True\n",
        "    if not cfg.TRAIN.FLAG:\n",
        "        # bshuffle = False\n",
        "        split_dir = 'test'\n",
        "    #cfg.TRAIN.FLAG=False\n",
        "    # Get data loader\n",
        "    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM - 1))\n",
        "    image_transform = transforms.Compose([\n",
        "        transforms.Scale(int(imsize * 76 / 64)),\n",
        "        transforms.RandomCrop(imsize),\n",
        "        transforms.RandomHorizontalFlip()])\n",
        "    dataset = TextDataset(cfg.DATA_DIR, split_dir,\n",
        "                          base_size=cfg.TREE.BASE_SIZE,\n",
        "                          transform=image_transform)\n",
        "    assert dataset\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "        drop_last=True, shuffle=bshuffle, num_workers=int(cfg.WORKERS))\n",
        "\n",
        "    # Define models and go to train/evaluate\n",
        "    model_dict = {}\n",
        "    cfg.TRAIN.NET_E= '/content/models/xlnet/XLNet_text_encoder200.pth'\n",
        "    cfg.TRAIN.NET_G = '/content/models/xlnet/netG_epoch_630.pth'\n",
        "    pprint.pprint(cfg)\n",
        "    algo1 = condGANTrainer(output_dir, dataloader, dataset.n_words, dataset.ixtoword, 'xlnet')\n",
        "    model_dict['AttnXLNet'] = algo1\n",
        "    cfg.TRAIN.NET_E= '/content/models/bert/bert_text_encoder200.pth'\n",
        "    cfg.TRAIN.NET_G = '/content/models/bert/netG_epoch_658.pth'\n",
        "    pprint.pprint(cfg)\n",
        "    algo2 = condGANTrainer(output_dir, dataloader, dataset.n_words, dataset.ixtoword, 'bert')\n",
        "    model_dict['AttnBERT'] = algo2\n",
        "    cfg.TRAIN.NET_E='/content/models/gpt2/gpt2_text_encoder200.pth'\n",
        "    cfg.TRAIN.NET_G = '/content/models/gpt2/netG_epoch_695.pth'\n",
        "    pprint.pprint(cfg)\n",
        "    algo3 = condGANTrainer(output_dir, dataloader, dataset.n_words, dataset.ixtoword, 'gpt2')\n",
        "    model_dict['AttnGPT'] = algo3\n",
        "    cfg.TRAIN.NET_E='/content/models/attngan/text_encoder200.pth'\n",
        "    cfg.TRAIN.NET_G = '/content/models/attngan/netG_epoch_620.pth'\n",
        "    pprint.pprint(cfg)\n",
        "    algo4 = condGANTrainer(output_dir, dataloader, dataset.n_words, dataset.ixtoword, 'attngan')\n",
        "    model_dict['AttnGAN'] = algo4\n",
        "    # inception_model = INCEPTION_V3()\n",
        "    # if cfg.CUDA:\n",
        "    #     inception_model = inception_model.cuda()\n",
        "    # inception_model.eval()\n",
        "    # start_t = time.time()\n",
        "    # for epoch in [695]:\n",
        "    #     print(\"Epoch number :\",epoch)\n",
        "    #     cfg.TRAIN.NET_G = f'/content/drive/My Drive/data/gpt2_output_attn1/Model/netG_epoch_{epoch}.pth'\n",
        "    #     pprint.pprint(cfg)\n",
        "    #     if cfg.TRAIN.FLAG:\n",
        "    #         algo.train()\n",
        "    #     else:\n",
        "    #         '''generate images from pre-extracted embeddings'''\n",
        "    #         if cfg.B_VALIDATION:\n",
        "    #             algo.sampling(split_dir)  # generate images for the whole valid dataset\n",
        "    #         else:\n",
        "    #             gen_example(epoch,dataset.wordtoix, algo,False)  # generate images for customized captions\n",
        "    #     end_t = time.time()\n",
        "    # print('Total time for training:', end_t - start_t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/data/:96: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:310: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: /content/data/birds//train/filenames.pickle (8855)\n",
            "Load filenames from: /content/data/birds//test/filenames.pickle (2933)\n",
            "Save to:  /content/data/birds/captions.pickle\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '/content/data/birds/',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': False,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '/content/models/xlnet/XLNet_text_encoder200.pth',\n",
            "           'NET_G': '/content/models/xlnet/netG_epoch_630.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 50},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '/content/data/birds/',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': False,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '/content/models/bert/bert_text_encoder200.pth',\n",
            "           'NET_G': '/content/models/bert/netG_epoch_658.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 50},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '/content/data/birds/',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': False,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '/content/models/gpt2/gpt2_text_encoder200.pth',\n",
            "           'NET_G': '/content/models/gpt2/netG_epoch_695.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 50},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '/content/data/birds/',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': False,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '/content/models/attngan/text_encoder200.pth',\n",
            "           'NET_G': '/content/models/attngan/netG_epoch_620.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 50},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbC9HbXupQIP"
      },
      "source": [
        "#gen_example(-1,dataset.wordtoix, model_dict['AttnGAN'],False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlKl0ddWB4lT"
      },
      "source": [
        "# import torch\n",
        "# from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "# model_name = 'tuner007/pegasus_paraphrase'\n",
        "# torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# print(torch_device)\n",
        "# tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "# model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "# def get_response(input_text,num_return_sequences,num_beams):\n",
        "#   batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
        "#   translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "#   tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "#   return tgt_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "707ced6c67094a46bf00297bd5635dd3",
            "a191364c6f1d4b9e8f4755514ff4c622",
            "0fa90f7ab9284b8ab5637ecb4be9dfc4",
            "46144499eaf84459997b1f58b1f72dd5",
            "79be8d08e4544025aaae73e010af3bf3",
            "dd249ab53d9a4b09b74f5feba8116a50",
            "af20c8fa5cd84d5fb17b656b7e13f1f0",
            "d3d586c225664db0ae2824dc087e4439",
            "c7dd4d676ed1454b811436a5ee0f8231",
            "123130aa2f804b39800936ac1b9305f1",
            "5a9a82bdc5a84514a0b742503118ef75",
            "00b0aa70cf564b26ab15d6ed3fcbd857",
            "870b45c2668b4ac4affcb498c73bbed2",
            "46aba3b6957c4849bf83e164cc9b5fa6",
            "6d6eea7dfa684dd5aee60aed90a6b089",
            "af8118d460ae447c99e0d903a8ef62bb",
            "47367f4e0c0b4b80ba99345192d3b630",
            "07a9eb02b5b34feeaebe80da37de1ecb",
            "1dbfd05f5abb4b97a221a22f3c0096e5",
            "b7bd8df18a5547af9ef0ce23c18849f8",
            "930f26bf24144118939980b0e2968d59",
            "cb0fe75f67894cbfb40c3604c40d47c8"
          ]
        },
        "id": "g02vqOY-UxSa",
        "outputId": "59aca0fe-6378-4957-fa37-7f361800679f"
      },
      "source": [
        "app = Flask(__name__, template_folder='/content/html_files', static_folder= '/content/static')    \n",
        "run_with_ngrok(app)\n",
        "# A decorator used to tell the application \n",
        "# which URL is associated function \n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template(\"home.html\",title=\"home\",post_url=None)\n",
        "\n",
        "@app.route('/about')\n",
        "def about():\n",
        "    return render_template(\"about.html\",title=\"about\")\n",
        "\n",
        "@app.route('/contact')\n",
        "def contact():\n",
        "    return render_template(\"contact.html\",title=\"Contact\")\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/eval', methods =[\"GET\", \"POST\"]) \n",
        "def eval(): \n",
        "#     global var\n",
        "    \n",
        "     if request.method == \"POST\": \n",
        "         if not os.path.exists('./static/results'):\n",
        "             mkdir_p('./static/results')\n",
        "         for file_path in os.listdir('./static/results'):\n",
        "             try:\n",
        "                 os.remove('./static/results'+file_path)\n",
        "             except OSError as e:\n",
        "                 print(\"Error: %s : %s\" % (file_path, e.strerror))\n",
        "         post = []\n",
        "         caption = request.form.get(\"caption\") \n",
        "         get_caption(caption)\n",
        "         name = request.form\n",
        "         global model_name\n",
        "         modelName = name['action']\n",
        "         model_name = modelName\n",
        "         print(modelName,name,\"----------------------------------\")\n",
        "\n",
        "#   #    modelname = request.form.get(\"modelname\")\n",
        "# #       caption = \"this bird has a blue crown as well as a brown throat\"\n",
        "# #       models = ['BERT','XLNet','Attn','BERT']\n",
        "# #       for modelname in models:\n",
        "#         for i in range(5):\n",
        "        #  global model_dict\n",
        "        #  model_dict = {}\n",
        "         \n",
        "         models = ['AttnGAN','AttnBERT','AttnGPT','AttnXLNet']\n",
        "        #  for model in models:\n",
        "        #     model_dict[model] = ev.eval(model) \n",
        "#         modelname = ''  \n",
        "#         caption = ''\n",
        "\n",
        "         global image_name\n",
        "         image_name = str(int(time.time()))\n",
        "         print(\"image_name :\",image_name)\n",
        "#         var = eval_main(caption, modelName, image_name)\n",
        "         #.generate(caption, image_name=image_name)\n",
        "         if modelName == 'ALL':\n",
        "            for model in models:\n",
        "                  model_name = model\n",
        "                  image_name = str(int(time.time()))\n",
        "                  time.sleep(2)\n",
        "                  gen_example(-1,dataset.wordtoix, model_dict[model],False)\n",
        "                  post_url = image_name+'.png'\n",
        "                  post.append(post_url)\n",
        "                  print(post)\n",
        "         else:\n",
        "            gen_example(-1,dataset.wordtoix, model_dict[modelName],False)\n",
        "            post_url = image_name+'.png'\n",
        "            post.append(post_url)\n",
        "            print(post_url)\n",
        "        #  for cap in get_response(caption, 2, 10):\n",
        "        #     get_caption(cap)\n",
        "        #     image_name = str(int(time.time()))\n",
        "        #     gen_example(-1,dataset.wordtoix, model_dict[modelName],False)\n",
        "        #     post_url = image_name+'.png'\n",
        "        #     post.append(post_url)\n",
        "\n",
        "         return render_template(\"eval.html\",post=post, caption=caption) \n",
        "     return render_template(\"home.html\",post_url=None) \n",
        "  \n",
        "if __name__=='__main__': \n",
        "# app.run(debug=True) \n",
        "      app.run()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://22ff-34-66-125-106.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [25/Oct/2021 11:14:05] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [25/Oct/2021 11:14:05] \"\u001b[37mGET /static/main.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [25/Oct/2021 11:14:06] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [25/Oct/2021 11:14:07] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [25/Oct/2021 11:14:08] \"\u001b[37mGET /static/main.css HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ALL ImmutableMultiDict([('caption', 'A little bird with pink wings'), ('action', 'ALL')]) ----------------------------------\n",
            "image_name : 1635160461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [25/Oct/2021 11:14:23] \"\u001b[37mGET /eval HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load from: example_captions\n",
            "text_encoder 0.11831045150756836\n",
            "text_encoder1 0.029105663299560547\n",
            "Load text encoder from: /content/models/attngan/text_encoder200.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "127.0.0.1 - - [25/Oct/2021 11:14:24] \"\u001b[37mGET /static/main.css HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text_encoder2 19.684295415878296\n",
            "model 0.15308761596679688\n",
            "Load G from:  /content/models/attngan/netG_epoch_620.pth\n",
            "model2 0.013976335525512695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/data/:742: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/content/data/:743: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/content/data/:763: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/data/:89: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/data/output_attn1/Eval/1504/epoch-1//example_captions_16351604851.png\n",
            "['1635160461.png']\n",
            "Load from: example_captions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:12<00:00, 31716797.59B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text_encoder 21.835594415664673\n",
            "text_encoder1 0.39332151412963867\n",
            "Load text encoder from: /content/models/bert/bert_text_encoder200.pth\n",
            "text_encoder2 0.1836850643157959\n",
            "model 0.20520639419555664\n",
            "Load G from:  /content/models/bert/netG_epoch_658.pth\n",
            "model2 0.011674642562866211\n",
            "/content/drive/My Drive/data/output_attn1/Eval/1504/epoch-1//example_captions_16351605141.png\n",
            "['1635160461.png', '1635160489.png']\n",
            "Load from: example_captions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 548118077/548118077 [00:16<00:00, 32339649.50B/s]\n",
            "100%|██████████| 665/665 [00:00<00:00, 451767.44B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text_encoder 21.124382495880127\n",
            "text_encoder1 10.921804904937744\n",
            "Load text encoder from: /content/models/gpt2/gpt2_text_encoder200.pth\n",
            "text_encoder2 0.1301863193511963\n",
            "model 0.23212075233459473\n",
            "Load G from:  /content/models/gpt2/netG_epoch_695.pth\n",
            "model2 0.01224517822265625\n",
            "/content/drive/My Drive/data/output_attn1/Eval/1504/epoch-1//example_captions_16351605481.png\n",
            "['1635160461.png', '1635160489.png', '1635160514.png']\n",
            "Load from: example_captions\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "707ced6c67094a46bf00297bd5635dd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00b0aa70cf564b26ab15d6ed3fcbd857",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
            "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text_encoder 19.38334584236145\n",
            "text_encoder1 11.52637767791748\n",
            "Load text encoder from: /content/models/xlnet/XLNet_text_encoder200.pth\n",
            "text_encoder2 0.26430535316467285\n",
            "model 0.8758454322814941\n",
            "Load G from:  /content/models/xlnet/netG_epoch_630.pth\n",
            "model2 0.012863397598266602\n",
            "/content/drive/My Drive/data/output_attn1/Eval/1504/epoch-1//example_captions_16351605831.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [25/Oct/2021 11:16:23] \"\u001b[37mPOST /eval HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['1635160461.png', '1635160489.png', '1635160514.png', '1635160548.png']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [25/Oct/2021 11:16:23] \"\u001b[37mGET /static/results/1635160461.png HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [25/Oct/2021 11:16:23] \"\u001b[37mGET /static/results/1635160489.png HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [25/Oct/2021 11:16:23] \"\u001b[37mGET /static/results/1635160514.png HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [25/Oct/2021 11:16:23] \"\u001b[37mGET /static/results/1635160548.png HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWxU0laUH8fK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}